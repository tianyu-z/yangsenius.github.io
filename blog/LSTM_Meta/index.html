<!DOCTYPE html><html><head>
      <title>Learning to learn by gradient descent by gradient descent - PyTorch&#x5B9E;&#x8DF5;</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
         <!--网页头--> <!--网页头-->
      <script type="text/javascript" src="/html_style/head.js"></script>
         <!--网页头--> <!--网页头-->

        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({"extensions":["tex2jax.js"],"jax":["input/TeX","output/HTML-CSS"],"messageStyle":"none","tex2jax":{"processEnvironments":false,"processEscapes":true,"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"TeX":{"extensions":["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]},"HTML-CSS":{"availableFonts":["TeX"]}});
        </script>
        <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js"></script>
        
      
      
      
      
      
      
      
      
      

      <style> 
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview .pagebreak,.markdown-preview .newpage{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center !important}.markdown-preview:not([for="preview"]) .code-chunk .btn-group{display:none}.markdown-preview:not([for="preview"]) .code-chunk .status{display:none}.markdown-preview:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0}@media screen and (min-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px)}}@media screen and (max-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{font-size:14px !important;padding:1em}}@media print{html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,0.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{padding:0 1.6em;margin-top:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc li{margin-bottom:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{list-style-type:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% -  300px);padding:2em calc(50% - 457px -  150px);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */
 
      </style>
    </head>
    <body for="html-export">
           <!--网页 主体--><!--网页 主体--><!--网页 主体-->
            <script type="text/javascript" src="/html_style/body.js"></script>
            <!--网页 主体--><!--网页 主体--><!--网页 主体-->
      <div class="mume markdown-preview   ">
      <p></p>
<blockquote>
<p>&#x539F;&#x521B;&#x535A;&#x6587;&#xFF0C;&#x8F6C;&#x8F7D;&#x8BF7;&#x6CE8;&#x660E;&#x6765;&#x6E90;</p>
</blockquote>
<h2 class="mume-header" id="%E5%BC%95%E8%A8%80">&#x5F15;&#x8A00;</h2>

<p>&#x201C;&#x6D6A;&#x8D39;75&#x91D1;&#x5E01;&#x4E70;&#x63A7;&#x5236;&#x5B88;&#x536B;&#x6709;&#x4EC0;&#x4E48;&#x7528;&#xFF0C;&#x8FD8;&#x4E0D;&#x662F;&#x8BA9;&#x4EBA;&#x7ED9;&#x62C6;&#x4E86;&#xFF1F;&#x6211;&#x8981;&#x6512;&#x94B1;&#xFF01;&#x65E9;&#x665A;&#x618B;&#x51FA;&#x6765;&#x6211;&#x7684;&#x706D;&#x4E16;&#x8005;&#x7684;&#x6B7B;&#x4EA1;&#x4E4B;&#x5E3D;&#xFF01;&#x201D;</p>
<p>Learning to learn&#xFF0C;&#x5373;&#x5B66;&#x4F1A;&#x5B66;&#x4E60;&#xFF0C;&#x662F;&#x6BCF;&#x4E2A;&#x4EBA;&#x90FD;&#x5177;&#x5907;&#x7684;&#x80FD;&#x529B;&#xFF0C;&#x5177;&#x4F53;&#x6307;&#x7684;&#x662F;&#x4E00;&#x79CD;&#x5728;&#x5B66;&#x4E60;&#x7684;&#x8FC7;&#x7A0B;&#x4E2D;&#x53BB;&#x53CD;&#x601D;&#x81EA;&#x5DF1;&#x7684;&#x5B66;&#x4E60;&#x884C;&#x4E3A;&#x6765;&#x8FDB;&#x4E00;&#x6B65;&#x63D0;&#x5347;&#x5B66;&#x4E60;&#x80FD;&#x529B;&#x7684;&#x80FD;&#x529B;&#x3002;&#x8FD9;&#x5728;&#x65E5;&#x5E38;&#x751F;&#x6D3B;&#x4E2D;&#x5176;&#x5B9E;&#x5F88;&#x5E38;&#x89C1;&#xFF0C;&#x6BD4;&#x5982;&#x5728;&#x901A;&#x8FC7;&#x4E00;&#x672C;&#x4E66;&#x6765;&#x5B66;&#x4E60;&#x67D0;&#x4E2A;&#x964C;&#x751F;&#x4E13;&#x4E1A;&#x7684;&#x9886;&#x57DF;&#x77E5;&#x8BC6;&#x65F6;&#xFF08;&#x5982;&#x300A;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x300B;&#xFF09;&#xFF0C;&#x9762;&#x5BF9;&#x5927;&#x91CF;&#x7684;&#x4E13;&#x4E1A;&#x672F;&#x8BED;&#x4E0E;&#x964C;&#x751F;&#x7684;&#x516C;&#x5F0F;&#x7B26;&#x53F7;&#xFF0C;&#x521D;&#x5B66;&#x8005;&#x5F88;&#x5BB9;&#x6613;&#x770B;&#x4E0D;&#x4E0B;&#x53BB;&#xFF0C;&#x800C;&#x6BD4;&#x8F83;&#x597D;&#x7684;&#x65B9;&#x6CD5;&#x5C31;&#x662F;&#x5148;&#x6D4F;&#x89C8;&#x76EE;&#x5F55;&#xFF0C;&#x638C;&#x63E1;&#x4E00;&#x4E9B;&#x7B80;&#x5355;&#x7684;&#x6982;&#x5FF5;&#xFF08;&#x56DE;&#x5F52;&#x4E0E;&#x5206;&#x7C7B;&#x554A;&#xFF0C;&#x76D1;&#x7763;&#x4E0E;&#x65E0;&#x76D1;&#x7763;&#x554A;&#xFF09;&#xFF0C;&#x5E76;&#x5728;&#x6309;&#x987A;&#x5E8F;&#x7684;&#x9605;&#x8BFB;&#x8FC7;&#x7A0B;&#x5B66;&#x4F1A;&#x201C;&#x524D;&#x77BB;&#x201D;&#x4E0E;&#x201C;&#x56DE;&#x987E;&#x201D;&#xFF0C;&#x8FDB;&#x884C;&#x5FEB;&#x901F;&#x5B66;&#x4E60;&#x3002;&#x53C8;&#x6BD4;&#x5982;&#x5728;&#x65E9;&#x671F;&#x63A5;&#x53D7;&#x6559;&#x80B2;&#x7684;&#x5B66;&#x4E60;&#x9636;&#x6BB5;&#xFF0C;&#x76F2;&#x76EE;&#x7684;&#x201C;&#x9898;&#x6D77;&#x6218;&#x672F;&#x201D;&#x6216;&#x6B7B;&#x8BB0;&#x786C;&#x80CC;&#x7684;&#x201C;&#x77E5;&#x8BC6;&#x704C;&#x8F93;&#x201D;&#x5982;&#x679C;&#x4E0D;&#x52A0;&#x4E0A;&#x6070;&#x5F53;&#x7684;&#x53CD;&#x601D;&#x548C;&#x603B;&#x7ED3;&#xFF0C;&#x5F80;&#x5F80;&#x4F1A;&#x8017;&#x65F6;&#x8017;&#x529B;&#xFF0C;&#x6700;&#x540E;&#x8FBE;&#x5230;&#x7684;&#x6548;&#x679C;&#x5374;&#x4E00;&#x822C;&#xFF0C;&#x8FD9;&#x662F;&#x56E0;&#x4E3A;&#x5728;&#x63A5;&#x89E6;&#x65B0;&#x4E1C;&#x897F;&#xFF0C;&#x638C;&#x63E1;&#x65B0;&#x6280;&#x80FD;&#x65F6;&#xFF0C;&#x662F;&#x9700;&#x8981;&#x201C;&#x6280;&#x5DE7;&#x6027;&#x201D;&#x7684;&#x3002;</p>
<p>&#x4ECE;&#x5B66;&#x4E60;&#x77E5;&#x8BC6;&#x5230;&#x5B66;&#x4E60;&#x7B56;&#x7565;&#x7684;&#x5C42;&#x9762;&#x4E0A;&#xFF0C;&#x603B;&#x4F1A;&#x6709;&#x201C;&#x6700;&#x5F3A;&#x738B;&#x8005;&#x201D;&#x5728;&#x544A;&#x8BC9;&#x6211;&#x4EEC;&#xFF0C;&#x201C;&#x94BB;&#x77F3;&#x7684;&#x64CD;&#x4F5C;&#x3001;&#x9EC4;&#x94DC;&#x7684;&#x610F;&#x8BC6;&#x201D;&#x4E5F;&#x8BB8;&#x5E76;&#x4E0D;&#x80FD;&#x53D6;&#x80DC;&#xFF0C;&#x8981;&#x201C;&#x6218;&#x7565;&#x4E0A;&#x6700;&#x4F73;&#xFF0C;&#x6218;&#x672F;&#x4E0A;&#x8C28;&#x614E;&#x201D;&#x624D;&#x80FD;&#x66F4;&#x5FEB;&#x66F4;&#x597D;&#x5730;&#x8FDB;&#x6B65;&#x3002;</p>
<p>&#x8FD9;&#x8DDF;&#x672C;&#x6587;&#x8981;&#x8BB2;&#x7684;&#x5185;&#x5BB9;&#x6709;&#x4EC0;&#x4E48;&#x5173;&#x7CFB;&#x5462;&#xFF1F;&#x8FDB;&#x5165;&#x6B63;&#x9898;&#x3002;</p>
<blockquote>
<p>&#x5176;&#x5B9E;&#x8BFB;&#x8005;&#x53EF;&#x4EE5;&#x5148;&#x56DE;&#x987E;&#x81EA;&#x5DF1;&#x4ECE;&#x521D;&#x9AD8;&#x4E2D;&#x5230;&#x5927;&#x5B66;&#x751A;&#x81F3;&#x7814;&#x7A76;&#x751F;&#x7684;&#x6574;&#x4E2A;&#x5386;&#x7A0B;&#xFF0C;&#x662F;&#x4E0D;&#x662F;&#x53D1;&#x73B0;&#x81EA;&#x5DF1;&#x5DF2;&#x7ECF;&#x5177;&#x5907;&#x4E86;&#x201C;learning to learn&#x201D;&#x7684;&#x80FD;&#x529B;&#xFF1F;</p>
</blockquote>
<p><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/0fbfea6bf315e9609bd2baf28c145b6390f8de1c/2-Figure1-1.png" alt="&#x5728;&#x8FD9;&#x91CC;&#x63D2;&#x5165;&#x56FE;&#x7247;&#x63CF;&#x8FF0;"></p>
<h2 class="mume-header" id="learning-to-learn-by-gradient-descent-by-gradient-descent">Learning to learn  by gradient descent  by gradient descent</h2>

<p><strong>&#x901A;&#x8FC7;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x6765;&#x5B66;&#x4E60;&#x5982;&#x4F55;&#x901A;&#x8FC7;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x5B66;&#x4E60;</strong></p>
<blockquote>
<p>&#x662F;&#x5426;&#x53EF;&#x4EE5;&#x8BA9;&#x4F18;&#x5316;&#x5668;&#x5B66;&#x4F1A;   &quot;&#x4E3A;&#x4E86;&#x66F4;&#x597D;&#x5730;&#x5F97;&#x5230;&#xFF0C;&#x8981;&#x5148;&#x53BB;&#x820D;&#x5F03;&quot;  &#x8FD9;&#x6837;&#x7684;&#x201C;&#x7B56;&#x7565;&#x201D;&#xFF1F;</p>
</blockquote>
<p>&#x672C;&#x535A;&#x5BA2;&#x7ED3;&#x5408;&#x5177;&#x4F53;&#x5B9E;&#x8DF5;&#x6765;&#x89E3;&#x8BFB;&#x300A;Learning to learn by gradient descent by gradient descent&#x300B;&#xFF0C;&#x8FD9;&#x662F;&#x4E00;&#x7BC7;Meta-learning&#xFF08;&#x5143;&#x5B66;&#x4E60;&#xFF09;&#x9886;&#x57DF;&#x7684;<a href="https://arxiv.org/abs/1606.04474">&#x8BBA;&#x6587;</a>,&#x53D1;&#x8868;&#x5728;2016&#x5E74;&#x7684;NIPS&#x3002;&#x7C7B;&#x4F3C;&#x201C;&#x56DE;&#x6587;&#x201D;&#x7ED3;&#x6784;&#x7684;&#x8D77;&#x540D;&#xFF0C;&#x8BA9;&#x8FD9;&#x7BC7;&#x8BBA;&#x6587;&#x53D8;&#x5F97;&#x6709;&#x8DA3;&#xFF0C;&#x662F;&#x4E0D;&#x662F;&#x53EF;&#x4EE5;&#x518D;&#x5957;&#x4E00;&#x5C42;,&quot;Learning to learn to learn by gradient descent by gradient descent by gradient descent&quot;?&#x518D;&#x5957;&#x4E00;&#x5C42;&#xFF1F;</p>
<p>&#x9996;&#x5148;&#x522B;&#x88AB;&#x8BBA;&#x6587;&#x9898;&#x76EE;&#x7ED9;&#x8BEF;&#x5BFC;&#xFF0C;<mark>&#x5B83;&#x4E0D;&#x662F;&#x6C42;&#x68AF;&#x5EA6;&#x7684;&#x68AF;&#x5EA6;&#xFF0C;&#x8FD9;&#x91CC;&#x4E0D;&#x6D89;&#x53CA;&#x5230;&#x4E8C;&#x9636;&#x5BFC;&#x7684;&#x4EFB;&#x4F55;&#x64CD;&#x4F5C;&#xFF0C;&#x800C;&#x662F;&#x8DDF;&#x5982;&#x4F55;&#x5B66;&#x4F1A;&#x66F4;&#x597D;&#x7684;&#x4F18;&#x5316;&#x6709;&#x5173;</mark>&#xFF0C;&#x6B63;&#x786E;&#x7684;&#x65AD;&#x53E5;&#x65B9;&#x6CD5;&#x4E3A;learning to (learn by gradient descent ) by gradient descent &#x3002;</p>
<p>&#x7B2C;&#x4E00;&#x6B21;&#x8BFB;&#x5B8C;&#x540E;&#xFF0C;&#x4E0D;&#x7981;&#x60CA;&#x53F9;&#x4F5C;&#x8005;&#x5DE7;&#x5999;&#x7684;&#x6784;&#x601D;--&#x4F7F;&#x7528;LSTM&#xFF08;long short-term memory&#xFF09;&#x4F18;&#x5316;&#x5668;&#x6765;&#x66FF;&#x4EE3;&#x4F20;&#x7EDF;&#x4F18;&#x5316;&#x5668;&#x5982;&#xFF08;SGD&#xFF0C;RMSProp&#xFF0C;Adam&#x7B49;&#xFF09;&#xFF0C;&#x7136;&#x540E;&#x4F7F;&#x7528;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x6765;&#x4F18;&#x5316;&#x4F18;&#x5316;&#x5668;&#x672C;&#x8EAB;&#x3002;</p>
<p>&#x867D;&#x7136;&#x660E;&#x767D;&#x4E86;&#x4F5C;&#x8005;&#x7684;&#x51FA;&#x53D1;&#x70B9;&#xFF0C;&#x4F46;&#x603B;&#x611F;&#x89C9;&#x4E00;&#x4E9B;&#x7EC6;&#x8282;&#x81EA;&#x5DF1;&#x6CA1;&#x6709;&#x771F;&#x6B63;&#x7406;&#x89E3;&#x3002;&#x7136;&#x540E;&#x5C31;&#x53BB;&#x770B;&#x539F;&#x4F5C;&#x7684;&#x4EE3;&#x7801;&#x5B9E;&#x73B0;&#xFF0C;&#x8BFB;&#x8D77;&#x6765;&#x4E5F;&#x662F;&#x5F88;&#x8D39;&#x52B2;&#x3002;&#x67E5;&#x9605;&#x4E86;&#x4E00;&#x4E9B;&#x535A;&#x5BA2;&#xFF0C;&#x4F46;&#x7F51;&#x4E0A;&#x5BF9;&#x8FD9;&#x7BC7;&#x8BBA;&#x6587;&#x89E3;&#x8BFB;&#x5F88;&#x5C11;&#xFF0C;&#x505C;&#x7559;&#x4E8E;&#x8BBA;&#x6587;&#x7FFB;&#x8BD1;&#x7406;&#x89E3;&#x4E0A;&#x3002;&#x518D;&#x6B21;&#x63E3;&#x6469;&#x8BBA;&#x6587;&#x540E;&#xFF0C;&#x6253;&#x7B97;&#x505A;&#x4E00;&#x4E9B;&#x5B9E;&#x9A8C;&#x6765;&#x7406;&#x89E3;&#x3002; &#x5728;&#x7528;PyTorch&#x5199;&#x4EE3;&#x7801;&#x7684;&#x8FC7;&#x7A0B;&#xFF0C;&#x624D;&#x604D;&#x7136;&#x5927;&#x609F;&#xFF0C;&#x4F5C;&#x8005;&#x7684;&#x601D;&#x8DEF;&#x662F;&#x5982;&#x6B64;&#x7B80;&#x5355;&#x5DE7;&#x5999;&#xFF0C;&#x8BBA;&#x6587;&#x540D;&#x5B57;&#x8D77;&#x7684;&#x4E5F;&#x5F88;&#x6070;&#x5F53;&#xFF0C;&#x6CA1;&#x5728;&#x6545;&#x5F04;&#x7384;&#x865A;&#xFF0C;&#x4F46;&#x662F;&#x5728;&#x5B9E;&#x73B0;&#x7684;&#x8FC7;&#x7A0B;&#x5374;&#x8D39;&#x52B2;&#x4E86;&#x5468;&#x6298;&#xFF01;</p>
<h2 class="mume-header" id="%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95">&#x6587;&#x7AE0;&#x76EE;&#x5F55;</h2>

<ul>
<li><a href="#%E5%BC%95%E8%A8%80">&#x5F15;&#x8A00;</a></li>
<li><a href="#learning-to-learn-by-gradient-descent-by-gradient-descent">Learning to learn  by gradient descent  by gradient descent</a></li>
<li><a href="#%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95">&#x6587;&#x7AE0;&#x76EE;&#x5F55;</a></li>
<li><a href="#%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98">&#x4F18;&#x5316;&#x95EE;&#x9898;</a>
<ul>
<li><a href="#%E5%AE%9A%E4%B9%89%E8%A6%81%E4%BC%98%E5%8C%96%E7%9A%84%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0">&#x5B9A;&#x4E49;&#x8981;&#x4F18;&#x5316;&#x7684;&#x76EE;&#x6807;&#x51FD;&#x6570;</a></li>
<li><a href="#%E5%AE%9A%E4%B9%89%E5%B8%B8%E7%94%A8%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%A6%82sgd-rmsprop-adam">&#x5B9A;&#x4E49;&#x5E38;&#x7528;&#x7684;&#x4F18;&#x5316;&#x5668;&#x5982;SGD, RMSProp, Adam&#x3002;</a></li>
<li><a href="#%E6%8E%A5%E4%B8%8B%E6%9D%A5-%E6%9E%84%E9%80%A0%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95">&#x63A5;&#x4E0B;&#x6765; &#x6784;&#x9020;&#x4F18;&#x5316;&#x7B97;&#x6CD5;</a></li>
<li><a href="#%E5%AF%B9%E6%AF%94%E4%B8%8D%E5%90%8C%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E4%BC%98%E5%8C%96%E6%95%88%E6%9E%9C">&#x5BF9;&#x6BD4;&#x4E0D;&#x540C;&#x4F18;&#x5316;&#x5668;&#x7684;&#x4F18;&#x5316;&#x6548;&#x679C;</a></li>
</ul>
</li>
<li><a href="#meta-optimizer-%E4%BB%8E%E6%89%8B%E5%B7%A5%E8%AE%BE%E8%AE%A1%E4%BC%98%E5%8C%96%E5%99%A8%E8%BF%88%E6%AD%A5%E5%88%B0%E8%87%AA%E5%8A%A8%E8%AE%BE%E8%AE%A1%E4%BC%98%E5%8C%96%E5%99%A8">Meta-optimizer &#xFF1A;&#x4ECE;&#x624B;&#x5DE5;&#x8BBE;&#x8BA1;&#x4F18;&#x5316;&#x5668;&#x8FC8;&#x6B65;&#x5230;&#x81EA;&#x52A8;&#x8BBE;&#x8BA1;&#x4F18;&#x5316;&#x5668;</a>
<ul>
<li><a href="#%E7%94%A8%E4%B8%80%E4%B8%AA%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99%E6%9B%BF%E4%BB%A3%E6%89%8B%E5%B7%A5%E8%AE%BE%E8%AE%A1%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99">&#x7528;&#x4E00;&#x4E2A;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x68AF;&#x5EA6;&#x66F4;&#x65B0;&#x89C4;&#x5219;&#xFF0C;&#x66FF;&#x4EE3;&#x624B;&#x5DE5;&#x8BBE;&#x8BA1;&#x7684;&#x68AF;&#x5EA6;&#x66F4;&#x65B0;&#x89C4;&#x5219;</a>
<ul>
<li><a href="#%E6%9E%84%E5%BB%BAlstm%E4%BC%98%E5%8C%96%E5%99%A8">&#x6784;&#x5EFA;LSTM&#x4F18;&#x5316;&#x5668;</a></li>
<li><a href="#%E4%BC%98%E5%8C%96%E5%99%A8%E6%9C%AC%E8%BA%AB%E7%9A%84%E5%8F%82%E6%95%B0%E5%8D%B3lstm%E7%9A%84%E5%8F%82%E6%95%B0%E4%BB%A3%E8%A1%A8%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E6%9B%B4%E6%96%B0%E7%AD%96%E7%95%A5">&#x4F18;&#x5316;&#x5668;&#x672C;&#x8EAB;&#x7684;&#x53C2;&#x6570;&#x5373;LSTM&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x4EE3;&#x8868;&#x4E86;&#x6211;&#x4EEC;&#x7684;&#x66F4;&#x65B0;&#x7B56;&#x7565;</a>
<ul>
<li><a href="#%E5%A5%BD%E4%BA%86%E7%9C%8B%E4%B8%80%E4%B8%8B%E6%88%91%E4%BB%AC%E4%BD%BF%E7%94%A8%E5%88%9A%E5%88%9A%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84lstm%E4%BC%98%E5%8C%96%E5%99%A8%E5%90%8E%E7%9A%84%E4%BC%98%E5%8C%96%E7%BB%93%E6%9E%9C">&#x597D;&#x4E86;&#xFF0C;&#x770B;&#x4E00;&#x4E0B;&#x6211;&#x4EEC;&#x4F7F;&#x7528;&#x521A;&#x521A;&#x521D;&#x59CB;&#x5316;&#x7684;LSTM&#x4F18;&#x5316;&#x5668;&#x540E;&#x7684;&#x4F18;&#x5316;&#x7ED3;&#x679C;</a></li>
<li><a href="#%E5%92%A6%E4%B8%BA%E4%BB%80%E4%B9%88lstm%E4%BC%98%E5%8C%96%E5%99%A8%E9%82%A3%E4%B9%88%E5%B7%AE%E6%A0%B9%E6%9C%AC%E6%B2%A1%E6%9C%89%E4%BC%98%E5%8C%96%E6%95%88%E6%9E%9C">&#x54A6;&#xFF0C;&#x4E3A;&#x4EC0;&#x4E48;LSTM&#x4F18;&#x5316;&#x5668;&#x90A3;&#x4E48;&#x5DEE;&#xFF0C;&#x6839;&#x672C;&#x6CA1;&#x6709;&#x4F18;&#x5316;&#x6548;&#x679C;&#xFF1F;</a></li>
</ul>
</li>
<li><a href="#%E4%B8%8B%E9%9D%A2%E6%88%91%E4%BB%AC%E5%B0%B1%E6%9D%A5%E4%BC%98%E5%8C%96lstm%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E5%8F%82%E6%95%B0">&#x4E0B;&#x9762;&#x6211;&#x4EEC;&#x5C31;&#x6765;&#x4F18;&#x5316;LSTM&#x4F18;&#x5316;&#x5668;&#x7684;&#x53C2;&#x6570;&#xFF01;</a>
<ul>
<li><a href="#%E4%B8%8B%E6%A3%8B%E6%89%8B">&#x201C;&#x4E0B;&#x68CB;&#x624B; &#x201D;</a></li>
<li><a href="#%E7%89%B9%E7%82%B9-2%E8%80%83%E8%99%91%E4%BC%98%E5%8C%96%E5%99%A8%E4%BC%98%E5%8C%96%E8%BF%87%E7%A8%8B%E7%9A%84%E5%8E%86%E5%8F%B2%E5%85%A8%E5%B1%80%E6%80%A7%E4%BF%A1%E6%81%AF-3%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83%E5%9C%B0%E9%87%87%E6%A0%B7%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%82%E6%95%B0">&#x7279;&#x70B9; &#xFF1A; 2.&#x8003;&#x8651;&#x4F18;&#x5316;&#x5668;&#x4F18;&#x5316;&#x8FC7;&#x7A0B;&#x7684;&#x5386;&#x53F2;&#x5168;&#x5C40;&#x6027;&#x4FE1;&#x606F;     3.&#x72EC;&#x7ACB;&#x540C;&#x5206;&#x5E03;&#x5730;&#x91C7;&#x6837;&#x4F18;&#x5316;&#x95EE;&#x9898;&#x76EE;&#x6807;&#x51FD;&#x6570;&#x7684;&#x53C2;&#x6570;</a></li>
</ul>
</li>
<li><a href="#%E9%80%9A%E8%BF%87%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%9D%A5%E4%BC%98%E5%8C%96-%E4%BC%98%E5%8C%96%E5%99%A8">&#x901A;&#x8FC7;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x6CD5;&#x6765;&#x4F18;&#x5316;  &#x4F18;&#x5316;&#x5668;</a>
<ul>
<li><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88loss%E5%80%BC%E6%B2%A1%E6%9C%89%E6%94%B9%E5%8F%98%E4%B8%BA%E4%BB%80%E4%B9%88lstm%E5%8F%82%E6%95%B0%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8D%E5%AD%98%E5%9C%A8%E7%9A%84">&#x4E3A;&#x4EC0;&#x4E48;loss&#x503C;&#x6CA1;&#x6709;&#x6539;&#x53D8;&#xFF1F;&#x4E3A;&#x4EC0;&#x4E48;LSTM&#x53C2;&#x6570;&#x7684;&#x68AF;&#x5EA6;&#x4E0D;&#x5B58;&#x5728;&#x7684;&#xFF1F;</a></li>
<li><a href="#%E9%97%AE%E9%A2%98%E5%87%BA%E5%9C%A8%E5%93%AA%E9%87%8C">&#x95EE;&#x9898;&#x51FA;&#x5728;&#x54EA;&#x91CC;&#xFF1F;</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8D%E5%86%8D%E4%B8%A2%E5%A4%B1%E4%BA%86lstm%E7%9A%84%E5%8F%82%E6%95%B0%E7%9A%84%E6%A2%AF%E5%BA%A6%E7%BB%8F%E8%BF%87%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%9A%84%E6%B5%81%E5%8A%A8%E5%B7%B2%E7%BB%8F%E4%BA%A7%E7%94%9F%E4%BA%86">&#x8BA1;&#x7B97;&#x56FE;&#x4E0D;&#x518D;&#x4E22;&#x5931;&#x4E86;&#xFF0C;LSTM&#x7684;&#x53C2;&#x6570;&#x7684;&#x68AF;&#x5EA6;&#x7ECF;&#x8FC7;&#x8BA1;&#x7B97;&#x56FE;&#x7684;&#x6D41;&#x52A8;&#x5DF2;&#x7ECF;&#x4EA7;&#x751F;&#x4E86;&#xFF01;</a></li>
</ul>
</li>
<li><a href="#%E5%8F%88%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E5%B9%BA%E8%9B%BE%E5%AD%90">&#x53C8;&#x51FA;&#x4E86;&#x4EC0;&#x4E48;&#x5E7A;&#x86FE;&#x5B50;&#xFF1F;</a>
<ul>
<li><a href="#%E4%B8%8D%E5%90%8C%E5%91%A8%E6%9C%9F%E4%B8%8B%E8%BE%93%E5%85%A5lstm%E7%9A%84%E6%A2%AF%E5%BA%A6%E5%B9%85%E5%80%BC%E6%95%B0%E9%87%8F%E7%BA%A7%E4%B8%8D%E5%9C%A8%E4%B8%80%E4%B8%AA%E7%AD%89%E7%BA%A7%E4%B8%8A%E9%9D%A2">&#x4E0D;&#x540C;&#x5468;&#x671F;&#x4E0B;&#x8F93;&#x5165;LSTM&#x7684;&#x68AF;&#x5EA6;&#x5E45;&#x503C;&#x6570;&#x91CF;&#x7EA7;&#x4E0D;&#x5728;&#x4E00;&#x4E2A;&#x7B49;&#x7EA7;&#x4E0A;&#x9762;</a></li>
<li><a href="#%E7%94%A8%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%BD%92%E4%B8%80%E5%8C%96%E5%B9%85%E5%80%BC%E6%96%B9%E5%90%91%E4%BA%8C%E5%85%83%E7%BB%84%E6%9B%BF%E4%BB%A3%E5%8E%9F%E6%A2%AF%E5%BA%A6%E4%BD%9C%E4%B8%BAlstm%E7%9A%84%E8%BE%93%E5%85%A5">&#x7528;&#x68AF;&#x5EA6;&#x7684;&#xFF08;&#x5F52;&#x4E00;&#x5316;&#x5E45;&#x503C;&#xFF0C;&#x65B9;&#x5411;&#xFF09;&#x4E8C;&#x5143;&#x7EC4;&#x66FF;&#x4EE3;&#x539F;&#x68AF;&#x5EA6;&#x4F5C;&#x4E3A;LSTM&#x7684;&#x8F93;&#x5165;</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E4%BB%A5%E4%B8%8A%E6%98%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E5%86%99%E9%81%87%E5%88%B0%E7%9A%84%E7%A7%8D%E7%A7%8D%E9%97%AE%E9%A2%98%E4%B8%8B%E9%9D%A2%E5%B0%B1%E6%98%AF%E6%9C%80%E5%AE%8C%E6%95%B4%E7%9A%84%E6%9C%89%E6%95%88%E4%BB%A3%E7%A0%81%E4%BA%86">&#x4EE5;&#x4E0A;&#x662F;&#x4EE3;&#x7801;&#x7F16;&#x5199;&#x9047;&#x5230;&#x7684;&#x79CD;&#x79CD;&#x95EE;&#x9898;&#xFF0C;&#x4E0B;&#x9762;&#x5C31;&#x662F;&#x6700;&#x5B8C;&#x6574;&#x7684;&#x6709;&#x6548;&#x4EE3;&#x7801;&#x4E86;&#xFF01;&#xFF01;&#xFF01;</a>
<ul>
<li><a href="#%E6%88%91%E4%BB%AC%E5%85%88%E6%9D%A5%E7%9C%8B%E7%9C%8B%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84lstm%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E6%95%88%E6%9E%9C">&#x6211;&#x4EEC;&#x5148;&#x6765;&#x770B;&#x770B;&#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x7684;LSTM&#x4F18;&#x5316;&#x5668;&#x7684;&#x6548;&#x679C;</a><br>
* <a href="#%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84lstm%E4%BC%98%E5%8C%96%E5%99%A8%E6%B2%A1%E6%9C%89%E4%BB%BB%E4%BD%95%E6%95%88%E6%9E%9Closs%E5%8F%91%E6%95%A3%E4%BA%86%E5%9B%A0%E4%B8%BA%E8%BF%98%E6%B2%A1%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96%E5%99%A8">&#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x7684;LSTM&#x4F18;&#x5316;&#x5668;&#x6CA1;&#x6709;&#x4EFB;&#x4F55;&#x6548;&#x679C;&#xFF0C;loss&#x53D1;&#x6563;&#x4E86;&#xFF0C;&#x56E0;&#x4E3A;&#x8FD8;&#x6CA1;&#x8BAD;&#x7EC3;&#x4F18;&#x5316;&#x5668;</a><br>
* <a href="#%E6%8E%A5%E4%B8%8B%E6%9D%A5%E7%9C%8B%E4%B8%80%E4%B8%8B%E4%BC%98%E5%8C%96%E5%A5%BD%E7%9A%84lstm%E4%BC%98%E5%8C%96%E5%99%A8%E6%A8%A1%E5%9E%8B%E5%92%8Csgdrmspropadam%E7%9A%84%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94%E8%A1%A8%E7%8E%B0%E5%90%A7~">&#x63A5;&#x4E0B;&#x6765;&#x770B;&#x4E00;&#x4E0B;&#x4F18;&#x5316;&#x597D;&#x7684;LSTM&#x4F18;&#x5316;&#x5668;&#x6A21;&#x578B;&#x548C;SGD&#xFF0C;RMSProp&#xFF0C;Adam&#x7684;&#x4F18;&#x5316;&#x6027;&#x80FD;&#x5BF9;&#x6BD4;&#x8868;&#x73B0;&#x5427;~</a></li>
</ul>
</li>
<li><a href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90%E4%B8%8E%E7%BB%93%E8%AE%BA">&#x5B9E;&#x9A8C;&#x7ED3;&#x679C;&#x5206;&#x6790;&#x4E0E;&#x7ED3;&#x8BBA;</a>
<ul>
<li><a href="#%E8%AF%B7%E6%B3%A8%E6%84%8Flstm%E4%BC%98%E5%8C%96%E5%99%A8%E6%9C%80%E7%BB%88%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5%E6%98%AF%E6%B2%A1%E6%9C%89%E4%BB%BB%E4%BD%95%E4%BA%BA%E5%B7%A5%E8%AE%BE%E8%AE%A1%E7%9A%84%E7%BB%8F%E9%AA%8C">&#x8BF7;&#x6CE8;&#x610F;&#xFF1A;LSTM&#x4F18;&#x5316;&#x5668;&#x6700;&#x7EC8;&#x4F18;&#x5316;&#x7B56;&#x7565;&#x662F;&#x6CA1;&#x6709;&#x4EFB;&#x4F55;&#x4EBA;&#x5DE5;&#x8BBE;&#x8BA1;&#x7684;&#x7ECF;&#x9A8C;</a></li>
</ul>
</li>
<li><a href="#%E5%AE%9E%E9%AA%8C%E6%9D%A1%E4%BB%B6">&#x5B9E;&#x9A8C;&#x6761;&#x4EF6;&#xFF1A;</a></li>
<li><a href="#%E5%90%8E%E5%8F%99">&#x540E;&#x53D9;</a></li>
<li><a href="#%E4%B8%8B%E8%BD%BD%E5%9C%B0%E5%9D%80%E4%B8%8E%E5%8F%82%E8%80%83">&#x4E0B;&#x8F7D;&#x5730;&#x5740;&#x4E0E;&#x53C2;&#x8003;</a></li>
</ul>
<p><strong>&#x5982;&#x679C;&#x60F3;&#x770B;&#x6700;&#x7EC8;&#x7248;&#x4EE3;&#x7801;&#x548C;&#x7ED3;&#x679C;&#xFF0C;&#x53EF;&#x4EE5;&#x76F4;&#x63A5;&#x8DF3;&#x5230;&#x6587;&#x6863;&#x7684;&#x6700;&#x540E;&#xFF01;&#xFF01;</strong></p>
<p>&#x4E0B;&#x9762;&#x5199;&#x7684;&#x4E00;&#x4E9B;&#x6587;&#x5B57;&#x4E0E;&#x4EE3;&#x7801;&#x4E3B;&#x8981;&#x7AD9;&#x5728;&#x6211;&#x81EA;&#x8EAB;&#x7684;&#x89D2;&#x5EA6;&#xFF0C;&#x8BB0;&#x5F55;&#x81EA;&#x5DF1;&#x5728;&#x5B66;&#x4E60;&#x7814;&#x7A76;&#x8FD9;&#x7BC7;&#x8BBA;&#x6587;&#x548C;&#x4EE3;&#x7801;&#x8FC7;&#x7A0B;&#x4E2D;&#x7684;&#x6240;&#x6709;&#x5386;&#x7A0B;&#xFF0C;&#x5982;&#x4F55;&#x60F3;&#x7684;&#xFF0C;&#x9047;&#x5230;&#x4E86;&#x4EC0;&#x4E48;&#x9519;&#x8BEF;&#xFF0C;&#x95EE;&#x9898;&#x5728;&#x54EA;&#x91CC;&#xFF0C;&#x6211;&#x628A;&#x81EA;&#x5DF1;&#x7406;&#x89E3;&#x9886;&#x609F;&#x201C;learning to learn&#x201D;&#x8FD9;&#x7BC7;&#x8BBA;&#x6587;&#x7684;&#x8FC7;&#x7A0B;&#x5256;&#x6790;&#x4E86;&#x4E00;&#x4E0B;&#xFF0C;&#x4E5F;&#x610F;&#x5473;&#x7740;&#x6211;&#x81EA;&#x5DF1;&#x4E5F;&#x5728;&#x201C;learning to learn&#x201D;&#xFF01;&#x4E3A;&#x4E86;&#x5C55;&#x73B0;&#x81EA;&#x5DF1;&#x7684;&#x5FC3;&#x8DEF;&#x5386;&#x7A0B;&#xFF0C;&#x6211;&#x57FA;&#x672C;&#x4FDD;&#x7559;&#x4E86;&#x6240;&#x6709;&#x7684;&#x75D5;&#x8FF9;&#xFF0C;&#x8FD9;&#x610F;&#x5473;&#x7740;&#x6709;&#x4E9B;&#x4EE3;&#x7801;&#x4E0D;&#x591F;&#x6574;&#x6D01;&#xFF0C;&#x4E0D;&#x8FC7;&#x6587;&#x6863;&#x7684;&#x6700;&#x540E;&#x662F;&#x6700;&#x7EC8;&#x7B80;&#x6D01;&#x5B8C;&#x6574;&#x7248;&#x3002;<br>
<mark>&#x63D0;&#x9192;&#xFF1A;&#x770B;&#x5B8C;&#x6574;&#x4E2A;&#x6587;&#x6863;&#x9700;&#x8981;&#x5927;&#x91CF;&#x7684;&#x8010;&#x5FC3;  : )</mark></p>
<p>&#x6211;&#x9ED8;&#x8BA4;&#x8BFB;&#x8005;&#x5DF2;&#x7ECF;&#x638C;&#x63E1;&#x4E86;&#x4E00;&#x4E9B;&#x5FC5;&#x8981;&#x77E5;&#x8BC6;&#xFF0C;&#x4E5F;&#x5E0C;&#x671B;&#x901A;&#x8FC7;&#x56DE;&#x987E;&#x8FD9;&#x4E9B;&#x7ECF;&#x5178;&#x7814;&#x7A76;&#x7ED9;&#x81EA;&#x5DF1;&#x548C;&#x4E00;&#x4E9B;&#x8BFB;&#x8005;&#x5E26;&#x6765;&#x5207;&#x5B9E;&#x7684;&#x5E2E;&#x52A9;&#x548C;&#x542F;&#x53D1;&#x3002;</p>
<p>&#x7528;Pytorch&#x5B9E;&#x73B0;&#x8FD9;&#x7BC7;&#x8BBA;&#x6587;&#x60F3;&#x6CD5;&#x5176;&#x5B9E;&#x5F88;&#x65B9;&#x4FBF;&#xFF0C;&#x4F46;&#x662F;&#x8BBA;&#x6587;&#x4F5C;&#x8005;&#x6765;&#x81EA;DeepMind&#xFF0C;&#x4ED6;&#x4EEC;&#x7528;<a href="https://github.com/deepmind/learning-to-learn">Tensorflow&#x5199;&#x7684;&#x9879;&#x76EE;</a>&#xFF0C;&#x8BFB;&#x4ED6;&#x4EEC;&#x7684;&#x4EE3;&#x7801;&#x4F60;&#x5C31;&#x4F1A;&#x9886;&#x6559;&#x5230;&#x6700;&#x524D;&#x6CBF;&#x7684;&#x4E00;&#x7EBF;AI&#x5DE5;&#x7A0B;&#x5E08;&#x4EEC;&#x662F;&#x5982;&#x4F55;&#x8FDB;&#x884C;&#x5DE5;&#x7A0B;&#x5B9E;&#x8DF5;&#x7684;&#x3002;</p>
<p>&#x4E0B;&#x9762;&#x8FDB;&#x5165;&#x6B63;&#x9898;&#xFF0C;&#x6211;&#x4F1A;&#x6309;&#x7167;&#x6700;&#x7B80;&#x5355;&#x7684;&#x601D;&#x8DEF;&#xFF0C;&#x5FAA;&#x5E8F;&#x6E10;&#x8FDB;&#x5730;&#x5C55;&#x5F00;, &lt;0..0&gt;&#x3002;</p>
<h2 class="mume-header" id="%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98">&#x4F18;&#x5316;&#x95EE;&#x9898;</h2>

<p>&#x7ECF;&#x5178;&#x7684;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x95EE;&#x9898;&#xFF0C;&#x5305;&#x62EC;&#x5F53;&#x4E0B;&#x7684;&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x76F8;&#x5173;&#x95EE;&#x9898;&#xFF0C;&#x5927;&#x591A;&#x53EF;&#x4EE5;&#x88AB;&#x8868;&#x8FBE;&#x6210;&#x4E00;&#x4E2A;&#x76EE;&#x6807;&#x51FD;&#x6570;&#x7684;&#x4F18;&#x5316;&#x95EE;&#x9898;&#xFF1A;</p>
<p></p><div class="mathjax-exps">$$\theta ^{*}= \arg\min_{\theta\in \Theta }f\left ( \theta  \right )$$</div><p></p>
<p>&#x4E00;&#x4E9B;&#x4F18;&#x5316;&#x65B9;&#x6CD5;&#x53EF;&#x4EE5;&#x6C42;&#x89E3;&#x4E0A;&#x8FF0;&#x95EE;&#x9898;&#xFF0C;&#x6700;&#x5E38;&#x89C1;&#x7684;&#x5373;&#x68AF;&#x5EA6;&#x66F4;&#x65B0;&#x7B56;&#x7565;&#xFF1A;<br>
</p><div class="mathjax-exps">$$\theta_{t+1}=\theta_{t}-\alpha_{t}*\nabla f\left ( \theta_{t}\right )$$</div><p></p>
<p>&#x65E9;&#x671F;&#x7684;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x4F1A;&#x5FFD;&#x7565;&#x68AF;&#x5EA6;&#x7684;&#x4E8C;&#x9636;&#x4FE1;&#x606F;&#xFF0C;&#x800C;&#x7ECF;&#x5178;&#x7684;&#x4F18;&#x5316;&#x6280;&#x672F;&#x901A;&#x8FC7;&#x52A0;&#x5165;<strong>&#x66F2;&#x7387;&#x4FE1;&#x606F;</strong>&#x6539;&#x53D8;&#x6B65;&#x957F;&#x6765;&#x7EA0;&#x6B63;&#xFF0C;&#x6BD4;&#x5982;Hessian&#x77E9;&#x9635;&#x7684;&#x4E8C;&#x9636;&#x504F;&#x5BFC;&#x6570;&#x3002;<br>
<strong>Deep learning</strong>&#x793E;&#x533A;&#x7684;&#x58EE;&#x5927;&#xFF0C;&#x6F14;&#x751F;&#x51FA;&#x5F88;&#x591A;&#x6C42;&#x89E3;&#x9AD8;&#x7EF4;&#x975E;&#x51F8;&#x7684;&#x4F18;&#x5316;&#x6C42;&#x89E3;&#x5668;&#xFF0C;&#x5982;<br>
<strong>momentum</strong>[Nesterov, 1983, Tseng, 1998], <strong>Rprop</strong> [Riedmiller and Braun, 1993], <strong>Adagrad</strong> [Duchi et al., 2011], <strong>RMSprop</strong> [Tieleman and Hinton, 2012], and <strong>ADAM</strong> [Kingma and Ba, 2015].</p>
<p>&#x76EE;&#x524D;&#x7528;&#x4E8E;&#x5927;&#x89C4;&#x6A21;&#x56FE;&#x50CF;&#x8BC6;&#x522B;&#x7684;&#x6A21;&#x578B;&#x5F80;&#x5F80;&#x4F7F;&#x7528;&#x5377;&#x79EF;&#x7F51;&#x7EDC;CNN&#x901A;&#x8FC7;&#x5B9A;&#x4E49;&#x4E00;&#x4E2A;&#x4EE3;&#x4EF7;&#x51FD;&#x6570;&#x6765;&#x62DF;&#x5408;&#x6570;&#x636E;&#x4E0E;&#x6807;&#x7B7E;&#xFF0C;&#x5176;&#x672C;&#x8D28;&#x8FD8;&#x662F;&#x4E00;&#x4E2A;&#x4F18;&#x5316;&#x95EE;&#x9898;&#x3002;</p>
<p>&#x8FD9;&#x91CC;&#x6211;&#x4EEC;&#x8003;&#x8651;&#x4E00;&#x4E2A;&#x7B80;&#x5355;&#x7684;&#x4F18;&#x5316;&#x95EE;&#x9898;&#xFF0C;&#x6BD4;&#x5982;&#x6C42;&#x4E00;&#x4E2A;&#x56DB;&#x6B21;&#x975E;&#x51F8;&#x51FD;&#x6570;&#x7684;&#x6700;&#x5C0F;&#x503C;&#x70B9;&#x3002;&#x5BF9;&#x4E8E;&#x66F4;&#x590D;&#x6742;&#x7684;&#x6A21;&#x578B;&#xFF0C;&#x4E0B;&#x9762;&#x7684;&#x65B9;&#x6CD5;&#x540C;&#x6837;&#x9002;&#x7528;&#x3002;</p>
<h3 class="mume-header" id="%E5%AE%9A%E4%B9%89%E8%A6%81%E4%BC%98%E5%8C%96%E7%9A%84%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0">&#x5B9A;&#x4E49;&#x8981;&#x4F18;&#x5316;&#x7684;&#x76EE;&#x6807;&#x51FD;&#x6570;</h3>

<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
DIM <span class="token operator">=</span> <span class="token number">10</span>
w <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>DIM<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>w<span class="token punctuation">,</span>a<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>b<span class="token operator">=</span><span class="token number">1.5</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#&#x5B9A;&#x4E49;&#x8981;&#x4F18;&#x5316;&#x7684;&#x51FD;&#x6570;&#xFF0C;&#x6C42;x&#x7684;&#x6700;&#x4F18;&#x89E3;</span>
    x<span class="token operator">=</span> w<span class="token operator">*</span><span class="token punctuation">(</span>x<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token punctuation">(</span>x<span class="token operator">+</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token operator">*</span>x<span class="token operator">*</span><span class="token punctuation">(</span>x<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

</pre><h3 class="mume-header" id="%E5%AE%9A%E4%B9%89%E5%B8%B8%E7%94%A8%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%A6%82sgd-rmsprop-adam">&#x5B9A;&#x4E49;&#x5E38;&#x7528;&#x7684;&#x4F18;&#x5316;&#x5668;&#x5982;SGD, RMSProp, Adam&#x3002;</h3>

<p>SGD&#x4EC5;&#x4EC5;&#x53EA;&#x662F;&#x7ED9;&#x68AF;&#x5EA6;&#x4E58;&#x4EE5;&#x4E00;&#x4E2A;&#x5B66;&#x4E60;&#x7387;&#x3002;</p>
<p>RMSProp&#x7684;&#x65B9;&#x6CD5;&#x662F;&#xFF1A;</p>
<p></p><div class="mathjax-exps">$$E[g^2]_t = 0.9 E[g^2]_{t-1} + 0.1 g^2_t$$</div><p></p>
<p></p><div class="mathjax-exps">$$\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}$$</div><p></p>
<p>&#x5F53;&#x524D;&#x65F6;&#x523B;&#x4E0B;&#xFF0C;&#x7528;&#x5F53;&#x524D;&#x68AF;&#x5EA6;&#x548C;&#x5386;&#x53F2;&#x68AF;&#x5EA6;&#x7684;&#x5E73;&#x65B9;&#x52A0;&#x6743;&#x548C;&#xFF08;&#x8D8A;&#x8001;&#x7684;&#x5386;&#x53F2;&#x68AF;&#x5EA6;&#xFF0C;&#x5176;&#x6743;&#x91CD;&#x8D8A;&#x4F4E;&#xFF09;&#x6765;&#x91CD;&#x65B0;&#x8C03;&#x8282;&#x5B66;&#x4E60;&#x7387;(&#x5982;&#x679C;&#x5386;&#x53F2;&#x68AF;&#x5EA6;&#x8D8A;&#x4F4E;&#xFF0C;&#x201C;&#x66F2;&#x9762;&#x66F4;&#x5E73;&#x5766;&#x201D;&#xFF0C;&#x90A3;&#x4E48;&#x5B66;&#x4E60;&#x7387;&#x8D8A;&#x5927;&#xFF0C;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x66F4;&#x201C;&#x6FC0;&#x8FDB;&#x201D;&#x4E00;&#x4E9B;&#xFF0C;&#x5982;&#x679C;&#x5386;&#x53F2;&#x68AF;&#x5EA6;&#x8D8A;&#x9AD8;&#xFF0C;&#x201C;&#x66F2;&#x9762;&#x66F4;&#x9661;&#x5CED;&#x201D;&#x90A3;&#x4E48;&#x5B66;&#x4E60;&#x7387;&#x8D8A;&#x5C0F;&#xFF0C;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x66F4;&#x201C;&#x8C28;&#x614E;&#x201D;&#x4E00;&#x4E9B;)&#xFF0C;&#x6765;&#x66F4;&#x5FEB;&#x66F4;&#x597D;&#x5730;&#x671D;&#x7740;&#x5168;&#x5C40;&#x6700;&#x4F18;&#x89E3;&#x6536;&#x655B;&#x3002;</p>
<p>Adam&#x662F;RMSProp&#x7684;&#x53D8;&#x4F53;&#xFF1A;<br>
</p><div class="mathjax-exps">$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\  v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$</div><p></p>
<p></p><div class="mathjax-exps">$$\hat{m}_t = \dfrac{m_t}{1 - \beta^t_1} \\  \hat{v}_t = \dfrac{v_t}{1 - \beta^t_2}$$</div><p></p>
<p></p><div class="mathjax-exps">$$\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$</div><br>
&#x5373;&#x901A;&#x8FC7;&#x4F30;&#x8BA1;&#x5F53;&#x524D;&#x68AF;&#x5EA6;&#x7684;&#x4E00;&#x9636;&#x77E9;&#x4F30;&#x8BA1;&#x548C;&#x4E8C;&#x9636;&#x77E9;&#x4F30;&#x8BA1;&#x6765;&#x4EE3;&#x66FF;&#xFF0C;&#x68AF;&#x5EA6;&#x548C;&#x68AF;&#x5EA6;&#x7684;&#x5E73;&#x65B9;&#xFF0C;&#x7136;&#x540E;&#x66F4;&#x65B0;&#x7B56;&#x7565;&#x548C;RMSProp&#x4E00;&#x6837;&#x3002;<p></p>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token keyword">def</span> <span class="token function">SGD</span><span class="token punctuation">(</span>gradients<span class="token punctuation">,</span> state<span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
   
    <span class="token keyword">return</span> <span class="token operator">-</span>gradients<span class="token operator">*</span>learning_rate<span class="token punctuation">,</span> state

<span class="token keyword">def</span> <span class="token function">RMS</span><span class="token punctuation">(</span>gradients<span class="token punctuation">,</span> state<span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> decay_rate<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> state <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        state <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>DIM<span class="token punctuation">)</span>
    
    state <span class="token operator">=</span> decay_rate<span class="token operator">*</span>state <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>decay_rate<span class="token punctuation">)</span><span class="token operator">*</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>gradients<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    update <span class="token operator">=</span> <span class="token operator">-</span>learning_rate<span class="token operator">*</span>gradients <span class="token operator">/</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>state<span class="token operator">+</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> update<span class="token punctuation">,</span> state

<span class="token keyword">def</span> <span class="token function">Adam</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">)</span>
</pre><p>&#x8FD9;&#x91CC;&#x7684;Adam&#x4F18;&#x5316;&#x5668;&#x76F4;&#x63A5;&#x7528;&#x4E86;Pytorch&#x91CC;&#x5B9A;&#x4E49;&#x7684;&#x3002;&#x7136;&#x540E;&#x6211;&#x4EEC;&#x901A;&#x8FC7;&#x4F18;&#x5316;&#x5668;&#x6765;&#x6C42;&#x89E3;&#x6781;&#x5C0F;&#x503C;x&#xFF0C;&#x901A;&#x8FC7;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x7684;&#x8FC7;&#x7A0B;&#xFF0C;&#x6211;&#x4EEC;&#x671F;&#x671B;&#x7684;&#x51FD;&#x6570;&#x503C;&#x662F;&#x9010;&#x6B65;&#x4E0B;&#x964D;&#x7684;&#x3002;<br>
&#x8FD9;&#x662F;&#x6211;&#x4EEC;&#x4E00;&#x822C;&#x4EBA;&#x4E3A;&#x8BBE;&#x8BA1;&#x7684;&#x5B66;&#x4E60;&#x7B56;&#x7565;&#xFF0C;&#x5373;<mark>&#x9010;&#x6B65;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x6CD5;&#xFF0C;&#x4EE5;&#x201C;&#x6BCF;&#x6B21;&#x90FD;&#x6BD4;&#x4E0A;&#x4E00;&#x6B21;&#x8FDB;&#x6B65;&#x4E00;&#x4E9B;&#x201D; &#x4E3A;&#x539F;&#x5219;&#x8FDB;&#x884C;&#x5B66;&#x4E60;&#xFF01;</mark></p>
<h3 class="mume-header" id="%E6%8E%A5%E4%B8%8B%E6%9D%A5-%E6%9E%84%E9%80%A0%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95">&#x63A5;&#x4E0B;&#x6765; &#x6784;&#x9020;&#x4F18;&#x5316;&#x7B97;&#x6CD5;</h3>

<pre data-role="codeBlock" data-info="python" class="language-python">TRAINING_STEPS <span class="token operator">=</span> <span class="token number">15</span>
theta <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>DIM<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>theta<span class="token punctuation">,</span>a<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>b<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span> 
theta_init <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>theta<span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">learn</span><span class="token punctuation">(</span>optimizee<span class="token punctuation">,</span>unroll_train_steps<span class="token punctuation">,</span>retain_graph_flag<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>reset_theta <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span> 
    <span class="token triple-quoted-string string">&quot;&quot;&quot;retain_graph_flag=False   PyTorch &#x9ED8;&#x8BA4;&#x6BCF;&#x6B21;loss_backward&#x540E; &#x91CA;&#x653E;&#x52A8;&#x6001;&#x56FE;
    #  reset_theta = False     &#x9ED8;&#x8BA4;&#x6BCF;&#x6B21;&#x5B66;&#x4E60;&#x524D; &#x4E0D;&#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x53C2;&#x6570;&quot;&quot;&quot;</span>
    
    <span class="token keyword">if</span> reset_theta <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
        theta_new <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>DIM<span class="token punctuation">)</span>
        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>theta_new<span class="token punctuation">,</span>a<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>b<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span> 
        theta_init_new <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>theta<span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> theta_init_new
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> theta_init
        
    global_loss_graph <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment">#&#x8FD9;&#x4E2A;&#x662F;&#x4E3A;LSTM&#x4F18;&#x5316;&#x5668;&#x6C42;&#x6240;&#x6709;loss&#x76F8;&#x52A0;&#x4EA7;&#x751F;&#x8BA1;&#x7B97;&#x56FE;&#x51C6;&#x5907;&#x7684;</span>
    state <span class="token operator">=</span> <span class="token boolean">None</span>
    x<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
    <span class="token keyword">if</span> optimizee<span class="token punctuation">.</span>__name__ <span class="token operator">!=</span><span class="token string">&apos;Adam&apos;</span><span class="token punctuation">:</span>
        losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>unroll_train_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
            x<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
            
            loss <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            
            <span class="token comment">#global_loss_graph += (0.8*torch.log10(torch.Tensor([i]))+1)*loss</span>
            
            global_loss_graph <span class="token operator">+=</span> loss
            
            <span class="token comment">#print(loss)</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span>retain_graph_flag<span class="token punctuation">)</span> <span class="token comment"># &#x9ED8;&#x8BA4;&#x4E3A;False,&#x5F53;&#x4F18;&#x5316;LSTM&#x8BBE;&#x7F6E;&#x4E3A;True</span>
            update<span class="token punctuation">,</span> state <span class="token operator">=</span> optimizee<span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">,</span> state<span class="token punctuation">)</span>
            losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
            
            <span class="token comment">#&#x8FD9;&#x4E2A;&#x64CD;&#x4F5C; &#x76F4;&#x63A5;&#x628A;x&#x4E2D;&#x5305;&#x542B;&#x7684;&#x56FE;&#x7ED9;&#x91CA;&#x653E;&#x4E86;&#xFF0C;</span>
           
            x <span class="token operator">=</span> x <span class="token operator">+</span> update
            
            x <span class="token operator">=</span> x<span class="token punctuation">.</span>detach_<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment">#&#x8FD9;&#x4E2A;&#x64CD;&#x4F5C; &#x76F4;&#x63A5;&#x628A;x&#x4E2D;&#x5305;&#x542B;&#x7684;&#x56FE;&#x7ED9;&#x91CA;&#x653E;&#x4E86;&#xFF0C;</span>
            <span class="token comment">#&#x90A3;&#x4F20;&#x9012;&#x7ED9;&#x4E0B;&#x6B21;&#x8BAD;&#x7EC3;&#x7684;x&#x4ECE;&#x5B50;&#x8282;&#x70B9;&#x53D8;&#x6210;&#x4E86;&#x53F6;&#x8282;&#x70B9;&#xFF0C;&#x90A3;&#x4E48;&#x68AF;&#x5EA6;&#x5C31;&#x4E0D;&#x80FD;&#x6CBF;&#x7740;&#x8FD9;&#x4E2A;&#x8DEF;&#x56DE;&#x4F20;&#x4E86;&#xFF0C;        </span>
            <span class="token comment">#&#x4E4B;&#x524D;&#x5199;&#x8FD9;&#x4E00;&#x6B65;&#x662F;&#x56E0;&#x4E3A;&#x8FD9;&#x4E2A;&#x5B50;&#x8282;&#x70B9;&#x5728;&#x4E0B;&#x4E00;&#x6B21;&#x8FED;&#x4EE3;&#x4E0D;&#x53EF;&#x4EE5;&#x6C42;&#x5BFC;&#xFF0C;&#x90A3;&#x4E48;&#x5E94;&#x8BE5;&#x7528;x.retain_grad()&#x8FD9;&#x4E2A;&#x64CD;&#x4F5C;&#xFF0C;</span>
            <span class="token comment">#&#x7136;&#x540E;&#x4E0D;&#x9700;&#x8981;&#x6BCF;&#x6B21;&#x65B0;&#x7684;&#x7684;&#x5F00;&#x59CB;&#x7ED9;x.requires_grad = True</span>
            
            <span class="token comment">#x.retain_grad()</span>
            <span class="token comment">#print(x.retain_grad())</span>
            
            
        <span class="token comment">#print(x)</span>
        <span class="token keyword">return</span> losses <span class="token punctuation">,</span>global_loss_graph 
    
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        x<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
        optimizee<span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span> <span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.1</span> <span class="token punctuation">)</span>
        
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>unroll_train_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
            
            optimizee<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            loss <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            global_loss_graph <span class="token operator">+=</span> loss
            
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span>retain_graph_flag<span class="token punctuation">)</span>
            optimizee<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>detach_<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment">#print(x)</span>
        <span class="token keyword">return</span> losses<span class="token punctuation">,</span>global_loss_graph 

</pre><h3 class="mume-header" id="%E5%AF%B9%E6%AF%94%E4%B8%8D%E5%90%8C%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E4%BC%98%E5%8C%96%E6%95%88%E6%9E%9C">&#x5BF9;&#x6BD4;&#x4E0D;&#x540C;&#x4F18;&#x5316;&#x5668;&#x7684;&#x4F18;&#x5316;&#x6548;&#x679C;</h3>

<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token keyword">import</span> matplotlib
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token operator">%</span>matplotlib inline
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

T <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>TRAINING_STEPS<span class="token punctuation">)</span>
<span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> 
    
    sgd_losses<span class="token punctuation">,</span> sgd_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>SGD<span class="token punctuation">,</span>TRAINING_STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    rms_losses<span class="token punctuation">,</span> rms_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>RMS<span class="token punctuation">,</span>TRAINING_STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    adam_losses<span class="token punctuation">,</span> adam_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>Adam<span class="token punctuation">,</span>TRAINING_STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    p1<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>T<span class="token punctuation">,</span> sgd_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;SGD&apos;</span><span class="token punctuation">)</span>
    p2<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>T<span class="token punctuation">,</span> rms_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;RMS&apos;</span><span class="token punctuation">)</span>
    p3<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>T<span class="token punctuation">,</span> adam_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;Adam&apos;</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>handles<span class="token operator">=</span><span class="token punctuation">[</span>p1<span class="token punctuation">,</span> p2<span class="token punctuation">,</span> p3<span class="token punctuation">]</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">&apos;Losses&apos;</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;sum_loss:sgd={},rms={},adam={}&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>sgd_sum_loss<span class="token punctuation">,</span>rms_sum_loss<span class="token punctuation">,</span>adam_sum_loss <span class="token punctuation">)</span><span class="token punctuation">)</span>
</pre><p><img src="https://img-blog.csdnimg.cn/20181123201209548.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="100"></p>
<pre class="language-text">sum_loss:sgd=289.9213562011719,rms=60.56287384033203,adam=117.2123031616211
</pre>
<p>&#x901A;&#x8FC7;&#x4E0A;&#x8FF0;&#x5B9E;&#x9A8C;&#x53EF;&#x4EE5;&#x53D1;&#x73B0;&#xFF0C;&#x8FD9;&#x4E9B;&#x4F18;&#x5316;&#x5668;&#x90FD;&#x53EF;&#x4EE5;&#x53D1;&#x6325;&#x4F5C;&#x7528;&#xFF0C;&#x4F3C;&#x4E4E;RMS&#x8868;&#x73B0;&#x66F4;&#x52A0;&#x4F18;&#x8D8A;&#x4E00;&#x4E9B;&#xFF0C;&#x4E0D;&#x8FC7;&#x8FD9;&#x5E76;&#x4E0D;&#x4EE3;&#x8868;RMS&#x5C31;&#x6BD4;&#x5176;&#x4ED6;&#x7684;&#x597D;,&#x53EF;&#x80FD;&#x8FD9;&#x4E2A;&#x4F18;&#x5316;&#x95EE;&#x9898;&#x8FD8;&#x662F;&#x8F83;&#x4E3A;&#x7B80;&#x5355;&#xFF0C;&#x8C03;&#x6574;&#x8981;&#x4F18;&#x5316;&#x7684;&#x51FD;&#x6570;&#xFF0C;&#x53EF;&#x80FD;&#x5C31;&#x4F1A;&#x770B;&#x5230;&#x4E0D;&#x540C;&#x7684;&#x7ED3;&#x679C;&#x3002;</p>
<h2 class="mume-header" id="meta-optimizer-%E4%BB%8E%E6%89%8B%E5%B7%A5%E8%AE%BE%E8%AE%A1%E4%BC%98%E5%8C%96%E5%99%A8%E8%BF%88%E6%AD%A5%E5%88%B0%E8%87%AA%E5%8A%A8%E8%AE%BE%E8%AE%A1%E4%BC%98%E5%8C%96%E5%99%A8">Meta-optimizer &#xFF1A;&#x4ECE;&#x624B;&#x5DE5;&#x8BBE;&#x8BA1;&#x4F18;&#x5316;&#x5668;&#x8FC8;&#x6B65;&#x5230;&#x81EA;&#x52A8;&#x8BBE;&#x8BA1;&#x4F18;&#x5316;&#x5668;</h2>

<p>&#x4E0A;&#x8FF0;&#x8FD9;&#x4E9B;&#x4F18;&#x5316;&#x5668;&#x7684;&#x66F4;&#x65B0;&#x7B56;&#x7565;&#x662F;&#x6839;&#x636E;&#x4EBA;&#x7684;&#x7ECF;&#x9A8C;&#x4E3B;&#x89C2;&#x8BBE;&#x8BA1;&#xFF0C;&#x8981;&#x6765;&#x89E3;&#x51B3;&#x4E00;&#x822C;&#x7684;&#x4F18;&#x5316;&#x95EE;&#x9898;&#x7684;&#x3002;</p>
<p><strong>No Free Lunch Theorems for Optimization</strong> [Wolpert and Macready, 1997] &#x8868;&#x660E;&#x7EC4;&#x5408;&#x4F18;&#x5316;&#x8BBE;&#x7F6E;&#x4E0B;&#xFF0C;<mark>&#x6CA1;&#x6709;&#x4E00;&#x4E2A;&#x7B97;&#x6CD5;&#x53EF;&#x4EE5;&#x7EDD;&#x5BF9;&#x597D;&#x8FC7;&#x4E00;&#x4E2A;&#x968F;&#x673A;&#x7B56;&#x7565;</mark>&#x3002;&#x8FD9;&#x6697;&#x793A;&#xFF0C;&#x4E00;&#x822C;&#x6765;&#x8BB2;&#xFF0C;&#x5BF9;&#x4E8E;&#x4E00;&#x4E2A;&#x5B50;&#x95EE;&#x9898;&#xFF0C;&#x7279;&#x6B8A;&#x5316;&#x5176;&#x4F18;&#x5316;&#x65B9;&#x6CD5;&#x662F;&#x63D0;&#x5347;&#x6027;&#x80FD;&#x7684;&#x552F;&#x4E00;&#x65B9;&#x6CD5;&#x3002;</p>
<p>&#x800C;&#x9488;&#x5BF9;&#x4E00;&#x4E2A;&#x7279;&#x5B9A;&#x7684;&#x4F18;&#x5316;&#x95EE;&#x9898;&#xFF0C;&#x4E5F;&#x8BB8;&#x4E00;&#x4E2A;&#x7279;&#x5B9A;&#x7684;&#x4F18;&#x5316;&#x5668;&#x80FD;&#x591F;&#x66F4;&#x597D;&#x7684;&#x4F18;&#x5316;&#x5B83;&#xFF0C;&#x6211;&#x4EEC;&#x662F;&#x5426;&#x53EF;&#x4EE5;&#x4E0D;&#x6839;&#x636E;&#x4EBA;&#x5DE5;&#x8BBE;&#x8BA1;&#xFF0C;&#x800C;&#x662F;&#x8BA9;&#x4F18;&#x5316;&#x5668;&#x672C;&#x8EAB;&#x6839;&#x636E;&#x6A21;&#x578B;&#x4E0E;&#x6570;&#x636E;&#xFF0C;&#x81EA;&#x9002;&#x5E94;&#x5730;&#x8C03;&#x8282;&#xFF0C;&#x8FD9;&#x5C31;&#x6D89;&#x53CA;&#x5230;&#x4E86;meta-learning</p>
<h3 class="mume-header" id="%E7%94%A8%E4%B8%80%E4%B8%AA%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99%E6%9B%BF%E4%BB%A3%E6%89%8B%E5%B7%A5%E8%AE%BE%E8%AE%A1%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99">&#x7528;&#x4E00;&#x4E2A;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x68AF;&#x5EA6;&#x66F4;&#x65B0;&#x89C4;&#x5219;&#xFF0C;&#x66FF;&#x4EE3;&#x624B;&#x5DE5;&#x8BBE;&#x8BA1;&#x7684;&#x68AF;&#x5EA6;&#x66F4;&#x65B0;&#x89C4;&#x5219;</h3>

<p></p><div class="mathjax-exps">$$\theta_{t+1}=\theta_{t}+g\textit{}_{t}\left (f\left ( \theta_{t}\right ),\phi \right)$$</div><p></p>
<p>&#x8FD9;&#x91CC;&#x7684;<span class="mathjax-exps">$g(\cdot)$</span>&#x4EE3;&#x8868;&#x5176;&#x68AF;&#x5EA6;&#x66F4;&#x65B0;&#x89C4;&#x5219;&#x51FD;&#x6570;&#xFF0C;&#x901A;&#x8FC7;&#x53C2;&#x6570;<span class="mathjax-exps">$\phi$</span>&#x6765;&#x786E;&#x5B9A;&#xFF0C;&#x5176;&#x8F93;&#x51FA;&#x4E3A;&#x76EE;&#x6807;&#x51FD;&#x6570;f&#x5F53;&#x524D;&#x8FED;&#x4EE3;&#x7684;&#x66F4;&#x65B0;&#x68AF;&#x5EA6;&#x503C;&#xFF0C;<span class="mathjax-exps">$g$</span>&#x51FD;&#x6570;&#x901A;&#x8FC7;RNN&#x6A21;&#x578B;&#x6765;&#x8868;&#x793A;&#xFF0C;&#x4FDD;&#x6301;&#x72B6;&#x6001;&#x5E76;&#x52A8;&#x6001;&#x8FED;&#x4EE3;</p>
<p>&#x5047;&#x5982;&#x4E00;&#x4E2A;&#x4F18;&#x5316;&#x5668;&#x53EF;&#x4EE5;&#x6839;&#x636E;&#x5386;&#x53F2;&#x4F18;&#x5316;&#x7684;&#x7ECF;&#x9A8C;&#x6765;&#x81EA;&#x8EAB;&#x8C03;&#x89E3;&#x81EA;&#x5DF1;&#x7684;&#x4F18;&#x5316;&#x7B56;&#x7565;&#xFF0C;&#x90A3;&#x4E48;&#x5C31;&#x4E00;&#x5B9A;&#x7A0B;&#x5EA6;&#x4E0A;&#x505A;&#x5230;&#x4E86;&#x81EA;&#x9002;&#x5E94;&#xFF0C;&#x8FD9;&#x4E2A;&#x4E0D;&#x662F;&#x8BF4;&#x50CF;Adam&#xFF0C;momentum&#xFF0C;RMSprop&#x90A3;&#x6837;&#x81EA;&#x9002;&#x5E94;&#x5730;&#x6839;&#x636E;&#x68AF;&#x5EA6;&#x8C03;&#x8282;&#x5B66;&#x4E60;&#x7387;&#xFF0C;&#xFF08;&#x5176;&#x68AF;&#x5EA6;&#x66F4;&#x65B0;&#x89C4;&#x5219;&#x8FD8;&#x662F;&#x4E0D;&#x53D8;&#x7684;&#xFF09;&#xFF0C;&#x800C;&#x662F;&#x8BF4;&#x81EA;&#x9002;&#x5E94;&#x5730;&#x6539;&#x53D8;&#x5176;&#x68AF;&#x5EA6;&#x66F4;&#x65B0;&#x89C4;&#x5219;&#xFF0C;&#x800C;Learning to learn &#x8FD9;&#x7BC7;&#x8BBA;&#x6587;&#x5C31;&#x4F7F;&#x7528;LSTM&#xFF08;RNN&#xFF09;&#x4F18;&#x5316;&#x5668;&#x505A;&#x5230;&#x4E86;&#x8FD9;&#x4E00;&#x70B9;&#xFF0C;&#x6BD5;&#x7ADF;RNN&#x5B58;&#x5728;&#x4E00;&#x4E2A;&#x53EF;&#x4EE5;&#x4FDD;&#x5B58;&#x5386;&#x53F2;&#x4FE1;&#x606F;&#x7684;&#x9690;&#x72B6;&#x6001;&#xFF0C;LSTM&#x53EF;&#x4EE5;&#x4ECE;&#x4E00;&#x4E2A;&#x5386;&#x53F2;&#x7684;&#x5168;&#x5C40;&#x53BB;&#x9002;&#x5E94;&#x8FD9;&#x4E2A;&#x7279;&#x5B9A;&#x7684;&#x4F18;&#x5316;&#x8FC7;&#x7A0B;&#xFF0C;&#x505A;&#x5230;&#x8BBA;&#x6587;&#x63D0;&#x5230;&#x7684;&#x6240;&#x8C13;&#x7684;&#x201C;CoordinateWise&#x201D;&#xFF0C;&#x6211;&#x7684;&#x7406;&#x89E3;&#x662F;&#xFF1A;LSTM&#x7684;&#x53C2;&#x6570;&#x5BF9;&#x6BCF;&#x4E2A;&#x65F6;&#x523B;&#x8282;&#x70B9;&#x90FD;&#x4FDD;&#x6301;&#x201C;&#x806A;&#x660E;&#x201D;&#xFF0C;&#x662F;&#x4E00;&#x79CD;&#x201C;&#x5168;&#x5C40;&#x6027;&#x7684;&#x806A;&#x660E;&#x201D;&#xFF0C;&#x9002;&#x5E94;&#x6BCF;&#x5206;&#x6BCF;&#x79D2;&#x3002;</p>
<h4 class="mume-header" id="%E6%9E%84%E5%BB%BAlstm%E4%BC%98%E5%8C%96%E5%99%A8">&#x6784;&#x5EFA;LSTM&#x4F18;&#x5316;&#x5668;</h4>

<pre data-role="codeBlock" data-info="python" class="language-python">Layers <span class="token operator">=</span> <span class="token number">2</span>
Hidden_nums <span class="token operator">=</span> <span class="token number">20</span>
Input_DIM <span class="token operator">=</span> DIM
Output_DIM <span class="token operator">=</span> DIM
<span class="token comment"># &quot;coordinate-wise&quot; RNN </span>
lstm<span class="token operator">=</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>Input_DIM<span class="token punctuation">,</span>Hidden_nums <span class="token punctuation">,</span>Layers<span class="token punctuation">)</span>
Linear <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>Hidden_nums<span class="token punctuation">,</span>Output_DIM<span class="token punctuation">)</span>
batchsize <span class="token operator">=</span> <span class="token number">1</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>lstm<span class="token punctuation">)</span>
    
<span class="token keyword">def</span> <span class="token function">LSTM_Optimizee</span><span class="token punctuation">(</span>gradients<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment">#LSTM&#x7684;&#x8F93;&#x5165;&#x4E3A;&#x68AF;&#x5EA6;&#xFF0C;pytorch&#x8981;&#x6C42;torch.nn.lstm&#x7684;&#x8F93;&#x5165;&#x4E3A;&#xFF08;1&#xFF0C;batchsize,input_dim&#xFF09;</span>
    <span class="token comment">#&#x539F;gradient.size()=torch.size[5] -&gt;[1,1,5]</span>
    gradients <span class="token operator">=</span> gradients<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>   
    <span class="token keyword">if</span> state <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        state <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>Layers<span class="token punctuation">,</span>batchsize<span class="token punctuation">,</span>Hidden_nums<span class="token punctuation">)</span><span class="token punctuation">,</span>
                 torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>Layers<span class="token punctuation">,</span>batchsize<span class="token punctuation">,</span>Hidden_nums<span class="token punctuation">)</span><span class="token punctuation">)</span>
   
    update<span class="token punctuation">,</span> state <span class="token operator">=</span> lstm<span class="token punctuation">(</span>gradients<span class="token punctuation">,</span> state<span class="token punctuation">)</span> <span class="token comment"># &#x7528;optimizee_lstm&#x4EE3;&#x66FF; lstm</span>
    update <span class="token operator">=</span> Linear<span class="token punctuation">(</span>update<span class="token punctuation">)</span>
    <span class="token comment"># Squeeze to make it a single batch again.[1,1,5]-&gt;[5]</span>
    <span class="token keyword">return</span> update<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> state
</pre><pre class="language-text">LSTM(10, 20, num_layers=2)
</pre>
<p>&#x4ECE;&#x4E0A;&#x9762;LSTM&#x4F18;&#x5316;&#x5668;&#x7684;&#x8BBE;&#x8BA1;&#x6765;&#x770B;&#xFF0C;&#x6211;&#x4EEC;&#x51E0;&#x4E4E;&#x6CA1;&#x6709;&#x52A0;&#x5165;&#x4EFB;&#x4F55;&#x5148;&#x9A8C;&#x7684;&#x4EBA;&#x4E3A;&#x7ECF;&#x9A8C;&#x5728;&#x91CC;&#x9762;&#xFF0C;&#x53EA;&#x662F;&#x7528;&#x4E86;&#x957F;&#x77ED;&#x671F;&#x8BB0;&#x5FC6;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x67B6;&#x6784;</p>
<h4 class="mume-header" id="%E4%BC%98%E5%8C%96%E5%99%A8%E6%9C%AC%E8%BA%AB%E7%9A%84%E5%8F%82%E6%95%B0%E5%8D%B3lstm%E7%9A%84%E5%8F%82%E6%95%B0%E4%BB%A3%E8%A1%A8%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E6%9B%B4%E6%96%B0%E7%AD%96%E7%95%A5">&#x4F18;&#x5316;&#x5668;&#x672C;&#x8EAB;&#x7684;&#x53C2;&#x6570;&#x5373;LSTM&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x4EE3;&#x8868;&#x4E86;&#x6211;&#x4EEC;&#x7684;&#x66F4;&#x65B0;&#x7B56;&#x7565;</h4>

<p><strong>&#x8FD9;&#x4E2A;&#x4F18;&#x5316;&#x5668;&#x7684;&#x53C2;&#x6570;&#x4EE3;&#x8868;&#x4E86;&#x6211;&#x4EEC;&#x7684;&#x66F4;&#x65B0;&#x7B56;&#x7565;&#xFF0C;&#x540E;&#x9762;&#x6211;&#x4EEC;&#x4F1A;&#x5B66;&#x4E60;&#x8FD9;&#x4E2A;&#x53C2;&#x6570;&#xFF0C;&#x5373;&#x5B66;&#x4E60;&#x7528;&#x4EC0;&#x4E48;&#x6837;&#x7684;&#x66F4;&#x65B0;&#x7B56;&#x7565;</strong></p>
<p>&#x5BF9;&#x4E86;&#x5982;&#x679C;&#x4F60;&#x4E0D;&#x592A;&#x4E86;&#x89E3;LSTM&#x7684;&#x8BDD;&#xFF0C;&#x6211;&#x5C31;&#x653E;&#x8FD9;&#x4E2A;&#x7F51;&#x7AD9; <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a> &#x535A;&#x5BA2;&#x7684;&#x51E0;&#x4E2A;&#x56FE;&#xFF0C;&#x5B83;&#x5F88;&#x597D;&#x89E3;&#x91CA;&#x4E86;&#x4EC0;&#x4E48;&#x662F;RNN&#x548C;LSTM&#xFF1A;</p>
<center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" width="600"></center>
<center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png" width="600"> </center>
<center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" width="600"> </center>
<center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" width="600"> </center>
<center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" width="600"> </center>
<center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" width="600"> </center>
<h5 class="mume-header" id="%E5%A5%BD%E4%BA%86%E7%9C%8B%E4%B8%80%E4%B8%8B%E6%88%91%E4%BB%AC%E4%BD%BF%E7%94%A8%E5%88%9A%E5%88%9A%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84lstm%E4%BC%98%E5%8C%96%E5%99%A8%E5%90%8E%E7%9A%84%E4%BC%98%E5%8C%96%E7%BB%93%E6%9E%9C">&#x597D;&#x4E86;&#xFF0C;&#x770B;&#x4E00;&#x4E0B;&#x6211;&#x4EEC;&#x4F7F;&#x7528;&#x521A;&#x521A;&#x521D;&#x59CB;&#x5316;&#x7684;LSTM&#x4F18;&#x5316;&#x5668;&#x540E;&#x7684;&#x4F18;&#x5316;&#x7ED3;&#x679C;</h5>

<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token keyword">import</span> matplotlib
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token operator">%</span>matplotlib inline
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>TRAINING_STEPS<span class="token punctuation">)</span>

    
<span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> 
    
    sgd_losses<span class="token punctuation">,</span> sgd_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>SGD<span class="token punctuation">,</span>TRAINING_STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    rms_losses<span class="token punctuation">,</span> rms_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>RMS<span class="token punctuation">,</span>TRAINING_STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    adam_losses<span class="token punctuation">,</span> adam_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>Adam<span class="token punctuation">,</span>TRAINING_STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    lstm_losses<span class="token punctuation">,</span>lstm_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>LSTM_Optimizee<span class="token punctuation">,</span>TRAINING_STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>retain_graph_flag <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
    p1<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>T<span class="token punctuation">,</span> sgd_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;SGD&apos;</span><span class="token punctuation">)</span>
    p2<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>T<span class="token punctuation">,</span> rms_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;RMS&apos;</span><span class="token punctuation">)</span>
    p3<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>T<span class="token punctuation">,</span> adam_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;Adam&apos;</span><span class="token punctuation">)</span>
    p4<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> lstm_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;LSTM&apos;</span><span class="token punctuation">)</span>
    p1<span class="token punctuation">.</span>set_dashes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 2pt line, 2pt break, 10pt line, 2pt break</span>
    p2<span class="token punctuation">.</span>set_dashes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 2pt line, 2pt break, 10pt line, 2pt break</span>
    p3<span class="token punctuation">.</span>set_dashes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 2pt line, 2pt break, 10pt line, 2pt break</span>
    <span class="token comment">#plt.yscale(&apos;log&apos;)</span>
    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>handles<span class="token operator">=</span><span class="token punctuation">[</span>p1<span class="token punctuation">,</span> p2<span class="token punctuation">,</span> p3<span class="token punctuation">,</span> p4<span class="token punctuation">]</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">&apos;Losses&apos;</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;sum_loss:sgd={},rms={},adam={},lstm={}&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>sgd_sum_loss<span class="token punctuation">,</span>rms_sum_loss<span class="token punctuation">,</span>adam_sum_loss<span class="token punctuation">,</span>lstm_sum_loss <span class="token punctuation">)</span><span class="token punctuation">)</span>
</pre><p><img src="https://img-blog.csdnimg.cn/20181123201313760.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="&#x5728;&#x8FD9;&#x91CC;&#x63D2;&#x5165;&#x56FE;&#x7247;&#x63CF;&#x8FF0;"></p>
<pre class="language-text">sum_loss:sgd=289.9213562011719,rms=60.56287384033203,adam=117.2123031616211,lstm=554.2158203125
</pre>
<h5 class="mume-header" id="%E5%92%A6%E4%B8%BA%E4%BB%80%E4%B9%88lstm%E4%BC%98%E5%8C%96%E5%99%A8%E9%82%A3%E4%B9%88%E5%B7%AE%E6%A0%B9%E6%9C%AC%E6%B2%A1%E6%9C%89%E4%BC%98%E5%8C%96%E6%95%88%E6%9E%9C">&#x54A6;&#xFF0C;&#x4E3A;&#x4EC0;&#x4E48;LSTM&#x4F18;&#x5316;&#x5668;&#x90A3;&#x4E48;&#x5DEE;&#xFF0C;&#x6839;&#x672C;&#x6CA1;&#x6709;&#x4F18;&#x5316;&#x6548;&#x679C;&#xFF1F;</h5>

<p><strong>&#x5148;&#x522B;&#x7740;&#x6025;&#x8D28;&#x7591;&#xFF01;&#x56E0;&#x4E3A;&#x6211;&#x4EEC;&#x8FD8;&#x6CA1;&#x6709;&#x5B66;&#x4E60;LSTM&#x4F18;&#x5316;&#x5668;&#xFF01;</strong></p>
<p>&#x7528;&#x5230;&#x7684;LSTM&#x6A21;&#x578B;&#x5B8C;&#x5168;&#x662F;&#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x7684;&#xFF01;&#x5E76;&#x4E14;LSTM&#x7684;&#x53C2;&#x6570;&#x5728;TRAIN_STEPS=[0,T]&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x8282;&#x70B9;&#x90FD;&#x662F;&#x4FDD;&#x6301;&#x4E0D;&#x53D8;&#x7684;&#xFF01;</p>
<h4 class="mume-header" id="%E4%B8%8B%E9%9D%A2%E6%88%91%E4%BB%AC%E5%B0%B1%E6%9D%A5%E4%BC%98%E5%8C%96lstm%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E5%8F%82%E6%95%B0">&#x4E0B;&#x9762;&#x6211;&#x4EEC;&#x5C31;&#x6765;&#x4F18;&#x5316;LSTM&#x4F18;&#x5316;&#x5668;&#x7684;&#x53C2;&#x6570;&#xFF01;</h4>

<p>&#x4E0D;&#x8BBA;&#x662F;&#x539F;&#x59CB;&#x4F18;&#x5316;&#x95EE;&#x9898;&#xFF0C;&#x8FD8;&#x662F;&#x96B6;&#x5C5E;&#x5143;&#x5B66;&#x4E60;&#x7684;LSTM&#x4F18;&#x5316;&#x76EE;&#x6807;&#xFF0C;&#x6211;&#x4EEC;&#x90FD;&#x4E00;&#x4E2A;&#x5171;&#x540C;&#x7684;&#x5B66;&#x4E60;&#x76EE;&#x6807;&#xFF1A;</p>
<p></p><div class="mathjax-exps">$$\theta ^{*}= \arg\min_{\theta\in \Theta }f\left ( \theta  \right )$$</div><p></p>
<p>&#x6216;&#x8005;&#x8BF4;&#x6211;&#x4EEC;&#x5E0C;&#x671B;&#x8FED;&#x4EE3;&#x540E;&#x7684;loss&#x503C;&#x53D8;&#x5F97;&#x5F88;&#x5C0F;&#xFF0C;&#x4F20;&#x7EDF;&#x65B9;&#x6CD5;&#xFF0C;&#x662F;&#x57FA;&#x4E8E;&#x6BCF;&#x4E2A;&#x8FED;&#x4EE3;&#x5468;&#x671F;&#xFF0C;&#x4E00;&#x6B65;&#x4E00;&#x6B65;&#xFF0C;&#x8BA9;loss&#x503C;&#x53D8;&#x5C0F;&#xFF0C;&#x53EF;&#x4EE5;&#x8BF4;&#xFF0C;&#x4F20;&#x7EDF;&#x4F18;&#x5316;&#x5668;&#x8FDB;&#x884C;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x65F6;&#x6240;&#x7AD9;&#x7684;&#x89C6;&#x89D2;&#x662F;&#x5728;&#x67D0;&#x4E2A;&#x5468;&#x671F;&#x4E0B;&#x7684;&#xFF0C;&#x90A3;&#x4E48;&#xFF0C;&#x6211;&#x4EEC;&#x5176;&#x5B9E;&#x53EF;&#x4EE5;&#x6362;&#x4E00;&#x4E2A;&#x89C6;&#x89D2;&#xFF0C;&#x66F4;&#x5168;&#x5C40;&#x7684;&#x89C6;&#x89D2;&#xFF0C;&#x5373;&#xFF0C;&#x6211;&#x4EEC;&#x5E0C;&#x671B;&#x6240;&#x6709;&#x5468;&#x671F;&#x8FED;&#x4EE3;&#x7684;loss&#x503C;&#x90FD;&#x5F88;&#x5C0F;&#xFF0C;&#x8FD9;&#x548C;&#x4F20;&#x7EDF;&#x4F18;&#x5316;&#x662F;&#x4E0D;&#x8FDD;&#x80CC;&#x7684;&#xFF0C;&#x5E76;&#x4E14;&#x662F;&#x5168;&#x5C40;&#x7684;&#xFF0C;&#x8FD9;&#x91CC;&#x505A;&#x4E2A;&#x6BD4;&#x55BB;&#xFF0C;&#x4F18;&#x5316;&#x5C31;&#x50CF;&#x662F;&#x4E0B;&#x68CB;&#xFF0C;&#x4F18;&#x5316;&#x5668;&#x5C31;&#x662F;</p>
<h5 class="mume-header" id="%E4%B8%8B%E6%A3%8B%E6%89%8B">&#x201C;&#x4E0B;&#x68CB;&#x624B; &#x201D;</h5>

<p>&#x5982;&#x679C;&#x4E00;&#x4E2A;&#x68CB;&#x624B;&#xFF0C;&#x5728;&#x6BCF;&#x8D70;&#x4E00;&#x6B65;&#x4E4B;&#x524D;&#xFF0C;&#x90FD;&#x80FD;&#x770B;&#x672A;&#x6765;&#x5F88;&#x591A;&#x6B65;&#x88AB;&#x8FD9;&#x4E00;&#x6B65;&#x7684;&#x5F71;&#x54CD;&#xFF0C;&#x90A3;&#x4E48;&#x5B83;&#x5C31;&#x80FD;&#x5728;&#x5F53;&#x524D;&#x6B65;&#x505A;&#x51FA;&#x6700;&#x4F73;&#x7B56;&#x7565;&#xFF0C;&#x800C;LSTM&#x7684;&#x4F18;&#x5316;&#x8FC7;&#x7A0B;&#xFF0C;&#x5C31;&#x662F;&#x628A;&#x4E00;&#x4E2A;&#x5386;&#x53F2;&#x5168;&#x5C40;&#x7684;&#x201C;&#x6B65;&#x201D;&#x653E;&#x5728;&#x4E00;&#x8D77;&#x8FDB;&#x884C;&#x4F18;&#x5316;&#xFF0C;&#x6240;&#x4EE5;LSTM&#x7684;&#x4F18;&#x5316;&#x5C31;&#x5177;&#x5907;&#x4E86;&#x201C;&#x77BB;&#x524D;&#x987E;&#x540E;&#x201D;&#x7684;&#x80FD;&#x529B;&#xFF01;</p>
<p>&#x5173;&#x4E8E;&#x8FD9;&#x4E00;&#x70B9;&#xFF0C;&#x8BBA;&#x6587;&#x7ED9;&#x51FA;&#x4E86;&#x4E00;&#x4E2A;&#x671F;&#x671B;loss&#x7684;&#x5B9A;&#x4E49;&#xFF1A;<br>
</p><div class="mathjax-exps">$$L\left(\phi \right) =E_f \left[ f \left ( \theta ^{*}\left ( f,\phi  \right )\right ) \right]$$</div><p></p>
<p>&#x4F46;&#x8FD9;&#x4E2A;&#x5B9E;&#x73B0;&#x8D77;&#x6765;&#x5E76;&#x4E0D;&#x73B0;&#x5B9E;&#xFF0C;&#x6211;&#x4EEC;&#x53EA;&#x9700;&#x8981;&#x5C06;&#x5176;&#x601D;&#x60F3;&#x5177;&#x4F53;&#x5316;&#x3002;</p>
<ul>
<li>
<p>Meta-optimizer&#x4F18;&#x5316;&#xFF1A;&#x76EE;&#x6807;&#x51FD;&#x6570;&#x201C;&#x6240;&#x6709;&#x5468;&#x671F;&#x7684;loss&#x90FD;&#x8981;&#x5F88;&#x5C0F;&#xFF01;&#x201D;&#xFF0C;&#x800C;&#x4E14;&#x8FD9;&#x4E2A;&#x76EE;&#x6807;&#x51FD;&#x6570;&#x662F;&#x72EC;&#x7ACB;&#x540C;&#x5206;&#x5E03;&#x91C7;&#x6837;&#x7684;&#xFF08;&#x6BD4;&#x5982;&#xFF0C;&#x8FD9;&#x91CC;&#x610F;&#x5473;&#x7740;&#x4EFB;&#x610F;&#x521D;&#x59CB;&#x5316;&#x4E00;&#x4E2A;&#x4F18;&#x5316;&#x95EE;&#x9898;&#x6A21;&#x578B;&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x6211;&#x4EEC;&#x90FD;&#x5E0C;&#x671B;&#x8FD9;&#x4E2A;&#x4F18;&#x5316;&#x5668;&#x80FD;&#x591F;&#x627E;&#x5230;&#x4E00;&#x4E2A;&#x4F18;&#x5316;&#x95EE;&#x9898;&#x7684;&#x7A33;&#x5B9A;&#x7684;&#x89E3;&#xFF09;</p>
</li>
<li>
<p>&#x4F20;&#x7EDF;&#x4F18;&#x5316;&#x5668;&#xFF1A;&quot;&#x5BF9;&#x4E8E;&#x5F53;&#x524D;&#x7684;&#x76EE;&#x6807;&#x51FD;&#x6570;&#xFF0C;&#x53EA;&#x8981;&#x8FD9;&#x4E00;&#x6B65;&#x7684;loss&#x6BD4;&#x4E0A;&#x4E00;&#x6B65;&#x7684;loss&#x503C;&#x8981;&#x5C0F;&#x5C31;&#x884C;&#x201D;</p>
</li>
</ul>
<h5 class="mume-header" id="%E7%89%B9%E7%82%B9-2%E8%80%83%E8%99%91%E4%BC%98%E5%8C%96%E5%99%A8%E4%BC%98%E5%8C%96%E8%BF%87%E7%A8%8B%E7%9A%84%E5%8E%86%E5%8F%B2%E5%85%A8%E5%B1%80%E6%80%A7%E4%BF%A1%E6%81%AF-3%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83%E5%9C%B0%E9%87%87%E6%A0%B7%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%82%E6%95%B0">&#x7279;&#x70B9; &#xFF1A; 2.&#x8003;&#x8651;&#x4F18;&#x5316;&#x5668;&#x4F18;&#x5316;&#x8FC7;&#x7A0B;&#x7684;&#x5386;&#x53F2;&#x5168;&#x5C40;&#x6027;&#x4FE1;&#x606F;     3.&#x72EC;&#x7ACB;&#x540C;&#x5206;&#x5E03;&#x5730;&#x91C7;&#x6837;&#x4F18;&#x5316;&#x95EE;&#x9898;&#x76EE;&#x6807;&#x51FD;&#x6570;&#x7684;&#x53C2;&#x6570;</h5>

<p>&#x63A5;&#x4E0B;&#x6765;&#x6211;&#x4EEC;&#x5C31;&#x7AD9;&#x5728;&#x66F4;&#x5168;&#x5C40;&#x7684;&#x89D2;&#x5EA6;&#xFF0C;&#x6765;&#x4F18;&#x5316;LSTM&#x4F18;&#x5316;&#x5668;&#x7684;&#x53C2;&#x6570;</p>
<p>LSTM&#x662F;&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#xFF0C;&#x5B83;&#x53EF;&#x4EE5;&#x8FDE;&#x7EED;&#x8BB0;&#x5F55;&#x5E76;&#x4F20;&#x9012;&#x6240;&#x6709;&#x5468;&#x671F;&#x65F6;&#x523B;&#x7684;&#x4FE1;&#x606F;&#xFF0C;&#x5176;&#x6BCF;&#x4E2A;&#x5468;&#x671F;&#x5FAA;&#x73AF;&#x91CC;&#x7684;&#x5B50;&#x56FE;&#x5171;&#x540C;&#x6784;&#x5EFA;&#x4E00;&#x4E2A;&#x5DE8;&#x5927;&#x7684;&#x56FE;&#xFF0C;&#x7136;&#x540E;&#x4F7F;&#x7528;Back-Propagation Through Time (BPTT)&#x6765;&#x6C42;&#x5BFC;&#x66F4;&#x65B0;</p>
<pre data-role="codeBlock" data-info="python" class="language-python">lstm_losses<span class="token punctuation">,</span>global_graph_loss<span class="token operator">=</span> learn<span class="token punctuation">(</span>LSTM_Optimizee<span class="token punctuation">,</span>TRAINING_STEPS<span class="token punctuation">,</span>retain_graph_flag <span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># [loss1,loss2,...lossT] &#x6240;&#x6709;&#x5468;&#x671F;&#x7684;loss</span>
<span class="token comment"># &#x56E0;&#x4E3A;&#x8FD9;&#x91CC;&#x8981;&#x4FDD;&#x7559;&#x6240;&#x6709;&#x5468;&#x671F;&#x7684;&#x8BA1;&#x7B97;&#x56FE;&#x6240;&#x4EE5;retain_graph_flag =True</span>
all_computing_graph_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>lstm_losses<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> 
<span class="token comment">#&#x6784;&#x5EFA;&#x4E00;&#x4E2A;&#x6240;&#x6709;&#x5468;&#x671F;&#x5B50;&#x56FE;&#x6784;&#x6210;&#x7684;&#x603B;&#x8BA1;&#x7B97;&#x56FE;,&#x4F7F;&#x7528;BPTT&#x6765;&#x68AF;&#x5EA6;&#x66F4;&#x65B0;LSTM&#x53C2;&#x6570;</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>all_computing_graph_loss<span class="token punctuation">,</span>global_graph_loss <span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>global_graph_loss<span class="token punctuation">)</span>
</pre><pre class="language-text">tensor(554.2158) tensor(554.2158, grad_fn=&lt;ThAddBackward&gt;)
tensor(554.2158, grad_fn=&lt;ThAddBackward&gt;)
</pre>
<p>&#x53EF;&#x4EE5;&#x770B;&#x5230;&#xFF0C;&#x53D8;&#x91CF;<strong>global_graph_loss</strong>&#x4FDD;&#x7559;&#x4E86;&#x6240;&#x6709;&#x5468;&#x671F;&#x4EA7;&#x751F;&#x7684;&#x8BA1;&#x7B97;&#x56FE;grad_fn=<thaddbackward></thaddbackward></p>
<p>&#x4E0B;&#x9762;&#x9488;&#x5BF9;LSTM&#x7684;&#x53C2;&#x6570;&#x8FDB;&#x884C;&#x5168;&#x5C40;&#x4F18;&#x5316;,&#x4F18;&#x5316;&#x76EE;&#x6807;&#xFF1A;&#x201C;&#x6240;&#x6709;&#x5468;&#x671F;&#x4E4B;&#x548C;&#x7684;loss&#x90FD;&#x5F88;&#x5C0F;&#x201D;&#x3002;<br>
&#x503C;&#x5F97;&#x8BF4;&#x660E;&#x4E00;&#x4E0B;&#xFF1A;&#x5728;LSTM&#x4F18;&#x5316;&#x65F6;&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x662F;&#x5728;&#x6240;&#x6709;Unroll_TRAIN_STEPS=[0,T]&#x4E2D;&#x4FDD;&#x6301;&#x4E0D;&#x53D8;&#x7684;&#xFF0C;&#x5728;&#x8FDB;&#x884C;&#x5B8C;&#x6240;&#x6709;Unroll_TRAIN_STEPS&#x4EE5;&#x540E;&#xFF0C;&#x518D;&#x6574;&#x4F53;&#x4F18;&#x5316;LSTM&#x7684;&#x53C2;&#x6570;&#x3002;</p>
<p>&#x8FD9;&#x4E5F;&#x5C31;&#x662F;&#x8BBA;&#x6587;&#x91CC;&#x9762;&#x63D0;&#x5230;&#x7684;coordinate-wise&#xFF0C;&#x5373;&#x201C;&#x5BF9;&#x6BCF;&#x4E2A;&#x65F6;&#x523B;&#x70B9;&#x90FD;&#x4FDD;&#x6301;&#x2018;&#x5168;&#x5C40;&#x806A;&#x660E;&#x2019;&#x201D;&#xFF0C;&#x5373;&#x5B66;&#x4E60;&#x5230;LSTM&#x7684;&#x53C2;&#x6570;&#x662F;&#x5168;&#x5C40;&#x6700;&#x4F18;&#x7684;&#x4E86;&#x3002;&#x56E0;&#x4E3A;&#x6211;&#x4EEC;&#x662F;&#x7AD9;&#x5728;&#x6240;&#x6709;TRAIN_STEPS=[0,T]&#x7684;&#x89C6;&#x89D2;&#x4E0B;&#x8FDB;&#x884C;&#x7684;&#x4F18;&#x5316;&#xFF01;</p>
<p>&#x4F18;&#x5316;LSTM&#x4F18;&#x5316;&#x5668;&#x9009;&#x62E9;&#x7684;&#x662F;Adam&#x4F18;&#x5316;&#x5668;&#x8FDB;&#x884C;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;</p>
<h4 class="mume-header" id="%E9%80%9A%E8%BF%87%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%9D%A5%E4%BC%98%E5%8C%96-%E4%BC%98%E5%8C%96%E5%99%A8">&#x901A;&#x8FC7;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x6CD5;&#x6765;&#x4F18;&#x5316;  &#x4F18;&#x5316;&#x5668;</h4>

<pre data-role="codeBlock" data-info="python" class="language-python">Global_Train_Steps <span class="token operator">=</span> <span class="token number">2</span>

<span class="token keyword">def</span> <span class="token function">global_training</span><span class="token punctuation">(</span>optimizee<span class="token punctuation">)</span><span class="token punctuation">:</span>
    global_loss_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    
    adam_global_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>optimizee<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> <span class="token number">0.0001</span><span class="token punctuation">)</span>
    _<span class="token punctuation">,</span>global_loss_1 <span class="token operator">=</span> learn<span class="token punctuation">(</span>LSTM_Optimizee<span class="token punctuation">,</span>TRAINING_STEPS<span class="token punctuation">,</span>retain_graph_flag <span class="token operator">=</span><span class="token boolean">True</span> <span class="token punctuation">,</span>reset_theta <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
    
    <span class="token comment">#print(global_loss_1)</span>
    
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>Global_Train_Steps<span class="token punctuation">)</span><span class="token punctuation">:</span>    
        _<span class="token punctuation">,</span>global_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>LSTM_Optimizee<span class="token punctuation">,</span>TRAINING_STEPS<span class="token punctuation">,</span>retain_graph_flag <span class="token operator">=</span><span class="token boolean">True</span> <span class="token punctuation">,</span>reset_theta <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span>       
        <span class="token comment">#adam_global_optimizer.zero_grad()</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&apos;xxx&apos;</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">(</span>z<span class="token punctuation">.</span>grad<span class="token punctuation">,</span>z<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span> <span class="token keyword">for</span> z <span class="token keyword">in</span> optimizee<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment">#print(i,global_loss)</span>
        global_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#&#x6BCF;&#x6B21;&#x90FD;&#x662F;&#x4F18;&#x5316;&#x8FD9;&#x4E2A;&#x56FA;&#x5B9A;&#x7684;&#x56FE;&#xFF0C;&#x4E0D;&#x53EF;&#x4EE5;&#x91CA;&#x653E;&#x52A8;&#x6001;&#x56FE;&#x7684;&#x7F13;&#x5B58;</span>
        <span class="token comment">#print(&apos;xxx&apos;,[(z.grad,z.requires_grad) for z in optimizee.parameters()  ])</span>
        adam_global_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&apos;xxx&apos;</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">(</span>z<span class="token punctuation">.</span>grad<span class="token punctuation">,</span>z<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span> <span class="token keyword">for</span> z <span class="token keyword">in</span> optimizee<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">]</span><span class="token punctuation">)</span>
        global_loss_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>global_loss<span class="token punctuation">.</span>detach_<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
    <span class="token comment">#print(global_loss)</span>
    <span class="token keyword">return</span> global_loss_list

<span class="token comment"># &#x8981;&#x628A;&#x56FE;&#x653E;&#x8FDB;&#x51FD;&#x6570;&#x4F53;&#x5185;&#xFF0C;&#x76F4;&#x63A5;&#x8D4B;&#x503C;&#x7684;&#x8BDD;&#x56FE;&#x4F1A;&#x4E22;&#x5931;</span>
<span class="token comment"># &#x4F18;&#x5316;optimizee</span>
global_loss_list <span class="token operator">=</span> global_training<span class="token punctuation">(</span>lstm<span class="token punctuation">)</span>

</pre><pre class="language-text">xxx [(None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True)]
xxx [(None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True)]
xxx [(None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True)]
xxx [(None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True)]
</pre>
<h5 class="mume-header" id="%E4%B8%BA%E4%BB%80%E4%B9%88loss%E5%80%BC%E6%B2%A1%E6%9C%89%E6%94%B9%E5%8F%98%E4%B8%BA%E4%BB%80%E4%B9%88lstm%E5%8F%82%E6%95%B0%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8D%E5%AD%98%E5%9C%A8%E7%9A%84">&#x4E3A;&#x4EC0;&#x4E48;loss&#x503C;&#x6CA1;&#x6709;&#x6539;&#x53D8;&#xFF1F;&#x4E3A;&#x4EC0;&#x4E48;LSTM&#x53C2;&#x6570;&#x7684;&#x68AF;&#x5EA6;&#x4E0D;&#x5B58;&#x5728;&#x7684;&#xFF1F;</h5>

<p>&#x901A;&#x8FC7;&#x5206;&#x6790;&#x63A8;&#x7406;&#xFF0C;&#x6211;&#x53D1;&#x73B0;&#x4E86;LSTM&#x53C2;&#x6570;&#x7684;&#x68AF;&#x5EA6;&#x4E3A;None,&#x90A3;&#x4E48;&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x5C31;&#x5B8C;&#x5168;&#x6CA1;&#x6709;&#x66F4;&#x65B0;LSTM&#x7684;&#x53C2;&#x6570;&#xFF01;</p>
<p>&#x4E3A;&#x4EC0;&#x4E48;&#x53C2;&#x6570;&#x7684;&#x68AF;&#x5EA6;&#x4E3A;None&#x5462;&#xFF0C;&#x4F18;&#x5316;&#x5668;&#x5E76;&#x6CA1;&#x6709;&#x66F4;&#x65B0;&#x6307;&#x5B9A;&#x7684;LSTM&#x7684;&#x6A21;&#x578B;&#x53C2;&#x6570;&#xFF0C;&#x4E00;&#x5B9A;&#x662F;&#x4EC0;&#x4E48;&#x5730;&#x65B9;&#x51FA;&#x4E86;&#x95EE;&#x9898;&#xFF0C;&#x6211;&#x60F3;&#x4E86;&#x597D;&#x4E45;&#xFF0C;&#x8FD8;&#x662F;&#x505A;&#x4E00;&#x4E9B;&#x7B80;&#x5355;&#x7684;&#x5B9E;&#x9A8C;&#x6765;&#x627E;&#x4E00;&#x627E;&#x95EE;&#x9898;&#x5427;&#x3002;</p>
<blockquote>
<p>ps: &#x5176;&#x5B9E;&#x5199;&#x4EE3;&#x7801;&#x505A;&#x5B9E;&#x9A8C;&#x7684;&#x8FC7;&#x7A0B;&#xFF0C;&#x4E5F;&#x4F53;&#x73B0;&#x4E86;&#x4EBA;&#x7C7B;&#x672C;&#x8EAB;&#x5B66;&#x4F1A;&#x5B66;&#x4E60;&#x7684;&#x9AD8;&#x7EA7;&#x80FD;&#x529B;&#xFF0C;&#x90A3;&#x5C31;&#x662F;&#xFF1A;&#x901A;&#x8FC7;&#x5B9E;&#x9A8C;&#x6765;&#x5B9E;&#x73B0;&#x60F3;&#x6CD5;&#x65F6;&#xFF0C;&#x5B9E;&#x9A8C;&#x7ED3;&#x679C;&#x5F80;&#x5F80;&#x548C;&#x9884;&#x671F;&#x5DEE;&#x522B;&#x5F88;&#x5927;&#xFF0C;&#x90A3;&#x4E00;&#x5B9A;&#x6709;&#x4EC0;&#x4E48;&#x5730;&#x65B9;&#x51FA;&#x4E86;&#x95EE;&#x9898;&#xFF0C;&#x76F2;&#x76EE;&#x5730;&#x5927;&#x91CF;&#x8BD5;&#x9519;&#x6CD5;&#x53EF;&#x80FD;&#x627E;&#x4E0D;&#x5230;&#x771F;&#x6B63;&#x95EE;&#x9898;&#x6240;&#x5728;&#xFF0C;&#x5982;&#x4F55;&#x627E;&#x5230;&#x95EE;&#x9898;&#x6240;&#x5728;&#x5E76;&#x89E3;&#x51B3;&#xFF0C;&#x5C31;&#x662F;&#x4E00;&#x79CD;&#x5B66;&#x4F1A;&#x5982;&#x4F55;&#x5B66;&#x4E60;&#x7684;&#x80FD;&#x529B;&#xFF0C;&#x4E5F;&#x662F;&#x4E00;&#x79CD;&#x5F3A;&#x5316;&#x5B66;&#x4E60;&#x7684;&#x80FD;&#x529B;&#x3002;&#x8FD9;&#x91CC;&#x6211;&#x91C7;&#x7528;&#x7684;&#x4EBA;&#x7C7B;&#x667A;&#x80FD;&#x662F;&#xFF1A;&#x4EE5;&#x5C0F;&#x89C1;&#x5927;&#x6CD5;&#x3002;<br>
In a word , if we want the machine achieving to AGI,  it must imiate human&apos;s ability of reasoning and finding where the problem is and figuring out how to solve the problem. Meta Learning contains this idea.</p>
</blockquote>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token keyword">import</span> torch
z<span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>z <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
z<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
z<span class="token punctuation">.</span>retain_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>z<span class="token operator">*</span>z<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">[</span>z<span class="token punctuation">]</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
grad <span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
losses<span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
zgrad <span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    q <span class="token operator">=</span> f<span class="token punctuation">(</span>z<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> q<span class="token operator">**</span><span class="token number">2</span>
    <span class="token comment">#z.retain_grad()</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment">#print(x,x.grad,loss,)</span>
    
    loss<span class="token punctuation">.</span>retain_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>q<span class="token punctuation">.</span>grad<span class="token punctuation">,</span>q<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span>
    grad<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>z<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token punctuation">)</span>
    losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
    zgrad<span class="token punctuation">.</span>append<span class="token punctuation">(</span>q<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
    
<span class="token keyword">print</span><span class="token punctuation">(</span>grad<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>losses<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>zgrad<span class="token punctuation">)</span>
</pre><pre class="language-text">None True
None True
[tensor([-44.4396, -36.7740]), tensor([-44.4396, -36.7740])]
[tensor(35.9191, grad_fn=&lt;PowBackward0&gt;), tensor(35.0999, grad_fn=&lt;PowBackward0&gt;)]
[None, None]
</pre>
<h5 class="mume-header" id="%E9%97%AE%E9%A2%98%E5%87%BA%E5%9C%A8%E5%93%AA%E9%87%8C">&#x95EE;&#x9898;&#x51FA;&#x5728;&#x54EA;&#x91CC;&#xFF1F;</h5>

<p>&#x7ECF;&#x8FC7;&#x591A;&#x65B9;&#x9762;&#x7684;&#x5B9E;&#x9A8C;&#x4FEE;&#x6539;&#xFF0C;&#x6211;&#x53D1;&#x73B0;LSTM&#x7684;&#x53C2;&#x6570;&#x5728;&#x6BCF;&#x4E2A;&#x5468;&#x671F;&#x5185;BPTT&#x7684;&#x5468;&#x671F;&#x5185;&#xFF0C;&#x5E76;&#x6CA1;&#x6709;&#x4EA7;&#x751F;&#x68AF;&#x5EA6;&#xFF01;&#xFF01;&#x600E;&#x4E48;&#x56DE;&#x4E8B;&#x5462;&#xFF1F;&#x6211;&#x505A;&#x4E86;&#x4E0A;&#x9762;&#x7684;&#x5C0F;&#x5B9E;&#x9A8C;&#x3002;</p>
<p>&#x53EF;&#x4EE5;&#x770B;&#x5230;z.grad = None,&#x4F46;&#x662F;z.requres_grad = True&#xFF0C;z&#x53D8;&#x91CF;&#x4F5C;&#x4E3A;x&#x53D8;&#x91CF;&#x7684;&#x5B50;&#x8282;&#x70B9;&#xFF0C;&#x5176;&#x5728;&#x8BA1;&#x7B97;&#x56FE;&#x4E2D;&#x7684;&#x68AF;&#x5EA6;&#x6CA1;&#x6709;&#x88AB;&#x4FDD;&#x7559;&#x6216;&#x8005;&#x6CA1;&#x529E;&#x6CD5;&#x83B7;&#x53D6;&#xFF0C;&#x90A3;&#x4E48;&#x6211;&#x5C31;&#x5E94;&#x8BE5;&#x901A;&#x8FC7;&#x4FEE;&#x6539;&#x4E00;&#x4E9B;PyTorch&#x7684;&#x4EE3;&#x7801;&#xFF0C;&#x4F7F;&#x5F97;&#x8BA1;&#x7B97;&#x56FE;&#x4E2D;&#x7684;&#x53F6;&#x5B50;&#x8282;&#x70B9;&#x7684;&#x68AF;&#x5EA6;&#x5F97;&#x4EE5;&#x5B58;&#x5728;&#x3002;&#x7136;&#x540E;&#x6211;&#x627E;&#x5230;&#x4E86;retain_grad()&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#xFF0C;&#x5B9E;&#x9A8C;&#x8BC1;&#x660E;&#xFF0C;&#x5B83;&#x5FC5;&#x987B;&#x5728;backward()&#x4E4B;&#x524D;&#x4F7F;&#x7528;&#x624D;&#x80FD;&#x4FDD;&#x5B58;&#x4E2D;&#x95F4;&#x53F6;&#x5B50;&#x8282;&#x70B9;&#x7684;&#x68AF;&#x5EA6;&#xFF01;&#x8FD9;&#x6837;&#x7684;&#x65B9;&#x6CD5;&#x4E5F;&#x5C31;&#x9002;&#x5408;&#x4E8E;LSTM&#x4F18;&#x5316;&#x5668;&#x6A21;&#x578B;&#x53C2;&#x6570;&#x7684;&#x66F4;&#x65B0;&#x4E86;&#x5427;&#xFF1F;</p>
<p>&#x90A3;&#x4E48;&#x5982;&#x4F55;&#x4FDD;&#x7559;LSTM&#x7684;&#x53C2;&#x6570;&#x5728;&#x6BCF;&#x4E2A;&#x5468;&#x671F;&#x4E2D;&#x4EA7;&#x751F;&#x7684;&#x68AF;&#x5EA6;&#x662F;&#x63A5;&#x4E0B;&#x6765;&#x8981;&#x4FEE;&#x6539;&#x7684;&#xFF01;</p>
<p>&#x8FD9;&#x662F;&#x56E0;&#x4E3A;&#x6211;&#x8BA1;&#x7B97;loss = f(x)&#xFF0C;&#x7136;&#x540E;loss.backward()  &#x8FD9;&#x91CC;&#x7684;loss&#x8BA1;&#x7B97;&#x5E76;&#x6CA1;&#x6709;&#x548C;LSTM&#x4EA7;&#x751F;&#x5173;&#x7CFB;&#xFF0C;&#x6211;&#x5148;&#x6765;&#x60F3;&#x4E00;&#x60F3;loss&#x548C;LSTM&#x7684;&#x5173;&#x7CFB;&#x5728;&#x54EA;&#x91CC;&#xFF1F;</p>
<p>&#x8BBA;&#x6587;&#x91CC;&#x6709;&#x4E00;&#x5F20;&#x56FE;&#xFF0C;&#x53EF;&#x4EE5;&#x4F5C;&#x4E3A;&#x53C2;&#x8003;&#xFF1A;</p>
<center>&#x56FE;2<img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/master/blog/learn-to-learn/graph.PNG" width="600"> </center>
<p><strong>LSTM&#x53C2;&#x6570;&#x7684;&#x68AF;&#x5EA6;&#x6765;&#x81EA;&#x4E8E;&#x6BCF;&#x6B21;&#x8F93;&#x51FA;&#x7684;&#x201C;update&#x201D;&#x7684;&#x68AF;&#x5EA6; update&#x7684;&#x68AF;&#x5EA6;&#x5305;&#x542B;&#x5728;&#x751F;&#x6210;&#x7684;&#x4E0B;&#x4E00;&#x6B21;&#x8FED;&#x4EE3;&#x7684;&#x53C2;&#x6570;x&#x7684;&#x68AF;&#x5EA6;&#x4E2D;</strong></p>
<p>&#x54E6;!&#x56E0;&#x4E3A;&#x53C2;&#x6570;<span class="mathjax-exps">$x_t = x_{t-1}+update_{t-1}$</span> &#x5728;BPTT&#x7684;&#x6BCF;&#x4E2A;&#x5468;&#x671F;&#x91CC;<span class="mathjax-exps">$\frac{\partial loss_t}{\partial \theta_{LSTM}}=\frac{\partial loss_t}{\partial update_{t-1}}*\frac{\partial update_{t-1}}{\partial \theta_{LSTM}}$</span>,&#x90A3;&#x4E48;&#x6211;&#x4EEC;&#x60F3;&#x901A;&#x8FC7;<span class="mathjax-exps">$loss_0,loss_1,..,loss_t$</span>&#x4E4B;&#x548C;&#x6765;&#x66F4;&#x65B0;<span class="mathjax-exps">$\theta_{LSTM}$</span>&#x7684;&#x8BDD;&#xFF0C;&#x5C31;&#x5FC5;&#x987B;&#x8BA9;&#x68AF;&#x5EA6;&#x7ECF;&#x8FC7;<span class="mathjax-exps">$x_t$</span> &#x4E2D; &#x7684;<span class="mathjax-exps">$update_{t-1}$</span>&#x6D41;&#x56DE;&#x53BB;&#xFF0C;&#x90A3;&#x4E48;&#x6BCF;&#x6B21;&#x5F97;&#x5230;&#x7684;<span class="mathjax-exps">$x_t$</span>&#x5C31;&#x5FC5;&#x987B;&#x5305;&#x542B;&#x4E86;&#x4E0A;&#x4E00;&#x6B21;&#x66F4;&#x65B0;&#x4EA7;&#x751F;&#x7684;&#x56FE;&#xFF08;&#x53EF;&#x4EE5;&#x60F3;&#x50CF;&#xFF0C;&#x8FD9;&#x4E2A;&#x8BA1;&#x7B97;&#x56FE;&#x662F;&#x8D8A;&#x6765;&#x8D8A;&#x5927;&#x7684;&#xFF09;&#xFF0C;&#x60F3;&#x4E00;&#x60F3;&#x6211;&#x5199;&#x7684;&#x4EE3;&#x7801;&#xFF0C;&#x4F3C;&#x4E4E;&#x6CA1;&#x6709;&#x4FDD;&#x7559;&#x4E0A;&#x4E00;&#x6B21;&#x7684;&#x8BA1;&#x7B97;&#x56FE;&#x5728;<span class="mathjax-exps">$x_t$</span>&#x8282;&#x70B9;&#x4E2D;&#xFF0C;&#x56E0;&#x4E3A;&#x6211;&#x7528;&#x4E86;x = x.detach_() &#x628A;x&#x4ECE;&#x56FE;&#x4E2D;&#x62FF;&#x4E86;&#x4E0B;&#x6765;&#xFF01;&#x8FD9;&#x4F3C;&#x4E4E;&#x662F;&#x95EE;&#x9898;&#x6700;&#x5173;&#x952E;&#x6240;&#x5728;&#xFF01;&#xFF01;&#xFF01;&#xFF08;&#x800C;tensorflow&#x9759;&#x6001;&#x56FE;&#x7684;&#x6784;&#x5EFA;&#xFF0C;&#x76F4;&#x63A5;&#x5EFA;&#x7ACB;&#x4E86;&#x4E00;&#x4E2A;&#x5B8C;&#x6574;&#x6240;&#x6709;&#x5468;&#x671F;&#x7684;&#x56FE;&#xFF0C;&#x4F3C;&#x4E4E;Pytorch&#x7684;&#x52A8;&#x6001;&#x56FE;&#x4E0D;&#x5408;&#x9002;&#xFF1F;no&#xFF0C;no&#xFF09;</p>
<p>&#xFF08;&#x6CE8;&#xFF1A;&#x4EE5;&#x4E0A;&#x6765;&#x81EA;&#x4EE3;&#x7801;&#x4E2D;&#x7684;<span class="mathjax-exps">$x_t$</span>&#x5BF9;&#x5E94;&#x4E0A;&#x56FE;&#x7684;<span class="mathjax-exps">$\theta_t$</span>&#xFF0C;<span class="mathjax-exps">$update_{t}$</span>&#x5BF9;&#x5E94;&#x4E0A;&#x56FE;&#x7684;<span class="mathjax-exps">$g_t$</span>&#xFF09;</p>
<p>&#x6211;&#x4E3A;&#x4EC0;&#x4E48;&#x4F1A;&#x52A0;&#x5165;x = x.detach_() &#x662F;&#x56E0;&#x4E3A;&#x4E0D;&#x52A0;&#x7684;&#x8BDD;&#xFF0C;x&#x53D8;&#x6210;&#x4E86;&#x5B50;&#x8282;&#x70B9;&#xFF0C;&#x4E0B;&#x4E00;&#x6B21;&#x6C42;&#x5BFC;pytorch&#x4E0D;&#x5141;&#x8BB8;&#xFF0C;&#x5176;&#x5B9E;&#x53EA;&#x9700;&#x8981;&#x52A0;&#x4E00;&#x884C;x.retain_grad()&#x4EE3;&#x7801;&#x5C31;&#x884C;&#x4E86;,&#x5E76;&#x4E14;&#x603B;&#x7684;&#x8BA1;&#x7B97;&#x56FE;&#x7684;globa_graph_loss&#x5728;&#x9010;&#x6B65;&#x964D;&#x4F4E;&#xFF01;&#x95EE;&#x9898;&#x89E3;&#x51B3;&#xFF01;</p>
<p>&#x76EE;&#x524D;&#x5728;&#x8FD0;&#x884C;global_training(lstm)&#x51FD;&#x6570;&#x7684;&#x8BDD;&#xFF0C;&#x5C31;&#x4F1A;&#x53D1;&#x73B0;LSTM&#x7684;&#x53C2;&#x6570;&#x5DF2;&#x7ECF;&#x6839;&#x636E;&#x8BA1;&#x7B97;&#x56FE;&#x4E2D;&#x7684;&#x68AF;&#x5EA6;&#x56DE;&#x6D41;&#x4EA7;&#x751F;&#x4E86;&#x68AF;&#x5EA6;&#xFF0C;&#x6BCF;&#x4E00;&#x6B65;&#x53EF;&#x4EE5;&#x66F4;&#x65B0;&#x53C2;&#x6570;&#x4E86;&#xFF0C;<br>
&#x4F46;&#x662F;&#x8FD9;&#x4E2A;BPTT&#x7B97;&#x6CD5;&#x7528;cpu&#x7B97;&#x8D77;&#x6765;&#xFF0C;&#x6709;&#x70B9;&#x6162;&#x4E86;~</p>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token keyword">def</span> <span class="token function">learn</span><span class="token punctuation">(</span>optimizee<span class="token punctuation">,</span>unroll_train_steps<span class="token punctuation">,</span>retain_graph_flag<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>reset_theta <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span> 
    <span class="token triple-quoted-string string">&quot;&quot;&quot;retain_graph_flag=False   &#x9ED8;&#x8BA4;&#x6BCF;&#x6B21;loss_backward&#x540E; &#x91CA;&#x653E;&#x52A8;&#x6001;&#x56FE;
    #  reset_theta = False     &#x9ED8;&#x8BA4;&#x6BCF;&#x6B21;&#x5B66;&#x4E60;&#x524D; &#x4E0D;&#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x53C2;&#x6570;&quot;&quot;&quot;</span>
    
    <span class="token keyword">if</span> reset_theta <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
        theta_new <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>DIM<span class="token punctuation">)</span>
        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>theta_new<span class="token punctuation">,</span>a<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>b<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span> 
        theta_init_new <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>theta<span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> theta_init_new
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> theta_init
        
    global_loss_graph <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment">#&#x8FD9;&#x4E2A;&#x662F;&#x4E3A;LSTM&#x4F18;&#x5316;&#x5668;&#x6C42;&#x6240;&#x6709;loss&#x76F8;&#x52A0;&#x4EA7;&#x751F;&#x8BA1;&#x7B97;&#x56FE;&#x51C6;&#x5907;&#x7684;</span>
    state <span class="token operator">=</span> <span class="token boolean">None</span>
    x<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
    <span class="token keyword">if</span> optimizee<span class="token punctuation">.</span>__name__ <span class="token operator">!=</span><span class="token string">&apos;Adam&apos;</span><span class="token punctuation">:</span>
        losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>unroll_train_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
            
            loss <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            
            <span class="token comment">#global_loss_graph += torch.exp(torch.Tensor([-i/20]))*loss</span>
            <span class="token comment">#global_loss_graph += (0.8*torch.log10(torch.Tensor([i+1]))+1)*loss</span>
            global_loss_graph <span class="token operator">+=</span> loss
            
            
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span>retain_graph_flag<span class="token punctuation">)</span> <span class="token comment"># &#x9ED8;&#x8BA4;&#x4E3A;False,&#x5F53;&#x4F18;&#x5316;LSTM&#x8BBE;&#x7F6E;&#x4E3A;True</span>
            update<span class="token punctuation">,</span> state <span class="token operator">=</span> optimizee<span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">,</span> state<span class="token punctuation">)</span>
            losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
           
            x <span class="token operator">=</span> x <span class="token operator">+</span> update
            
            <span class="token comment"># x = x.detach_()</span>
            <span class="token comment">#&#x8FD9;&#x4E2A;&#x64CD;&#x4F5C; &#x76F4;&#x63A5;&#x628A;x&#x4E2D;&#x5305;&#x542B;&#x7684;&#x56FE;&#x7ED9;&#x91CA;&#x653E;&#x4E86;&#xFF0C;</span>
            <span class="token comment">#&#x90A3;&#x4F20;&#x9012;&#x7ED9;&#x4E0B;&#x6B21;&#x8BAD;&#x7EC3;&#x7684;x&#x4ECE;&#x5B50;&#x8282;&#x70B9;&#x53D8;&#x6210;&#x4E86;&#x53F6;&#x8282;&#x70B9;&#xFF0C;&#x90A3;&#x4E48;&#x68AF;&#x5EA6;&#x5C31;&#x4E0D;&#x80FD;&#x6CBF;&#x7740;&#x8FD9;&#x4E2A;&#x8DEF;&#x56DE;&#x4F20;&#x4E86;&#xFF0C;        </span>
            <span class="token comment">#&#x4E4B;&#x524D;&#x5199;&#x8FD9;&#x4E00;&#x6B65;&#x662F;&#x56E0;&#x4E3A;&#x8FD9;&#x4E2A;&#x5B50;&#x8282;&#x70B9;&#x5728;&#x4E0B;&#x4E00;&#x6B21;&#x8FED;&#x4EE3;&#x4E0D;&#x53EF;&#x4EE5;&#x6C42;&#x5BFC;&#xFF0C;&#x90A3;&#x4E48;&#x5E94;&#x8BE5;&#x7528;x.retain_grad()&#x8FD9;&#x4E2A;&#x64CD;&#x4F5C;&#xFF0C;</span>
            <span class="token comment">#&#x7136;&#x540E;&#x4E0D;&#x9700;&#x8981;&#x6BCF;&#x6B21;&#x65B0;&#x7684;&#x7684;&#x5F00;&#x59CB;&#x7ED9;x.requires_grad = True</span>
            
            x<span class="token punctuation">.</span>retain_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment">#print(x.retain_grad())</span>
            
            
        <span class="token comment">#print(x)</span>
        <span class="token keyword">return</span> losses <span class="token punctuation">,</span>global_loss_graph 
    
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        x<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
        optimizee<span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span> <span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.1</span> <span class="token punctuation">)</span>
        
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>unroll_train_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
            
            optimizee<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            loss <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            global_loss_graph <span class="token operator">+=</span> loss
            
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span>retain_graph_flag<span class="token punctuation">)</span>
            optimizee<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>detach_<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment">#print(x)</span>
        <span class="token keyword">return</span> losses<span class="token punctuation">,</span>global_loss_graph 
    
Global_Train_Steps <span class="token operator">=</span> <span class="token number">1000</span>

<span class="token keyword">def</span> <span class="token function">global_training</span><span class="token punctuation">(</span>optimizee<span class="token punctuation">)</span><span class="token punctuation">:</span>
    global_loss_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    
    adam_global_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">&apos;params&apos;</span><span class="token punctuation">:</span>optimizee<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">,</span><span class="token punctuation">{</span><span class="token string">&apos;params&apos;</span><span class="token punctuation">:</span>Linear<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">]</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> <span class="token number">0.0001</span><span class="token punctuation">)</span>
    _<span class="token punctuation">,</span>global_loss_1 <span class="token operator">=</span> learn<span class="token punctuation">(</span>LSTM_Optimizee<span class="token punctuation">,</span>TRAINING_STEPS<span class="token punctuation">,</span>retain_graph_flag <span class="token operator">=</span><span class="token boolean">True</span> <span class="token punctuation">,</span>reset_theta <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>global_loss_1<span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>Global_Train_Steps<span class="token punctuation">)</span><span class="token punctuation">:</span>    
        _<span class="token punctuation">,</span>global_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>LSTM_Optimizee<span class="token punctuation">,</span>TRAINING_STEPS<span class="token punctuation">,</span>retain_graph_flag <span class="token operator">=</span><span class="token boolean">True</span> <span class="token punctuation">,</span>reset_theta <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span>       
        adam_global_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token comment">#print(i,global_loss)</span>
        global_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#&#x6BCF;&#x6B21;&#x90FD;&#x662F;&#x4F18;&#x5316;&#x8FD9;&#x4E2A;&#x56FA;&#x5B9A;&#x7684;&#x56FE;&#xFF0C;&#x4E0D;&#x53EF;&#x4EE5;&#x91CA;&#x653E;&#x52A8;&#x6001;&#x56FE;&#x7684;&#x7F13;&#x5B58;</span>
        <span class="token comment">#print(&apos;xxx&apos;,[(z,z.requires_grad) for z in optimizee.parameters()  ])</span>
        adam_global_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">#print(&apos;xxx&apos;,[(z.grad,z.requires_grad) for z in optimizee.parameters()  ])</span>
        global_loss_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>global_loss<span class="token punctuation">.</span>detach_<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
    <span class="token keyword">print</span><span class="token punctuation">(</span>global_loss<span class="token punctuation">)</span>
    <span class="token keyword">return</span> global_loss_list

<span class="token comment"># &#x8981;&#x628A;&#x56FE;&#x653E;&#x8FDB;&#x51FD;&#x6570;&#x4F53;&#x5185;&#xFF0C;&#x76F4;&#x63A5;&#x8D4B;&#x503C;&#x7684;&#x8BDD;&#x56FE;&#x4F1A;&#x4E22;&#x5931;</span>
<span class="token comment"># &#x4F18;&#x5316;optimizee</span>
global_loss_list <span class="token operator">=</span> global_training<span class="token punctuation">(</span>lstm<span class="token punctuation">)</span>
</pre><pre class="language-text">tensor(193.6147, grad_fn=&lt;ThAddBackward&gt;)
tensor(7.6411)
</pre>
<h5 class="mume-header" id="%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8D%E5%86%8D%E4%B8%A2%E5%A4%B1%E4%BA%86lstm%E7%9A%84%E5%8F%82%E6%95%B0%E7%9A%84%E6%A2%AF%E5%BA%A6%E7%BB%8F%E8%BF%87%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%9A%84%E6%B5%81%E5%8A%A8%E5%B7%B2%E7%BB%8F%E4%BA%A7%E7%94%9F%E4%BA%86">&#x8BA1;&#x7B97;&#x56FE;&#x4E0D;&#x518D;&#x4E22;&#x5931;&#x4E86;&#xFF0C;LSTM&#x7684;&#x53C2;&#x6570;&#x7684;&#x68AF;&#x5EA6;&#x7ECF;&#x8FC7;&#x8BA1;&#x7B97;&#x56FE;&#x7684;&#x6D41;&#x52A8;&#x5DF2;&#x7ECF;&#x4EA7;&#x751F;&#x4E86;&#xFF01;</h5>

<pre data-role="codeBlock" data-info="python" class="language-python">Global_T <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>Global_Train_Steps<span class="token punctuation">)</span>

p1<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>Global_T<span class="token punctuation">,</span> global_loss_list<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;Global_graph_loss&apos;</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>handles<span class="token operator">=</span><span class="token punctuation">[</span>p1<span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">&apos;Training LSTM optimizee by gradient descent &apos;</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</pre><p><img src="https://img-blog.csdnimg.cn/2018112320175122.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="&#x5728;&#x8FD9;&#x91CC;&#x63D2;&#x5165;&#x56FE;&#x7247;&#x63CF;&#x8FF0;"></p>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token keyword">import</span> matplotlib
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token operator">%</span>matplotlib inline
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
STEPS <span class="token operator">=</span> <span class="token number">15</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>STEPS<span class="token punctuation">)</span>

    
<span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> 
    
    sgd_losses<span class="token punctuation">,</span> sgd_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>SGD<span class="token punctuation">,</span>STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    rms_losses<span class="token punctuation">,</span> rms_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>RMS<span class="token punctuation">,</span>STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    adam_losses<span class="token punctuation">,</span> adam_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>Adam<span class="token punctuation">,</span>STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    lstm_losses<span class="token punctuation">,</span>lstm_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>LSTM_Optimizee<span class="token punctuation">,</span>STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>retain_graph_flag <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
    p1<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> sgd_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;SGD&apos;</span><span class="token punctuation">)</span>
    p2<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> rms_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;RMS&apos;</span><span class="token punctuation">)</span>
    p3<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> adam_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;Adam&apos;</span><span class="token punctuation">)</span>
    p4<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> lstm_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;LSTM&apos;</span><span class="token punctuation">)</span>
    p1<span class="token punctuation">.</span>set_dashes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 2pt line, 2pt break, 10pt line, 2pt break</span>
    p2<span class="token punctuation">.</span>set_dashes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 2pt line, 2pt break, 10pt line, 2pt break</span>
    p3<span class="token punctuation">.</span>set_dashes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 2pt line, 2pt break, 10pt line, 2pt break</span>
    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>handles<span class="token operator">=</span><span class="token punctuation">[</span>p1<span class="token punctuation">,</span> p2<span class="token punctuation">,</span> p3<span class="token punctuation">,</span> p4<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment">#plt.yscale(&apos;log&apos;)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">&apos;Losses&apos;</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;sum_loss:sgd={},rms={},adam={},lstm={}&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>sgd_sum_loss<span class="token punctuation">,</span>rms_sum_loss<span class="token punctuation">,</span>adam_sum_loss<span class="token punctuation">,</span>lstm_sum_loss <span class="token punctuation">)</span><span class="token punctuation">)</span>
</pre><p><img src="https://img-blog.csdnimg.cn/20181123201810120.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="&#x5728;&#x8FD9;&#x91CC;&#x63D2;&#x5165;&#x56FE;&#x7247;&#x63CF;&#x8FF0;"></p>
<pre class="language-text">sum_loss:sgd=48.513607025146484,rms=5.537945747375488,adam=11.781242370605469,lstm=15.151629447937012
</pre>
<p><img src="https://img-blog.csdnimg.cn/20181123201822472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="&#x5728;&#x8FD9;&#x91CC;&#x63D2;&#x5165;&#x56FE;&#x7247;&#x63CF;&#x8FF0;"></p>
<pre class="language-text">sum_loss:sgd=48.513607025146484,rms=5.537945747375488,adam=11.781242370605469,lstm=15.151629447937012
</pre>
<p>&#x53EF;&#x4EE5;&#x770B;&#x51FA;&#x6765;&#xFF0C;&#x7ECF;&#x8FC7;&#x4F18;&#x5316;&#x540E;&#x7684;LSTM&#x4F18;&#x5316;&#x5668;&#xFF0C;&#x4F3C;&#x4E4E;&#x5DF2;&#x7ECF;&#x5F00;&#x59CB;&#x638C;&#x63E1;&#x5982;&#x4F55;&#x4F18;&#x5316;&#x7684;&#x65B9;&#x6CD5;&#xFF0C;&#x5373;&#x6211;&#x4EEC;&#x57FA;&#x672C;&#x8BAD;&#x7EC3;&#x51FA;&#x4E86;&#x4E00;&#x4E2A;&#x53EF;&#x4EE5;&#x8BAD;&#x7EC3;&#x6A21;&#x578B;&#x7684;&#x4F18;&#x5316;&#x5668;&#xFF01;</p>
<p><strong>&#x4F46;&#x662F;&#x6548;&#x679C;&#x5E76;&#x4E0D;&#x662F;&#x5F88;&#x660E;&#x663E;&#xFF01;</strong></p>
<p>&#x4E0D;&#x8FC7;&#x4EE3;&#x7801;&#x7F16;&#x5199;&#x53D6;&#x5F97;&#x4E00;&#x5B9A;&#x8FDB;&#x5C55;\ (0 ..0) /&#xFF0C;&#x63A5;&#x4E0B;&#x5C31;&#x662F;&#x8BA9;&#x6548;&#x679C;&#x66F4;&#x660E;&#x663E;&#xFF0C;&#x6027;&#x80FD;&#x66F4;&#x7A33;&#x5B9A;&#x4E86;&#x5427;&#xFF1F;</p>
<p><strong>&#x4E0D;&#x8FC7;&#x6211;&#x5148;&#x518D;&#x591A;&#x6D4B;&#x8BD5;&#x4E00;&#x4E9B;&#x5468;&#x671F;&#x770B;&#x770B;LSTM&#x7684;&#x4F18;&#x5316;&#x6548;&#x679C;&#xFF01;&#x5148;&#x522B;&#x9AD8;&#x5174;&#x592A;&#x65E9;&#xFF01;</strong></p>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token keyword">import</span> matplotlib
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token operator">%</span>matplotlib inline
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
STEPS <span class="token operator">=</span> <span class="token number">50</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>STEPS<span class="token punctuation">)</span>

    
<span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> 
    
    sgd_losses<span class="token punctuation">,</span> sgd_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>SGD<span class="token punctuation">,</span>STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    rms_losses<span class="token punctuation">,</span> rms_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>RMS<span class="token punctuation">,</span>STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    adam_losses<span class="token punctuation">,</span> adam_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>Adam<span class="token punctuation">,</span>STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    lstm_losses<span class="token punctuation">,</span>lstm_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>LSTM_Optimizee<span class="token punctuation">,</span>STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>retain_graph_flag <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
    p1<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> sgd_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;SGD&apos;</span><span class="token punctuation">)</span>
    p2<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> rms_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;RMS&apos;</span><span class="token punctuation">)</span>
    p3<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> adam_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;Adam&apos;</span><span class="token punctuation">)</span>
    p4<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> lstm_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;LSTM&apos;</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>handles<span class="token operator">=</span><span class="token punctuation">[</span>p1<span class="token punctuation">,</span> p2<span class="token punctuation">,</span> p3<span class="token punctuation">,</span> p4<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment">#plt.yscale(&apos;log&apos;)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">&apos;Losses&apos;</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;sum_loss:sgd={},rms={},adam={},lstm={}&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>sgd_sum_loss<span class="token punctuation">,</span>rms_sum_loss<span class="token punctuation">,</span>adam_sum_loss<span class="token punctuation">,</span>lstm_sum_loss <span class="token punctuation">)</span><span class="token punctuation">)</span>
</pre><p><img src="https://img-blog.csdnimg.cn/20181123201835829.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="&#x5728;&#x8FD9;&#x91CC;&#x63D2;&#x5165;&#x56FE;&#x7247;&#x63CF;&#x8FF0;"></p>
<pre class="language-text">sum_loss:sgd=150.99790954589844,rms=5.940474033355713,adam=14.17563247680664,lstm=268.0199279785156
</pre>
<h4 class="mume-header" id="%E5%8F%88%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E5%B9%BA%E8%9B%BE%E5%AD%90">&#x53C8;&#x51FA;&#x4E86;&#x4EC0;&#x4E48;&#x5E7A;&#x86FE;&#x5B50;&#xFF1F;</h4>

<p>&#x5373;&#x6211;&#x4EEC;&#x57FA;&#x672C;&#x8BAD;&#x7EC3;&#x51FA;&#x4E86;&#x4E00;&#x4E2A;&#x53EF;&#x4EE5;&#x8BAD;&#x7EC3;&#x6A21;&#x578B;&#x7684;&#x4F18;&#x5316;&#x5668;&#xFF01;&#x4F46;&#x662F; &#x7ECF;&#x8FC7;&#x66F4;&#x957F;&#x5468;&#x671F;&#x7684;&#x6D4B;&#x8BD5;&#xFF0C;&#x53D1;&#x73B0;&#x8BAD;&#x7EC3;&#x597D;&#x7684;&#x4F18;&#x5316;&#x5668;&#x53EA;&#x662F;&#x4F18;&#x5316;&#x4E86;&#x6307;&#x5B9A;&#x7684;&#x5468;&#x671F;&#x7684;loss&#xFF0C;&#x800C;&#x5E76;&#x6CA1;&#x6709;&#x5B66;&#x4F1A;&#x201C;&#x5168;&#x5C40;&#x4F18;&#x5316;&#x201D;&#x7684;&#x672C;&#x9886;&#xFF0C;&#x8FD9;&#x4E2A;&#x4F3C;&#x4E4E;&#x662F;&#x4E2A;&#x5927;&#x95EE;&#x9898;&#xFF01;</p>
<h5 class="mume-header" id="%E4%B8%8D%E5%90%8C%E5%91%A8%E6%9C%9F%E4%B8%8B%E8%BE%93%E5%85%A5lstm%E7%9A%84%E6%A2%AF%E5%BA%A6%E5%B9%85%E5%80%BC%E6%95%B0%E9%87%8F%E7%BA%A7%E4%B8%8D%E5%9C%A8%E4%B8%80%E4%B8%AA%E7%AD%89%E7%BA%A7%E4%B8%8A%E9%9D%A2">&#x4E0D;&#x540C;&#x5468;&#x671F;&#x4E0B;&#x8F93;&#x5165;LSTM&#x7684;&#x68AF;&#x5EA6;&#x5E45;&#x503C;&#x6570;&#x91CF;&#x7EA7;&#x4E0D;&#x5728;&#x4E00;&#x4E2A;&#x7B49;&#x7EA7;&#x4E0A;&#x9762;</h5>

<p>&#x8981;&#x5904;&#x7406;&#x4E00;&#x4E0B;&#xFF01;</p>
<p>&#x8BBA;&#x6587;&#x91CC;&#x9762;&#x63D0;&#x5230;&#x4E86;&#x5BF9;&#x68AF;&#x5EA6;&#x7684;&#x9884;&#x5904;&#x7406;&#xFF0C;&#x5373;&#x5904;&#x7406;&#x4E0D;&#x540C;&#x6570;&#x91CF;&#x7EA7;&#x522B;&#x7684;&#x68AF;&#x5EA6;&#xFF0C;&#x6765;&#x8FDB;&#x884C;BPTT&#xFF0C;&#x56E0;&#x4E3A;&#x6BCF;&#x4E2A;&#x5468;&#x671F;&#x7684;&#x4EA7;&#x751F;&#x7684;&#x68AF;&#x5EA6;&#x5E45;&#x5EA6;&#x662F;&#x5B8C;&#x5168;&#x4E0D;&#x5728;&#x4E00;&#x4E2A;&#x6570;&#x91CF;&#x7EA7;&#xFF0C;&#x524D;&#x671F;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x5F88;&#x5FEB;&#xFF0C;&#x4E2D;&#x540E;&#x671F;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x5E73;&#x7F13;&#xFF0C;&#x8FD9;&#x4E2A;&#x5BF9;&#x4E8E;LSTM&#x7684;&#x8F93;&#x5165;&#xFF0C;&#x53D8;&#x5316;&#x88D5;&#x5EA6;&#x592A;&#x5927;&#xFF0C;&#x5E94;&#x8BE5;&#x5F52;&#x4E00;&#x5316;&#xFF0C;&#x4F46;&#x8FD9;&#x4E2A;&#x6211;&#x5E76;&#x6CA1;&#x6709;&#x8003;&#x8651;&#xFF0C;&#x63A5;&#x4E0B;&#x4F3C;&#x4E4E;&#x8BE5;&#x5199;&#x8FD9;&#x4E2A;&#x90E8;&#x5206;&#x7684;&#x4EE3;&#x7801;&#x4E86;</p>
<p>&#x8BBA;&#x6587;&#x91CC;&#x63D0;&#x5230;&#x4E86;&#xFF1A;<br>
One potential challenge in training optimizers is that different input coordinates (i.e. the gradients w.r.t. different<br>
optimizee parameters) can have very different magnitudes.<br>
This is indeed the case e.g. when the optimizee is a neural network and different parameters<br>
correspond to weights in different layers.<br>
This can make training an optimizer difficult, because neural networks<br>
naturally disregard small variations in input signals and concentrate on bigger input values.</p>
<h5 class="mume-header" id="%E7%94%A8%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%BD%92%E4%B8%80%E5%8C%96%E5%B9%85%E5%80%BC%E6%96%B9%E5%90%91%E4%BA%8C%E5%85%83%E7%BB%84%E6%9B%BF%E4%BB%A3%E5%8E%9F%E6%A2%AF%E5%BA%A6%E4%BD%9C%E4%B8%BAlstm%E7%9A%84%E8%BE%93%E5%85%A5">&#x7528;&#x68AF;&#x5EA6;&#x7684;&#xFF08;&#x5F52;&#x4E00;&#x5316;&#x5E45;&#x503C;&#xFF0C;&#x65B9;&#x5411;&#xFF09;&#x4E8C;&#x5143;&#x7EC4;&#x66FF;&#x4EE3;&#x539F;&#x68AF;&#x5EA6;&#x4F5C;&#x4E3A;LSTM&#x7684;&#x8F93;&#x5165;</h5>

<p>To this aim we propose to preprocess the optimizer&apos;s inputs.<br>
One solution would be to give the optimizer <span class="mathjax-exps">$\left(\log(|\nabla|),\,\operatorname{sgn}(\nabla)\right)$</span> as an input, where<br>
<span class="mathjax-exps">$\nabla$</span> is the gradient in the current timestep.<br>
This has a problem that <span class="mathjax-exps">$\log(|\nabla|)$</span> diverges for <span class="mathjax-exps">$\nabla \rightarrow 0$</span>.<br>
Therefore, we use the following preprocessing formula</p>
<p><span class="mathjax-exps">$\nabla$</span> is the gradient in the current timestep.<br>
This has a problem that <span class="mathjax-exps">$\log(|\nabla|)$</span> diverges for <span class="mathjax-exps">$\nabla \rightarrow 0$</span>.<br>
Therefore, we use the following preprocessing formula</p>
<p></p><div class="mathjax-exps">$$\nabla^k\rightarrow  \left\{\begin{matrix}  \left(\frac{\log(|\nabla|)}{p}\,, \operatorname{sgn}(\nabla)\right)&amp; \text{if } |\nabla| \geq e^{-p}\\  (-1, e^p \nabla)   &amp; \text{otherwise} \end{matrix}\right.$$</div><p></p>
<p>&#x4F5C;&#x8005;&#x5C06;&#x4E0D;&#x540C;&#x5E45;&#x5EA6;&#x548C;&#x65B9;&#x5411;&#x4E0B;&#x7684;&#x68AF;&#x5EA6;&#xFF0C;&#x7528;&#x4E00;&#x4E2A;&#x6807;&#x51C6;&#x5316;&#x5230;<span class="mathjax-exps">$[-1,1]$</span>&#x7684;&#x5E45;&#x503C;&#x548C;&#x7B26;&#x53F7;&#x65B9;&#x5411;&#x4E8C;&#x5143;&#x7EC4;&#x6765;&#x8868;&#x793A;&#x539F;&#x6765;&#x7684;&#x68AF;&#x5EA6;&#x5F20;&#x91CF;&#xFF01;&#x8FD9;&#x6837;&#x5C06;&#x6709;&#x52A9;&#x4E8E;LSTM&#x7684;&#x53C2;&#x6570;&#x5B66;&#x4E60;&#xFF01;<br>
&#x90A3;&#x6211;&#x5C31;&#x91CD;&#x65B0;&#x5B9A;&#x4E49;LSTM&#x4F18;&#x5316;&#x5668;&#xFF01;</p>
<pre data-role="codeBlock" data-info="python" class="language-python">Layers <span class="token operator">=</span> <span class="token number">2</span>
Hidden_nums <span class="token operator">=</span> <span class="token number">20</span>

Input_DIM <span class="token operator">=</span> DIM
Output_DIM <span class="token operator">=</span> DIM
<span class="token comment"># &quot;coordinate-wise&quot; RNN </span>
<span class="token comment">#lstm1=torch.nn.LSTM(Input_DIM*2,Hidden_nums ,Layers)</span>
<span class="token comment">#Linear = torch.nn.Linear(Hidden_nums,Output_DIM)</span>


<span class="token keyword">class</span> <span class="token class-name">LSTM_Optimizee_Model</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;LSTM&#x4F18;&#x5316;&#x5668;&quot;&quot;&quot;</span>
    
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>input_size<span class="token punctuation">,</span>output_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_stacks<span class="token punctuation">,</span> batchsize<span class="token punctuation">,</span> preprocess <span class="token operator">=</span> <span class="token boolean">True</span> <span class="token punctuation">,</span>p <span class="token operator">=</span> <span class="token number">10</span> <span class="token punctuation">,</span>output_scale <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LSTM_Optimizee_Model<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>preprocess_flag <span class="token operator">=</span> preprocess
        self<span class="token punctuation">.</span>p <span class="token operator">=</span> p
        self<span class="token punctuation">.</span>output_scale <span class="token operator">=</span> output_scale <span class="token comment">#&#x8BBA;&#x6587;</span>
        self<span class="token punctuation">.</span>lstm <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_stacks<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>Linear <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span>
        <span class="token comment">#elf.lstm = torch.nn.LSTM(10, 20,2)</span>
        <span class="token comment">#elf.Linear = torch.nn.Linear(20,10)</span>
    
    <span class="token keyword">def</span> <span class="token function">LogAndSign_Preprocess_Gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>gradients<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Args:
          gradients: `Tensor` of gradients with shape `[d_1, ..., d_n]`.
          p       : `p` &gt; 0 is a parameter controlling how small gradients are disregarded 
        Returns:
          `Tensor` with shape `[d_1, ..., d_n-1, 2 * d_n]`. The first `d_n` elements
          along the nth dimension correspond to the `log output` \in [-1,1] and the remaining
          `d_n` elements to the `sign output`.
        &quot;&quot;&quot;</span>
        p  <span class="token operator">=</span> self<span class="token punctuation">.</span>p
        log <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>gradients<span class="token punctuation">)</span><span class="token punctuation">)</span>
        clamp_log <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>log<span class="token operator">/</span>p <span class="token punctuation">,</span> <span class="token builtin">min</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token builtin">max</span> <span class="token operator">=</span> <span class="token number">1.0</span><span class="token punctuation">)</span>
        clamp_sign <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">*</span>gradients<span class="token punctuation">,</span> <span class="token builtin">min</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token builtin">max</span> <span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>clamp_log<span class="token punctuation">,</span>clamp_sign<span class="token punctuation">)</span><span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment">#&#x5728;gradients&#x7684;&#x6700;&#x540E;&#x4E00;&#x7EF4;input_dims&#x62FC;&#x63A5;</span>
    
    <span class="token keyword">def</span> <span class="token function">Output_Gradient_Increment_And_Update_LSTM_Hidden_State</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_gradients<span class="token punctuation">,</span> prev_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;LSTM&#x7684;&#x6838;&#x5FC3;&#x64CD;&#x4F5C;&quot;&quot;&quot;</span>
        <span class="token keyword">if</span> prev_state <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span> <span class="token comment">#init_state</span>
            prev_state <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>Layers<span class="token punctuation">,</span>batchsize<span class="token punctuation">,</span>Hidden_nums<span class="token punctuation">)</span><span class="token punctuation">,</span>
                         torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>Layers<span class="token punctuation">,</span>batchsize<span class="token punctuation">,</span>Hidden_nums<span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        update <span class="token punctuation">,</span> next_state <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">(</span>input_gradients<span class="token punctuation">,</span> prev_state<span class="token punctuation">)</span>
        
        update <span class="token operator">=</span> Linear<span class="token punctuation">(</span>update<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>output_scale <span class="token comment">#&#x56E0;&#x4E3A;LSTM&#x7684;&#x8F93;&#x51FA;&#x662F;&#x5F53;&#x524D;&#x6B65;&#x7684;Hidden&#xFF0C;&#x9700;&#x8981;&#x53D8;&#x6362;&#x5230;output&#x7684;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x4E0A; </span>
        <span class="token keyword">return</span> update<span class="token punctuation">,</span> next_state
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>gradients<span class="token punctuation">,</span> prev_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
       
        <span class="token comment">#LSTM&#x7684;&#x8F93;&#x5165;&#x4E3A;&#x68AF;&#x5EA6;&#xFF0C;pytorch&#x8981;&#x6C42;torch.nn.lstm&#x7684;&#x8F93;&#x5165;&#x4E3A;&#xFF08;1&#xFF0C;batchsize,input_dim&#xFF09;</span>
        <span class="token comment">#&#x539F;gradient.size()=torch.size[5] -&gt;[1,1,5]</span>
        gradients <span class="token operator">=</span> gradients<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>preprocess_flag <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
            gradients <span class="token operator">=</span> self<span class="token punctuation">.</span>LogAndSign_Preprocess_Gradient<span class="token punctuation">(</span>gradients<span class="token punctuation">)</span>
        
        update <span class="token punctuation">,</span> next_state <span class="token operator">=</span> self<span class="token punctuation">.</span>Output_Gradient_Increment_And_Update_LSTM_Hidden_State<span class="token punctuation">(</span>gradients <span class="token punctuation">,</span> prev_state<span class="token punctuation">)</span>
        
        <span class="token comment"># Squeeze to make it a single batch again.[1,1,5]-&gt;[5]</span>
        update <span class="token operator">=</span> update<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> update <span class="token punctuation">,</span> next_state

LSTM_Optimizee <span class="token operator">=</span> LSTM_Optimizee_Model<span class="token punctuation">(</span>Input_DIM<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> Output_DIM<span class="token punctuation">,</span> Hidden_nums <span class="token punctuation">,</span>Layers <span class="token punctuation">,</span> batchsize<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
    

grads <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">10</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>grads<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

update<span class="token punctuation">,</span>state <span class="token operator">=</span>  LSTM_Optimizee<span class="token punctuation">(</span>grads<span class="token punctuation">,</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>update<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
</pre><pre class="language-text">torch.Size([10])
torch.Size([10])
</pre>
<p>&#x7F16;&#x5199;&#x6210;&#x529F;&#xFF01;<br>
&#x6267;&#x884C;&#xFF01;</p>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token keyword">def</span> <span class="token function">learn</span><span class="token punctuation">(</span>optimizee<span class="token punctuation">,</span>unroll_train_steps<span class="token punctuation">,</span>retain_graph_flag<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>reset_theta <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span> 
    <span class="token triple-quoted-string string">&quot;&quot;&quot;retain_graph_flag=False   &#x9ED8;&#x8BA4;&#x6BCF;&#x6B21;loss_backward&#x540E; &#x91CA;&#x653E;&#x52A8;&#x6001;&#x56FE;
    #  reset_theta = False     &#x9ED8;&#x8BA4;&#x6BCF;&#x6B21;&#x5B66;&#x4E60;&#x524D; &#x4E0D;&#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x53C2;&#x6570;&quot;&quot;&quot;</span>
    
    <span class="token keyword">if</span> reset_theta <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
        theta_new <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>DIM<span class="token punctuation">)</span>
        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>theta_new<span class="token punctuation">,</span>a<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>b<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span> 
        theta_init_new <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>theta<span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> theta_init_new
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> theta_init
        
    global_loss_graph <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment">#&#x8FD9;&#x4E2A;&#x662F;&#x4E3A;LSTM&#x4F18;&#x5316;&#x5668;&#x6C42;&#x6240;&#x6709;loss&#x76F8;&#x52A0;&#x4EA7;&#x751F;&#x8BA1;&#x7B97;&#x56FE;&#x51C6;&#x5907;&#x7684;</span>
    state <span class="token operator">=</span> <span class="token boolean">None</span>
    x<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
    <span class="token keyword">if</span> optimizee<span class="token operator">!=</span><span class="token string">&apos;Adam&apos;</span><span class="token punctuation">:</span>
        losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>unroll_train_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>     
            loss <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    
            <span class="token comment">#global_loss_graph += torch.exp(torch.Tensor([-i/20]))*loss</span>
            <span class="token comment">#global_loss_graph += (0.8*torch.log10(torch.Tensor([i+1]))+1)*loss</span>
            global_loss_graph <span class="token operator">+=</span> loss
           <span class="token comment"># print(&apos;loss{}:&apos;.format(i),loss)</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span>retain_graph_flag<span class="token punctuation">)</span> <span class="token comment"># &#x9ED8;&#x8BA4;&#x4E3A;False,&#x5F53;&#x4F18;&#x5316;LSTM&#x8BBE;&#x7F6E;&#x4E3A;True</span>
            
            update<span class="token punctuation">,</span> state <span class="token operator">=</span> optimizee<span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">,</span> state<span class="token punctuation">)</span>
           <span class="token comment">#print(update)</span>
            losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>     
            x <span class="token operator">=</span> x <span class="token operator">+</span> update  
            x<span class="token punctuation">.</span>retain_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> losses <span class="token punctuation">,</span>global_loss_graph 
    
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        x<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
        optimizee<span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span> <span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.1</span> <span class="token punctuation">)</span>
        
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>unroll_train_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
            
            optimizee<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            loss <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            
            global_loss_graph <span class="token operator">+=</span> loss
            
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span>retain_graph_flag<span class="token punctuation">)</span>
            optimizee<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>detach_<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment">#print(x)</span>
        <span class="token keyword">return</span> losses<span class="token punctuation">,</span>global_loss_graph 
    
Global_Train_Steps <span class="token operator">=</span> <span class="token number">100</span>

<span class="token keyword">def</span> <span class="token function">global_training</span><span class="token punctuation">(</span>optimizee<span class="token punctuation">)</span><span class="token punctuation">:</span>
    global_loss_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    
    adam_global_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>optimizee<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> <span class="token number">0.0001</span><span class="token punctuation">)</span>
    _<span class="token punctuation">,</span>global_loss_1 <span class="token operator">=</span> learn<span class="token punctuation">(</span>optimizee<span class="token punctuation">,</span>TRAINING_STEPS<span class="token punctuation">,</span>retain_graph_flag <span class="token operator">=</span><span class="token boolean">True</span> <span class="token punctuation">,</span>reset_theta <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>global_loss_1<span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>Global_Train_Steps<span class="token punctuation">)</span><span class="token punctuation">:</span>    
        _<span class="token punctuation">,</span>global_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>optimizee<span class="token punctuation">,</span>TRAINING_STEPS<span class="token punctuation">,</span>retain_graph_flag <span class="token operator">=</span><span class="token boolean">True</span> <span class="token punctuation">,</span>reset_theta <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span>       
        adam_global_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token comment">#print(i,global_loss)</span>
        global_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#&#x6BCF;&#x6B21;&#x90FD;&#x662F;&#x4F18;&#x5316;&#x8FD9;&#x4E2A;&#x56FA;&#x5B9A;&#x7684;&#x56FE;&#xFF0C;&#x4E0D;&#x53EF;&#x4EE5;&#x91CA;&#x653E;&#x52A8;&#x6001;&#x56FE;&#x7684;&#x7F13;&#x5B58;</span>
        <span class="token comment">#print(&apos;xxx&apos;,[(z,z.requires_grad) for z in optimizee.parameters()  ])</span>
        adam_global_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">#print(&apos;xxx&apos;,[(z.grad,z.requires_grad) for z in optimizee.parameters()  ])</span>
        global_loss_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>global_loss<span class="token punctuation">.</span>detach_<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
    <span class="token keyword">print</span><span class="token punctuation">(</span>global_loss<span class="token punctuation">)</span>
    <span class="token keyword">return</span> global_loss_list

<span class="token comment"># &#x8981;&#x628A;&#x56FE;&#x653E;&#x8FDB;&#x51FD;&#x6570;&#x4F53;&#x5185;&#xFF0C;&#x76F4;&#x63A5;&#x8D4B;&#x503C;&#x7684;&#x8BDD;&#x56FE;&#x4F1A;&#x4E22;&#x5931;</span>
<span class="token comment"># &#x4F18;&#x5316;optimizee</span>
global_loss_list <span class="token operator">=</span> global_training<span class="token punctuation">(</span>LSTM_Optimizee<span class="token punctuation">)</span>
</pre><pre class="language-text">tensor(239.6029, grad_fn=&lt;ThAddBackward&gt;)
tensor(158.0625)
</pre>
<pre data-role="codeBlock" data-info="python" class="language-python">Global_T <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>Global_Train_Steps<span class="token punctuation">)</span>

p1<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>Global_T<span class="token punctuation">,</span> global_loss_list<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;Global_graph_loss&apos;</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>handles<span class="token operator">=</span><span class="token punctuation">[</span>p1<span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">&apos;Training LSTM optimizee by gradient descent &apos;</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</pre><p><img src="https://img-blog.csdnimg.cn/20181123201904948.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="&#x5728;&#x8FD9;&#x91CC;&#x63D2;&#x5165;&#x56FE;&#x7247;&#x63CF;&#x8FF0;"></p>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token keyword">import</span> matplotlib
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token operator">%</span>matplotlib inline
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
STEPS <span class="token operator">=</span> <span class="token number">30</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>STEPS<span class="token punctuation">)</span>

Adam <span class="token operator">=</span> <span class="token string">&apos;Adam&apos;</span>  
<span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> 
    
    sgd_losses<span class="token punctuation">,</span> sgd_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>SGD<span class="token punctuation">,</span>STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    rms_losses<span class="token punctuation">,</span> rms_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>RMS<span class="token punctuation">,</span>STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    adam_losses<span class="token punctuation">,</span> adam_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>Adam<span class="token punctuation">,</span>STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    lstm_losses<span class="token punctuation">,</span>lstm_sum_loss <span class="token operator">=</span> learn<span class="token punctuation">(</span>LSTM_Optimizee<span class="token punctuation">,</span>STEPS<span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>retain_graph_flag <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
    p1<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> sgd_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;SGD&apos;</span><span class="token punctuation">)</span>
    p2<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> rms_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;RMS&apos;</span><span class="token punctuation">)</span>
    p3<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> adam_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;Adam&apos;</span><span class="token punctuation">)</span>
    p4<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> lstm_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;LSTM&apos;</span><span class="token punctuation">)</span>
    p1<span class="token punctuation">.</span>set_dashes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 2pt line, 2pt break, 10pt line, 2pt break</span>
    p2<span class="token punctuation">.</span>set_dashes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 2pt line, 2pt break, 10pt line, 2pt break</span>
    p3<span class="token punctuation">.</span>set_dashes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 2pt line, 2pt break, 10pt line, 2pt break</span>
    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>handles<span class="token operator">=</span><span class="token punctuation">[</span>p1<span class="token punctuation">,</span> p2<span class="token punctuation">,</span> p3<span class="token punctuation">,</span> p4<span class="token punctuation">]</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">&apos;Losses&apos;</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;sum_loss:sgd={},rms={},adam={},lstm={}&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>sgd_sum_loss<span class="token punctuation">,</span>rms_sum_loss<span class="token punctuation">,</span>adam_sum_loss<span class="token punctuation">,</span>lstm_sum_loss <span class="token punctuation">)</span><span class="token punctuation">)</span>
</pre><p><img src="https://img-blog.csdnimg.cn/20181123201922189.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="&#x5728;&#x8FD9;&#x91CC;&#x63D2;&#x5165;&#x56FE;&#x7247;&#x63CF;&#x8FF0;"></p>
<pre class="language-text">sum_loss:sgd=94.19924926757812,rms=5.705832481384277,adam=13.772469520568848,lstm=1224.2393798828125
</pre>
<p><strong>&#x4E3A;&#x4EC0;&#x4E48;loss&#x51FA;&#x73B0;&#x4E86;NaN&#xFF1F;&#xFF1F;&#xFF1F;&#x4E3A;&#x4EC0;&#x4E48;&#x4F18;&#x5316;&#x5668;&#x7684;&#x6CDB;&#x5316;&#x6027;&#x80FD;&#x5F88;&#x5DEE;&#xFF1F;&#xFF1F;&#x5F88;&#x70E6;</strong></p>
<p>&#x5373;&#x8D85;&#x8FC7;Unroll&#x5468;&#x671F;&#x4EE5;&#x540E;&#xFF0C;LSTM&#x4F18;&#x5316;&#x5668;&#x4E0D;&#x518D;&#x5177;&#x5907;&#x4F18;&#x5316;&#x6027;&#x80FD;&#xFF1F;&#xFF1F;&#xFF1F;</p>
<p>&#x7136;&#x540E;&#x6211;&#x53C8;&#x56DE;&#x987E;&#x8BBA;&#x6587;&#xFF0C;&#x53D1;&#x73B0;&#xFF0C;&#x5BF9;&#x4F18;&#x5316;&#x5668;&#x8FDB;&#x884C;&#x4F18;&#x5316;&#x7684;&#x6BCF;&#x4E2A;&#x5468;&#x671F;&#x5F00;&#x59CB;&#xFF0C;&#x8981;&#x91CD;&#x65B0;&#x968F;&#x673A;&#x5316;&#xFF0C;&#x4F18;&#x5316;&#x95EE;&#x9898;&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x5373;&#x786E;&#x4FDD;&#x6211;&#x4EEC;&#x7684;LSTM&#x4E0D;&#x9488;&#x5BF9;&#x4E00;&#x4E2A;&#x7279;&#x5B9A;&#x4F18;&#x5316;&#x95EE;&#x9898;&#x8FC7;&#x62DF;&#x5408;&#xFF0C;&#x4EC0;&#x4E48;&#xFF1F;&#x4F18;&#x5316;&#x5668;&#x4E5F;&#x4F1A;&#x8FC7;&#x62DF;&#x5408;&#xFF01;&#x662F;&#x7684;&#xFF01;l</p>
<p>&#x8BBA;&#x6587;&#x91CC;&#x9762;&#x5176;&#x5B9E;&#x4E5F;&#x6709;&#x63D0;&#x5230;&#xFF1A;</p>
<p>Given a distribution of functions <span class="mathjax-exps">$f$</span> we will write<br>
the expected loss as:<br>
</p><div class="mathjax-exps">$$L\left(\phi \right) =E_f \left[ f \left ( \theta ^{*}\left ( f,\phi  \right )\right ) \right]&#xFF08;1&#xFF09;$$</div><p></p>
<p>&#x5176;&#x4E2D;f&#x662F;&#x968F;&#x673A;&#x5206;&#x5E03;&#x7684;&#xFF0C;&#x90A3;&#x4E48;&#x5C31;&#x9700;&#x8981;&#x5728;Unroll&#x7684;&#x521D;&#x59CB;&#x8FDB;&#x884C;&#x4ECE;IID &#x6807;&#x51C6;Gaussian&#x5206;&#x5E03;&#x968F;&#x673A;&#x91C7;&#x6837;&#x51FD;&#x6570;&#x7684;&#x53C2;&#x6570;</p>
<p>&#x53E6;&#x5916;&#x6211;&#x5B8C;&#x5584;&#x4E86;&#x4EE3;&#x7801;&#xFF0C;&#x6839;&#x636E;&#x539F;&#x4F5C;&#x4EE3;&#x7801;&#x5B9E;&#x73B0;&#xFF0C;&#x628A;LSTM&#x7684;&#x8F93;&#x51FA;&#x4E58;&#x4EE5;&#x4E00;&#x4E2A;&#x7CFB;&#x6570;0.01&#xFF0C;&#x90A3;&#x4E48;LSTM&#x7684;&#x5B66;&#x4E60;&#x53D8;&#x5F97;&#x66F4;&#x52A0;&#x5FEB;&#x901F;&#x4E86;&#x3002;</p>
<p>&#x8FD8;&#x6709;&#x4E00;&#x4E2A;&#x5730;&#x65B9;&#xFF0C;&#x5C31;&#x662F;&#x4F5C;&#x8005;&#x4F18;&#x5316;optimiee&#x7528;&#x4E86;100&#x4E2A;&#x5468;&#x671F;&#xFF0C;&#x5373;5&#x4E2A;&#x8FDE;&#x7EED;&#x7684;Unroll&#x5468;&#x671F;&#xFF0C;&#x8FD9;&#x4E00;&#x70B9;&#x4F3C;&#x4E4E;&#x6211;&#x4E4B;&#x524D;&#x4E5F;&#x6CA1;&#x6709;&#x8003;&#x8651;&#x5230;&#xFF01;</p>
<hr>
<hr>
<h2 class="mume-header" id="%E4%BB%A5%E4%B8%8A%E6%98%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E5%86%99%E9%81%87%E5%88%B0%E7%9A%84%E7%A7%8D%E7%A7%8D%E9%97%AE%E9%A2%98%E4%B8%8B%E9%9D%A2%E5%B0%B1%E6%98%AF%E6%9C%80%E5%AE%8C%E6%95%B4%E7%9A%84%E6%9C%89%E6%95%88%E4%BB%A3%E7%A0%81%E4%BA%86">&#x4EE5;&#x4E0A;&#x662F;&#x4EE3;&#x7801;&#x7F16;&#x5199;&#x9047;&#x5230;&#x7684;&#x79CD;&#x79CD;&#x95EE;&#x9898;&#xFF0C;&#x4E0B;&#x9762;&#x5C31;&#x662F;&#x6700;&#x5B8C;&#x6574;&#x7684;&#x6709;&#x6548;&#x4EE3;&#x7801;&#x4E86;&#xFF01;&#xFF01;&#xFF01;</h2>

<p><strong>&#x6211;&#x4EEC;&#x8003;&#x8651;&#x4F18;&#x5316;&#x8BBA;&#x6587;&#x4E2D;&#x63D0;&#x5230;&#x7684;Quadratic&#x51FD;&#x6570;&#xFF0C;&#x5E76;&#x4E14;&#x7528;&#x8BBA;&#x6587;&#x4E2D;&#x5B8C;&#x5168;&#x4E00;&#x6837;&#x7684;&#x5B9E;&#x9A8C;&#x6761;&#x4EF6;&#xFF01;</strong></p>
<p></p><div class="mathjax-exps">$$f(\theta) = \|W\theta - y\|_2^2$$</div><br>
for different 10x10 matrices <span class="mathjax-exps">$W$</span> and 10-dimensional vectors <span class="mathjax-exps">$y$</span> whose elements are drawn<br>
from an IID Gaussian distribution.<br>
Optimizers were trained by optimizing random functions from this family and<br>
tested on newly sampled functions from the same distribution.  Each function was<br>
optimized for 100 steps and the trained optimizers were unrolled for 20 steps.<br>
We have not used any preprocessing, nor postprocessing.<p></p>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token comment"># coding: utf-8</span>

<span class="token comment"># Learning to learn by gradient descent by gradient descent</span>
<span class="token comment"># =========================#</span>

<span class="token comment"># https://arxiv.org/abs/1611.03824</span>
<span class="token comment"># https://yangsenius.github.io/blog/LSTM_Meta/</span>
<span class="token comment"># https://github.com/yangsenius/learning-to-learn-by-pytorch</span>
<span class="token comment"># author&#xFF1A;yangsen</span>
<span class="token comment"># #### &#x201C;&#x901A;&#x8FC7;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x6765;&#x5B66;&#x4E60;&#x5982;&#x4F55;&#x901A;&#x8FC7;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x5B66;&#x4E60;&#x201D;</span>
<span class="token comment"># #### &#x8981;&#x8BA9;&#x4F18;&#x5316;&#x5668;&#x5B66;&#x4F1A;&#x8FD9;&#x6837;   &quot;&#x4E3A;&#x4E86;&#x66F4;&#x597D;&#x5730;&#x5F97;&#x5230;&#xFF0C;&#x8981;&#x5148;&#x53BB;&#x820D;&#x5F03;&quot;  &#x8FD9;&#x6837;&#x7C7B;&#x4F3C;&#x7684;&#x77E5;&#x8BC6;&#xFF01;</span>

<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">from</span> timeit <span class="token keyword">import</span> default_timer <span class="token keyword">as</span> timer
<span class="token comment">#####################      &#x4F18;&#x5316;&#x95EE;&#x9898;   ##########################</span>
USE_CUDA <span class="token operator">=</span> <span class="token boolean">False</span>
DIM <span class="token operator">=</span> <span class="token number">10</span>
batchsize <span class="token operator">=</span> <span class="token number">128</span>

<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    USE_CUDA <span class="token operator">=</span> <span class="token boolean">True</span>  
USE_CUDA <span class="token operator">=</span> <span class="token boolean">False</span>  


<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&apos;\n\nUSE_CUDA = {}\n\n&apos;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>USE_CUDA<span class="token punctuation">)</span><span class="token punctuation">)</span>
 

<span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>W<span class="token punctuation">,</span>Y<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;quadratic function : f(\theta) = \|W\theta - y\|_2^2&quot;&quot;&quot;</span>
    <span class="token keyword">if</span> USE_CUDA<span class="token punctuation">:</span>
        W <span class="token operator">=</span> W<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
        Y <span class="token operator">=</span> Y<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>W<span class="token punctuation">,</span>x<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span>Y<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

<span class="token comment">###############################################################</span>

<span class="token comment">######################    &#x624B;&#x5DE5;&#x7684;&#x4F18;&#x5316;&#x5668;   ###################</span>

<span class="token keyword">def</span> <span class="token function">SGD</span><span class="token punctuation">(</span>gradients<span class="token punctuation">,</span> state<span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
   
    <span class="token keyword">return</span> <span class="token operator">-</span>gradients<span class="token operator">*</span>learning_rate<span class="token punctuation">,</span> state

<span class="token keyword">def</span> <span class="token function">RMS</span><span class="token punctuation">(</span>gradients<span class="token punctuation">,</span> state<span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> decay_rate<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> state <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        state <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>DIM<span class="token punctuation">)</span>
        <span class="token keyword">if</span> USE_CUDA <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
            state <span class="token operator">=</span> state<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
            
    state <span class="token operator">=</span> decay_rate<span class="token operator">*</span>state <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>decay_rate<span class="token punctuation">)</span><span class="token operator">*</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>gradients<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    update <span class="token operator">=</span> <span class="token operator">-</span>learning_rate<span class="token operator">*</span>gradients <span class="token operator">/</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>state<span class="token operator">+</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> update<span class="token punctuation">,</span> state

<span class="token keyword">def</span> <span class="token function">adam</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment">##########################################################</span>


<span class="token comment">#####################    &#x81EA;&#x52A8; LSTM &#x4F18;&#x5316;&#x5668;&#x6A21;&#x578B;  ##########################</span>
<span class="token keyword">class</span> <span class="token class-name">LSTM_Optimizee_Model</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;LSTM&#x4F18;&#x5316;&#x5668;&quot;&quot;&quot;</span>
    
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>input_size<span class="token punctuation">,</span>output_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_stacks<span class="token punctuation">,</span> batchsize<span class="token punctuation">,</span> preprocess <span class="token operator">=</span> <span class="token boolean">True</span> <span class="token punctuation">,</span>p <span class="token operator">=</span> <span class="token number">10</span> <span class="token punctuation">,</span>output_scale <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LSTM_Optimizee_Model<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>preprocess_flag <span class="token operator">=</span> preprocess
        self<span class="token punctuation">.</span>p <span class="token operator">=</span> p
        self<span class="token punctuation">.</span>input_flag <span class="token operator">=</span> <span class="token number">2</span>
        <span class="token keyword">if</span> preprocess <span class="token operator">!=</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
             self<span class="token punctuation">.</span>input_flag <span class="token operator">=</span> <span class="token number">1</span>
        self<span class="token punctuation">.</span>output_scale <span class="token operator">=</span> output_scale <span class="token comment">#&#x8BBA;&#x6587;</span>
        self<span class="token punctuation">.</span>lstm <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>input_size<span class="token operator">*</span>self<span class="token punctuation">.</span>input_flag<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_stacks<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>Linear <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span> <span class="token comment">#1-&gt; output_size</span>
        
    <span class="token keyword">def</span> <span class="token function">LogAndSign_Preprocess_Gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>gradients<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Args:
          gradients: `Tensor` of gradients with shape `[d_1, ..., d_n]`.
          p       : `p` &gt; 0 is a parameter controlling how small gradients are disregarded 
        Returns:
          `Tensor` with shape `[d_1, ..., d_n-1, 2 * d_n]`. The first `d_n` elements
          along the nth dimension correspond to the `log output` \in [-1,1] and the remaining
          `d_n` elements to the `sign output`.
        &quot;&quot;&quot;</span>
        p  <span class="token operator">=</span> self<span class="token punctuation">.</span>p
        log <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>gradients<span class="token punctuation">)</span><span class="token punctuation">)</span>
        clamp_log <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>log<span class="token operator">/</span>p <span class="token punctuation">,</span> <span class="token builtin">min</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token builtin">max</span> <span class="token operator">=</span> <span class="token number">1.0</span><span class="token punctuation">)</span>
        clamp_sign <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">*</span>gradients<span class="token punctuation">,</span> <span class="token builtin">min</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token builtin">max</span> <span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>clamp_log<span class="token punctuation">,</span>clamp_sign<span class="token punctuation">)</span><span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment">#&#x5728;gradients&#x7684;&#x6700;&#x540E;&#x4E00;&#x7EF4;input_dims&#x62FC;&#x63A5;</span>
    
    <span class="token keyword">def</span> <span class="token function">Output_Gradient_Increment_And_Update_LSTM_Hidden_State</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_gradients<span class="token punctuation">,</span> prev_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;LSTM&#x7684;&#x6838;&#x5FC3;&#x64CD;&#x4F5C;
        coordinate-wise LSTM &quot;&quot;&quot;</span>
        <span class="token keyword">if</span> prev_state <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span> <span class="token comment">#init_state</span>
            prev_state <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>Layers<span class="token punctuation">,</span>batchsize<span class="token punctuation">,</span>Hidden_nums<span class="token punctuation">)</span><span class="token punctuation">,</span>
                            torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>Layers<span class="token punctuation">,</span>batchsize<span class="token punctuation">,</span>Hidden_nums<span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> USE_CUDA <span class="token punctuation">:</span>
                 prev_state <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>Layers<span class="token punctuation">,</span>batchsize<span class="token punctuation">,</span>Hidden_nums<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                            torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>Layers<span class="token punctuation">,</span>batchsize<span class="token punctuation">,</span>Hidden_nums<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
         			
        update <span class="token punctuation">,</span> next_state <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">(</span>input_gradients<span class="token punctuation">,</span> prev_state<span class="token punctuation">)</span>
        update <span class="token operator">=</span> self<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>update<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>output_scale <span class="token comment">#&#x56E0;&#x4E3A;LSTM&#x7684;&#x8F93;&#x51FA;&#x662F;&#x5F53;&#x524D;&#x6B65;&#x7684;Hidden&#xFF0C;&#x9700;&#x8981;&#x53D8;&#x6362;&#x5230;output&#x7684;&#x76F8;&#x540C;&#x5F62;&#x72B6;&#x4E0A; </span>
        <span class="token keyword">return</span> update<span class="token punctuation">,</span> next_state
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>input_gradients<span class="token punctuation">,</span> prev_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> USE_CUDA<span class="token punctuation">:</span>
            input_gradients <span class="token operator">=</span> input_gradients<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">#LSTM&#x7684;&#x8F93;&#x5165;&#x4E3A;&#x68AF;&#x5EA6;&#xFF0C;pytorch&#x8981;&#x6C42;torch.nn.lstm&#x7684;&#x8F93;&#x5165;&#x4E3A;&#xFF08;1&#xFF0C;batchsize,input_dim&#xFF09;</span>
        <span class="token comment">#&#x539F;gradient.size()=torch.size[5] -&gt;[1,1,5]</span>
        gradients <span class="token operator">=</span> input_gradients<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
      
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>preprocess_flag <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
            gradients <span class="token operator">=</span> self<span class="token punctuation">.</span>LogAndSign_Preprocess_Gradient<span class="token punctuation">(</span>gradients<span class="token punctuation">)</span>
      
        update <span class="token punctuation">,</span> next_state <span class="token operator">=</span> self<span class="token punctuation">.</span>Output_Gradient_Increment_And_Update_LSTM_Hidden_State<span class="token punctuation">(</span>gradients <span class="token punctuation">,</span> prev_state<span class="token punctuation">)</span>
        <span class="token comment"># Squeeze to make it a single batch again.[1,1,5]-&gt;[5]</span>
        update <span class="token operator">=</span> update<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
       
        <span class="token keyword">return</span> update <span class="token punctuation">,</span> next_state
    
<span class="token comment">#################   &#x4F18;&#x5316;&#x5668;&#x6A21;&#x578B;&#x53C2;&#x6570;  ##############################</span>
Layers <span class="token operator">=</span> <span class="token number">2</span>
Hidden_nums <span class="token operator">=</span> <span class="token number">20</span>
Input_DIM <span class="token operator">=</span> DIM
Output_DIM <span class="token operator">=</span> DIM
output_scale_value<span class="token operator">=</span><span class="token number">1</span>

<span class="token comment">#######   &#x6784;&#x9020;&#x4E00;&#x4E2A;&#x4F18;&#x5316;&#x5668;  #######</span>
LSTM_Optimizee <span class="token operator">=</span> LSTM_Optimizee_Model<span class="token punctuation">(</span>Input_DIM<span class="token punctuation">,</span> Output_DIM<span class="token punctuation">,</span> Hidden_nums <span class="token punctuation">,</span>Layers <span class="token punctuation">,</span> batchsize<span class="token operator">=</span>batchsize<span class="token punctuation">,</span>\
                preprocess<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>output_scale<span class="token operator">=</span>output_scale_value<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>LSTM_Optimizee<span class="token punctuation">)</span>

<span class="token keyword">if</span> USE_CUDA<span class="token punctuation">:</span>
    LSTM_Optimizee <span class="token operator">=</span> LSTM_Optimizee<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
   

<span class="token comment">######################  &#x4F18;&#x5316;&#x95EE;&#x9898;&#x76EE;&#x6807;&#x51FD;&#x6570;&#x7684;&#x5B66;&#x4E60;&#x8FC7;&#x7A0B;   ###############</span>


<span class="token keyword">class</span> <span class="token class-name">Learner</span><span class="token punctuation">(</span> <span class="token builtin">object</span> <span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Args :
        `f` : &#x8981;&#x5B66;&#x4E60;&#x7684;&#x95EE;&#x9898;
        `optimizee` : &#x4F7F;&#x7528;&#x7684;&#x4F18;&#x5316;&#x5668;
        `train_steps` : &#x5BF9;&#x4E8E;&#x5176;&#x4ED6;SGD,Adam&#x7B49;&#x662F;&#x8BAD;&#x7EC3;&#x5468;&#x671F;&#xFF0C;&#x5BF9;&#x4E8E;LSTM&#x8BAD;&#x7EC3;&#x65F6;&#x7684;&#x5C55;&#x5F00;&#x5468;&#x671F;
        `retain_graph_flag=False`  : &#x9ED8;&#x8BA4;&#x6BCF;&#x6B21;loss_backward&#x540E; &#x91CA;&#x653E;&#x52A8;&#x6001;&#x56FE;
        `reset_theta = False `  :  &#x9ED8;&#x8BA4;&#x6BCF;&#x6B21;&#x5B66;&#x4E60;&#x524D; &#x4E0D;&#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x53C2;&#x6570;
        `reset_function_from_IID_distirbution = True` : &#x9ED8;&#x8BA4;&#x4ECE;&#x5206;&#x5E03;&#x4E2D;&#x968F;&#x673A;&#x91C7;&#x6837;&#x51FD;&#x6570; 

    Return :
        `losses` : reserves each loss value in each iteration
        `global_loss_graph` : constructs the graph of all Unroll steps for LSTM&apos;s BPTT 
    &quot;&quot;&quot;</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>    f <span class="token punctuation">,</span>   optimizee<span class="token punctuation">,</span>  train_steps <span class="token punctuation">,</span>  
                                            eval_flag <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
                                            retain_graph_flag<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                                            reset_theta <span class="token operator">=</span> <span class="token boolean">False</span> <span class="token punctuation">,</span>
                                            reset_function_from_IID_distirbution <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>f <span class="token operator">=</span> f
        self<span class="token punctuation">.</span>optimizee <span class="token operator">=</span> optimizee
        self<span class="token punctuation">.</span>train_steps <span class="token operator">=</span> train_steps
        <span class="token comment">#self.num_roll=num_roll</span>
        self<span class="token punctuation">.</span>eval_flag <span class="token operator">=</span> eval_flag
        self<span class="token punctuation">.</span>retain_graph_flag <span class="token operator">=</span> retain_graph_flag
        self<span class="token punctuation">.</span>reset_theta <span class="token operator">=</span> reset_theta
        self<span class="token punctuation">.</span>reset_function_from_IID_distirbution <span class="token operator">=</span> reset_function_from_IID_distirbution  
        self<span class="token punctuation">.</span>init_theta_of_f<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>state <span class="token operator">=</span> <span class="token boolean">None</span>

        self<span class="token punctuation">.</span>global_loss_graph <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment">#&#x8FD9;&#x4E2A;&#x662F;&#x4E3A;LSTM&#x4F18;&#x5316;&#x5668;&#x6C42;&#x6240;&#x6709;loss&#x76F8;&#x52A0;&#x4EA7;&#x751F;&#x8BA1;&#x7B97;&#x56FE;&#x51C6;&#x5907;&#x7684;</span>
        self<span class="token punctuation">.</span>losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>   <span class="token comment"># &#x4FDD;&#x5B58;&#x6BCF;&#x4E2A;&#x8BAD;&#x7EC3;&#x5468;&#x671F;&#x7684;loss&#x503C;</span>

    <span class="token keyword">def</span> <span class="token function">init_theta_of_f</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  
        <span class="token triple-quoted-string string">&apos;&apos;&apos; &#x521D;&#x59CB;&#x5316; &#x4F18;&#x5316;&#x95EE;&#x9898; f &#x7684;&#x53C2;&#x6570; &apos;&apos;&apos;</span>
        self<span class="token punctuation">.</span>DIM <span class="token operator">=</span> <span class="token number">10</span>
        self<span class="token punctuation">.</span>batchsize <span class="token operator">=</span> <span class="token number">128</span>
        self<span class="token punctuation">.</span>W <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>batchsize<span class="token punctuation">,</span>DIM<span class="token punctuation">,</span>DIM<span class="token punctuation">)</span> <span class="token comment">#&#x4EE3;&#x8868; &#x5DF2;&#x77E5;&#x7684;&#x6570;&#x636E; # &#x72EC;&#x7ACB;&#x540C;&#x5206;&#x5E03;&#x7684;&#x6807;&#x51C6;&#x6B63;&#x592A;&#x5206;&#x5E03;</span>
        self<span class="token punctuation">.</span>Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>batchsize<span class="token punctuation">,</span>DIM<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>batchsize<span class="token punctuation">,</span>self<span class="token punctuation">.</span>DIM<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>x<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
        <span class="token keyword">if</span> USE_CUDA<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>W <span class="token operator">=</span> self<span class="token punctuation">.</span>W<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>Y <span class="token operator">=</span> self<span class="token punctuation">.</span>Y<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>x <span class="token operator">=</span> self<span class="token punctuation">.</span>x<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
            
    <span class="token keyword">def</span> <span class="token function">Reset_Or_Reuse</span><span class="token punctuation">(</span>self <span class="token punctuation">,</span> x <span class="token punctuation">,</span> W <span class="token punctuation">,</span> Y <span class="token punctuation">,</span> state<span class="token punctuation">,</span> num_roll<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&apos;&apos;&apos; re-initialize the `W, Y, x , state`  at the begining of each global training
            IF `num_roll` == 0    &apos;&apos;&apos;</span>

        reset_theta <span class="token operator">=</span>self<span class="token punctuation">.</span>reset_theta
        reset_function_from_IID_distirbution <span class="token operator">=</span> self<span class="token punctuation">.</span>reset_function_from_IID_distirbution

       
        <span class="token keyword">if</span> num_roll <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">and</span> reset_theta <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
            theta <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>batchsize<span class="token punctuation">,</span>DIM<span class="token punctuation">)</span>
           
            theta_init_new <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>theta<span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
            x <span class="token operator">=</span> theta_init_new
            
            
        <span class="token comment">################   &#x6BCF;&#x6B21;&#x5168;&#x5C40;&#x8BAD;&#x7EC3;&#x8FED;&#x4EE3;&#xFF0C;&#x4ECE;&#x72EC;&#x7ACB;&#x540C;&#x5206;&#x5E03;&#x7684;Normal Gaussian&#x91C7;&#x6837;&#x51FD;&#x6570;     ##################</span>
        <span class="token keyword">if</span> num_roll <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">and</span> reset_function_from_IID_distirbution <span class="token operator">==</span> <span class="token boolean">True</span> <span class="token punctuation">:</span>
            W <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>batchsize<span class="token punctuation">,</span>DIM<span class="token punctuation">,</span>DIM<span class="token punctuation">)</span> <span class="token comment">#&#x4EE3;&#x8868; &#x5DF2;&#x77E5;&#x7684;&#x6570;&#x636E; # &#x72EC;&#x7ACB;&#x540C;&#x5206;&#x5E03;&#x7684;&#x6807;&#x51C6;&#x6B63;&#x592A;&#x5206;&#x5E03;</span>
            Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>batchsize<span class="token punctuation">,</span>DIM<span class="token punctuation">)</span>     <span class="token comment">#&#x4EE3;&#x8868; &#x6570;&#x636E;&#x7684;&#x6807;&#x7B7E; #  &#x72EC;&#x7ACB;&#x540C;&#x5206;&#x5E03;&#x7684;&#x6807;&#x51C6;&#x6B63;&#x592A;&#x5206;&#x5E03;</span>
         
            
        <span class="token keyword">if</span> num_roll <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            state <span class="token operator">=</span> <span class="token boolean">None</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&apos;reset W, x , Y, state &apos;</span><span class="token punctuation">)</span>
            
        <span class="token keyword">if</span> USE_CUDA<span class="token punctuation">:</span>
            W <span class="token operator">=</span> W<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
            Y <span class="token operator">=</span> Y<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
            x <span class="token operator">=</span> x<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
            x<span class="token punctuation">.</span>retain_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
          
            
        <span class="token keyword">return</span>  x <span class="token punctuation">,</span> W <span class="token punctuation">,</span> Y <span class="token punctuation">,</span> state

    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_roll<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">:</span> 
        <span class="token triple-quoted-string string">&apos;&apos;&apos;
        Total Training steps = Unroll_Train_Steps * the times of  `Learner` been called
        
        SGD,RMS,LSTM &#x7528;&#x4E0A;&#x8FF0;&#x5B9A;&#x4E49;&#x7684;
         Adam&#x4F18;&#x5316;&#x5668;&#x76F4;&#x63A5;&#x4F7F;&#x7528;pytorch&#x91CC;&#x7684;&#xFF0C;&#x6240;&#x4EE5;&#x4EE3;&#x7801;&#x4E0A;&#x6709;&#x533A;&#x5206; &#x540E;&#x9762;&#x53EF;&#x4EE5;&#x5B8C;&#x5584;&#xFF01;&apos;&apos;&apos;</span>
        f  <span class="token operator">=</span> self<span class="token punctuation">.</span>f 
        x <span class="token punctuation">,</span> W <span class="token punctuation">,</span> Y <span class="token punctuation">,</span> state <span class="token operator">=</span>  self<span class="token punctuation">.</span>Reset_Or_Reuse<span class="token punctuation">(</span>self<span class="token punctuation">.</span>x <span class="token punctuation">,</span> self<span class="token punctuation">.</span>W <span class="token punctuation">,</span> self<span class="token punctuation">.</span>Y <span class="token punctuation">,</span> self<span class="token punctuation">.</span>state <span class="token punctuation">,</span> num_roll <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>global_loss_graph <span class="token operator">=</span> <span class="token number">0</span>   <span class="token comment">#&#x6BCF;&#x4E2A;unroll&#x7684;&#x5F00;&#x59CB;&#x9700;&#x8981; &#x91CD;&#x65B0;&#x7F6E;&#x96F6;</span>
        optimizee <span class="token operator">=</span> self<span class="token punctuation">.</span>optimizee
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&apos;state is None = {}&apos;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>state <span class="token operator">==</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
     
        <span class="token keyword">if</span> optimizee<span class="token operator">!=</span><span class="token string">&apos;Adam&apos;</span><span class="token punctuation">:</span>
            
            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>train_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>     
                loss <span class="token operator">=</span> f<span class="token punctuation">(</span>W<span class="token punctuation">,</span>Y<span class="token punctuation">,</span>x<span class="token punctuation">)</span>
                <span class="token comment">#self.global_loss_graph += (0.8*torch.log10(torch.Tensor([i+1]))+1)*loss</span>
                self<span class="token punctuation">.</span>global_loss_graph <span class="token operator">+=</span> loss
              
                loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span>self<span class="token punctuation">.</span>retain_graph_flag<span class="token punctuation">)</span> <span class="token comment"># &#x9ED8;&#x8BA4;&#x4E3A;False,&#x5F53;&#x4F18;&#x5316;LSTM&#x8BBE;&#x7F6E;&#x4E3A;True</span>
              
                update<span class="token punctuation">,</span> state <span class="token operator">=</span> optimizee<span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">,</span> state<span class="token punctuation">)</span>
              
                self<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
             
                x <span class="token operator">=</span> x <span class="token operator">+</span> update  
                x<span class="token punctuation">.</span>retain_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
                update<span class="token punctuation">.</span>retain_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
                
            <span class="token keyword">if</span> state <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>state <span class="token operator">=</span> <span class="token punctuation">(</span>state<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>state<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                
            <span class="token keyword">return</span> self<span class="token punctuation">.</span>losses <span class="token punctuation">,</span>self<span class="token punctuation">.</span>global_loss_graph 

        <span class="token keyword">else</span><span class="token punctuation">:</span> <span class="token comment">#Pytorch Adam</span>

            x<span class="token punctuation">.</span>detach_<span class="token punctuation">(</span><span class="token punctuation">)</span>
            x<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
            optimizee<span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span> <span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.1</span> <span class="token punctuation">)</span>
            
            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>train_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
                
                optimizee<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
                loss <span class="token operator">=</span> f<span class="token punctuation">(</span>W<span class="token punctuation">,</span>Y<span class="token punctuation">,</span>x<span class="token punctuation">)</span>
                
                self<span class="token punctuation">.</span>global_loss_graph <span class="token operator">+=</span> loss
                
                loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span>self<span class="token punctuation">.</span>retain_graph_flag<span class="token punctuation">)</span>
                optimizee<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>detach_<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                
            <span class="token keyword">return</span> self<span class="token punctuation">.</span>losses<span class="token punctuation">,</span> self<span class="token punctuation">.</span>global_loss_graph


<span class="token comment">#######   LSTM &#x4F18;&#x5316;&#x5668;&#x7684;&#x8BAD;&#x7EC3;&#x8FC7;&#x7A0B; Learning to learn   ###############</span>

<span class="token keyword">def</span> <span class="token function">Learning_to_learn_global_training</span><span class="token punctuation">(</span>optimizee<span class="token punctuation">,</span> global_taining_steps<span class="token punctuation">,</span> Optimizee_Train_Steps<span class="token punctuation">,</span> UnRoll_STEPS<span class="token punctuation">,</span> Evaluate_period <span class="token punctuation">,</span>optimizer_lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot; Training the LSTM optimizee . Learning to learn

    Args:   
        `optimizee` : DeepLSTMCoordinateWise optimizee model
        `global_taining_steps` : how many steps for optimizer training o&#x53EF;&#x4EE5;ptimizee
        `Optimizee_Train_Steps` : how many step for optimizee opimitzing each function sampled from IID.
        `UnRoll_STEPS` :: how many steps for LSTM optimizee being unrolled to construct a computing graph to BPTT.
    &quot;&quot;&quot;</span>
    global_loss_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    Total_Num_Unroll <span class="token operator">=</span> Optimizee_Train_Steps <span class="token operator">//</span> UnRoll_STEPS
    adam_global_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>optimizee<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> optimizer_lr<span class="token punctuation">)</span>

    LSTM_Learner <span class="token operator">=</span> Learner<span class="token punctuation">(</span>f<span class="token punctuation">,</span> optimizee<span class="token punctuation">,</span> UnRoll_STEPS<span class="token punctuation">,</span> retain_graph_flag<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
  <span class="token comment">#&#x8FD9;&#x91CC;&#x8003;&#x8651;Batchsize&#x4EE3;&#x8868;IID&#x7684;&#x8BDD;&#xFF0C;&#x90A3;&#x4E48;&#x5C31;&#x53EF;&#x4EE5;&#x4E0D;&#x9700;&#x8981;&#x6BCF;&#x6B21;&#x90FD;&#x91CD;&#x65B0;IID&#x91C7;&#x6837;</span>
  <span class="token comment">#&#x5373;reset_function_from_IID_distirbution = False &#x5426;&#x5219;&#x4E3A;True</span>

    best_sum_loss <span class="token operator">=</span> <span class="token number">999999</span>
    best_final_loss <span class="token operator">=</span> <span class="token number">999999</span>
    best_flag <span class="token operator">=</span> <span class="token boolean">False</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>Global_Train_Steps<span class="token punctuation">)</span><span class="token punctuation">:</span> 

        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&apos;\n=======&gt; global training steps: {}&apos;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">for</span> num <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>Total_Num_Unroll<span class="token punctuation">)</span><span class="token punctuation">:</span>
            
            start <span class="token operator">=</span> timer<span class="token punctuation">(</span><span class="token punctuation">)</span>
            _<span class="token punctuation">,</span>global_loss <span class="token operator">=</span> LSTM_Learner<span class="token punctuation">(</span>num<span class="token punctuation">)</span>   

            adam_global_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            global_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> 
       
            adam_global_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># print(&apos;xxx&apos;,[(z.grad,z.requires_grad) for z in optimizee.lstm.parameters()  ])</span>
            global_loss_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>global_loss<span class="token punctuation">.</span>detach_<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            time <span class="token operator">=</span> timer<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start
            <span class="token comment">#if i % 10 == 0:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&apos;-&gt; time consuming [{:.1f}s] optimizee train steps :  [{}] | Global_Loss = [{:.1f}] &apos;</span>\
                  <span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>time<span class="token punctuation">,</span><span class="token punctuation">(</span>num <span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span> UnRoll_STEPS<span class="token punctuation">,</span>global_loss<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> Evaluate_period <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            
            best_sum_loss<span class="token punctuation">,</span> best_final_loss<span class="token punctuation">,</span> best_flag  <span class="token operator">=</span> evaluate<span class="token punctuation">(</span>best_sum_loss<span class="token punctuation">,</span>best_final_loss<span class="token punctuation">,</span>best_flag <span class="token punctuation">,</span> optimizer_lr<span class="token punctuation">)</span>

    <span class="token keyword">return</span> global_loss_list<span class="token punctuation">,</span>best_flag


<span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>best_sum_loss<span class="token punctuation">,</span>best_final_loss<span class="token punctuation">,</span> best_flag<span class="token punctuation">,</span>lr<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&apos;\n --&gt; evalute the model&apos;</span><span class="token punctuation">)</span>
    STEPS <span class="token operator">=</span> <span class="token number">100</span>
    LSTM_learner <span class="token operator">=</span> Learner<span class="token punctuation">(</span>f <span class="token punctuation">,</span> LSTM_Optimizee<span class="token punctuation">,</span> STEPS<span class="token punctuation">,</span> eval_flag<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> retain_graph_flag<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    lstm_losses<span class="token punctuation">,</span> sum_loss <span class="token operator">=</span> LSTM_learner<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">try</span><span class="token punctuation">:</span>
        best <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">&apos;best_loss.txt&apos;</span><span class="token punctuation">)</span>
    <span class="token keyword">except</span> IOError<span class="token punctuation">:</span>
        <span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">&apos;can not find best_loss.txt&apos;</span><span class="token punctuation">)</span>
        <span class="token keyword">pass</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        best_sum_loss <span class="token operator">=</span> best<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        best_final_loss <span class="token operator">=</span> best<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;load_best_final_loss and sum_loss&quot;</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> lstm_losses<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> best_final_loss <span class="token operator">and</span>  sum_loss <span class="token operator">&lt;</span> best_sum_loss<span class="token punctuation">:</span>
        best_final_loss <span class="token operator">=</span> lstm_losses<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        best_sum_loss <span class="token operator">=</span>  sum_loss
        
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&apos;\n\n===&gt; update new best of final LOSS[{}]: =  {}, best_sum_loss ={}&apos;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>STEPS<span class="token punctuation">,</span> best_final_loss<span class="token punctuation">,</span>best_sum_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>
        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>LSTM_Optimizee<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">&apos;best_LSTM_optimizer.pth&apos;</span><span class="token punctuation">)</span>
        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token punctuation">[</span>best_sum_loss <span class="token punctuation">,</span>best_final_loss<span class="token punctuation">,</span>lr <span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">&apos;best_loss.txt&apos;</span><span class="token punctuation">)</span>
        best_flag <span class="token operator">=</span> <span class="token boolean">True</span>
        
    <span class="token keyword">return</span> best_sum_loss<span class="token punctuation">,</span> best_final_loss<span class="token punctuation">,</span> best_flag 


</pre><pre class="language-text">USE_CUDA = False


LSTM_Optimizee_Model(
  (lstm): LSTM(10, 20, num_layers=2)
  (Linear): Linear(in_features=20, out_features=10, bias=True)
)
</pre>
<h3 class="mume-header" id="%E6%88%91%E4%BB%AC%E5%85%88%E6%9D%A5%E7%9C%8B%E7%9C%8B%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84lstm%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E6%95%88%E6%9E%9C">&#x6211;&#x4EEC;&#x5148;&#x6765;&#x770B;&#x770B;&#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x7684;LSTM&#x4F18;&#x5316;&#x5668;&#x7684;&#x6548;&#x679C;</h3>

<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token comment">#############  &#x6CE8;&#x610F;&#xFF1A;&#x63A5;&#x4E0A;&#x4E00;&#x7247;&#x6BB5;&#x7684;&#x4EE3;&#x7801;&#xFF01;&#xFF01;   #######################3#</span>
<span class="token comment">##########################   before learning LSTM optimizee ###############################</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

STEPS <span class="token operator">=</span> <span class="token number">100</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>STEPS<span class="token punctuation">)</span>

Adam <span class="token operator">=</span> <span class="token string">&apos;Adam&apos;</span> <span class="token comment">#&#x56E0;&#x4E3A;&#x8FD9;&#x91CC;Adam&#x4F7F;&#x7528;Pytorch</span>

<span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> 
   
    SGD_Learner <span class="token operator">=</span> Learner<span class="token punctuation">(</span>f <span class="token punctuation">,</span> SGD<span class="token punctuation">,</span> STEPS<span class="token punctuation">,</span> eval_flag<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
    RMS_Learner <span class="token operator">=</span> Learner<span class="token punctuation">(</span>f <span class="token punctuation">,</span> RMS<span class="token punctuation">,</span> STEPS<span class="token punctuation">,</span> eval_flag<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
    Adam_Learner <span class="token operator">=</span> Learner<span class="token punctuation">(</span>f <span class="token punctuation">,</span> Adam<span class="token punctuation">,</span> STEPS<span class="token punctuation">,</span> eval_flag<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
    LSTM_learner <span class="token operator">=</span> Learner<span class="token punctuation">(</span>f <span class="token punctuation">,</span> LSTM_Optimizee<span class="token punctuation">,</span> STEPS<span class="token punctuation">,</span> eval_flag<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>retain_graph_flag<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

    sgd_losses<span class="token punctuation">,</span> sgd_sum_loss <span class="token operator">=</span> SGD_Learner<span class="token punctuation">(</span><span class="token punctuation">)</span>
    rms_losses<span class="token punctuation">,</span> rms_sum_loss <span class="token operator">=</span> RMS_Learner<span class="token punctuation">(</span><span class="token punctuation">)</span>
    adam_losses<span class="token punctuation">,</span> adam_sum_loss <span class="token operator">=</span> Adam_Learner<span class="token punctuation">(</span><span class="token punctuation">)</span>
    lstm_losses<span class="token punctuation">,</span> lstm_sum_loss <span class="token operator">=</span> LSTM_learner<span class="token punctuation">(</span><span class="token punctuation">)</span>

    p1<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> sgd_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;SGD&apos;</span><span class="token punctuation">)</span>
    p2<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> rms_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;RMS&apos;</span><span class="token punctuation">)</span>
    p3<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> adam_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;Adam&apos;</span><span class="token punctuation">)</span>
    p4<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> lstm_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;LSTM&apos;</span><span class="token punctuation">)</span>
    p1<span class="token punctuation">.</span>set_dashes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 2pt line, 2pt break, 10pt line, 2pt break</span>
    p2<span class="token punctuation">.</span>set_dashes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 2pt line, 2pt break, 10pt line, 2pt break</span>
    p3<span class="token punctuation">.</span>set_dashes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 2pt line, 2pt break, 10pt line, 2pt break</span>
    plt<span class="token punctuation">.</span>yscale<span class="token punctuation">(</span><span class="token string">&apos;log&apos;</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>handles<span class="token operator">=</span><span class="token punctuation">[</span>p1<span class="token punctuation">,</span> p2<span class="token punctuation">,</span> p3<span class="token punctuation">,</span> p4<span class="token punctuation">]</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">&apos;Losses&apos;</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\n\nsum_loss:sgd={},rms={},adam={},lstm={}&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>sgd_sum_loss<span class="token punctuation">,</span>rms_sum_loss<span class="token punctuation">,</span>adam_sum_loss<span class="token punctuation">,</span>lstm_sum_loss <span class="token punctuation">)</span><span class="token punctuation">)</span>
</pre><pre class="language-text">reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True
</pre>
<p><img src="https://img-blog.csdnimg.cn/20181123202026137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="&#x5728;&#x8FD9;&#x91CC;&#x63D2;&#x5165;&#x56FE;&#x7247;&#x63CF;&#x8FF0;"></p>
<pre class="language-text">sum_loss:sgd=945.0716552734375,rms=269.4500427246094,adam=134.2750244140625,lstm=562912.125
</pre>
<h5 class="mume-header" id="%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84lstm%E4%BC%98%E5%8C%96%E5%99%A8%E6%B2%A1%E6%9C%89%E4%BB%BB%E4%BD%95%E6%95%88%E6%9E%9Closs%E5%8F%91%E6%95%A3%E4%BA%86%E5%9B%A0%E4%B8%BA%E8%BF%98%E6%B2%A1%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96%E5%99%A8">&#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x7684;LSTM&#x4F18;&#x5316;&#x5668;&#x6CA1;&#x6709;&#x4EFB;&#x4F55;&#x6548;&#x679C;&#xFF0C;loss&#x53D1;&#x6563;&#x4E86;&#xFF0C;&#x56E0;&#x4E3A;&#x8FD8;&#x6CA1;&#x8BAD;&#x7EC3;&#x4F18;&#x5316;&#x5668;</h5>

<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token comment">#######   &#x6CE8;&#x610F;&#xFF1A;&#x63A5;&#x4E0A;&#x4E00;&#x6BB5;&#x7684;&#x4EE3;&#x7801;&#xFF01;&#xFF01;</span>
<span class="token comment">#################### Learning to learn (&#x4F18;&#x5316;optimizee) ######################</span>
Global_Train_Steps <span class="token operator">=</span> <span class="token number">1000</span> <span class="token comment">#&#x53EF;&#x4FEE;&#x6539;</span>
Optimizee_Train_Steps <span class="token operator">=</span> <span class="token number">100</span>
UnRoll_STEPS <span class="token operator">=</span> <span class="token number">20</span>
Evaluate_period <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment">#&#x53EF;&#x4FEE;&#x6539;</span>
optimizer_lr <span class="token operator">=</span> <span class="token number">0.1</span> <span class="token comment">#&#x53EF;&#x4FEE;&#x6539;</span>
global_loss_list <span class="token punctuation">,</span>flag <span class="token operator">=</span> Learning_to_learn_global_training<span class="token punctuation">(</span>   LSTM_Optimizee<span class="token punctuation">,</span>
                                                        Global_Train_Steps<span class="token punctuation">,</span>
                                                        Optimizee_Train_Steps<span class="token punctuation">,</span>
                                                        UnRoll_STEPS<span class="token punctuation">,</span>
                                                        Evaluate_period<span class="token punctuation">,</span>
                                                          optimizer_lr<span class="token punctuation">)</span>


<span class="token comment">######################################################################3#</span>
<span class="token comment">##########################   show learning process results </span>
<span class="token comment">#torch.load(&apos;best_LSTM_optimizer.pth&apos;))</span>
<span class="token comment">#import numpy as np</span>
<span class="token comment">#import matplotlib</span>
<span class="token comment">#import matplotlib.pyplot as plt</span>

<span class="token comment">#Global_T = np.arange(len(global_loss_list))</span>
<span class="token comment">#p1, = plt.plot(Global_T, global_loss_list, label=&apos;Global_graph_loss&apos;)</span>
<span class="token comment">#plt.legend(handles=[p1])</span>
<span class="token comment">#plt.title(&apos;Training LSTM optimizee by gradient descent &apos;)</span>
<span class="token comment">#plt.show()</span>
</pre><pre class="language-text">=======&gt; global training steps: 0
reset W, x , Y, state 
state is None = True
-&gt; time consuming [0.2s] optimizee train steps :  [20] | Global_Loss = [4009.4] 
state is None = False
-&gt; time consuming [0.3s] optimizee train steps :  [40] | Global_Loss = [21136.7] 
state is None = False
-&gt; time consuming [0.2s] optimizee train steps :  [60] | Global_Loss = [136640.5] 
state is None = False
-&gt; time consuming [0.2s] optimizee train steps :  [80] | Global_Loss = [4017.9] 
state is None = False
-&gt; time consuming [0.2s] optimizee train steps :  [100] | Global_Loss = [9107.1] 


 --&gt; evalute the model
reset W, x , Y, state 
state is None = True

...........
...........
</pre>
<p>&#x8F93;&#x51FA;&#x7ED3;&#x679C;&#x5DF2;&#x7ECF;&#x7701;&#x7565;&#x5927;&#x90E8;&#x5206;</p>
<h5 class="mume-header" id="%E6%8E%A5%E4%B8%8B%E6%9D%A5%E7%9C%8B%E4%B8%80%E4%B8%8B%E4%BC%98%E5%8C%96%E5%A5%BD%E7%9A%84lstm%E4%BC%98%E5%8C%96%E5%99%A8%E6%A8%A1%E5%9E%8B%E5%92%8Csgdrmspropadam%E7%9A%84%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94%E8%A1%A8%E7%8E%B0%E5%90%A7~">&#x63A5;&#x4E0B;&#x6765;&#x770B;&#x4E00;&#x4E0B;&#x4F18;&#x5316;&#x597D;&#x7684;LSTM&#x4F18;&#x5316;&#x5668;&#x6A21;&#x578B;&#x548C;SGD&#xFF0C;RMSProp&#xFF0C;Adam&#x7684;&#x4F18;&#x5316;&#x6027;&#x80FD;&#x5BF9;&#x6BD4;&#x8868;&#x73B0;&#x5427;~</h5>

<p>&#x9E21;&#x51BB;</p>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token comment">###############  **&#x6CE8;&#x610F;&#xFF1A; **&#x63A5;&#x4E0A;&#x4E00;&#x7247;&#x6BB5;&#x7684;&#x4EE3;&#x7801;****</span>
<span class="token comment">######################################################################3#</span>
<span class="token comment">##########################   show contrast results SGD,ADAM, RMS ,LSTM ###############################</span>
<span class="token keyword">import</span> copy
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token keyword">if</span> flag <span class="token operator">==</span><span class="token boolean">True</span> <span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&apos;\n==== &gt; load best LSTM model&apos;</span><span class="token punctuation">)</span>
    last_state_dict <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>LSTM_Optimizee<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>LSTM_Optimizee<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">&apos;final_LSTM_optimizer.pth&apos;</span><span class="token punctuation">)</span>
    LSTM_Optimizee<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">&apos;best_LSTM_optimizer.pth&apos;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
LSTM_Optimizee<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">&apos;best_LSTM_optimizer.pth&apos;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#LSTM_Optimizee.load_state_dict(torch.load(&apos;final_LSTM_optimizer.pth&apos;))</span>
STEPS <span class="token operator">=</span> <span class="token number">100</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>STEPS<span class="token punctuation">)</span>

Adam <span class="token operator">=</span> <span class="token string">&apos;Adam&apos;</span> <span class="token comment">#&#x56E0;&#x4E3A;&#x8FD9;&#x91CC;Adam&#x4F7F;&#x7528;Pytorch</span>

<span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#&#x53EF;&#x4EE5;&#x591A;&#x8BD5;&#x51E0;&#x6B21;&#x6D4B;&#x8BD5;&#x5B9E;&#x9A8C;&#xFF0C;LSTM&#x4E0D;&#x7A33;&#x5B9A;</span>
    
    SGD_Learner <span class="token operator">=</span> Learner<span class="token punctuation">(</span>f <span class="token punctuation">,</span> SGD<span class="token punctuation">,</span> STEPS<span class="token punctuation">,</span> eval_flag<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
    RMS_Learner <span class="token operator">=</span> Learner<span class="token punctuation">(</span>f <span class="token punctuation">,</span> RMS<span class="token punctuation">,</span> STEPS<span class="token punctuation">,</span> eval_flag<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
    Adam_Learner <span class="token operator">=</span> Learner<span class="token punctuation">(</span>f <span class="token punctuation">,</span> Adam<span class="token punctuation">,</span> STEPS<span class="token punctuation">,</span> eval_flag<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
    LSTM_learner <span class="token operator">=</span> Learner<span class="token punctuation">(</span>f <span class="token punctuation">,</span> LSTM_Optimizee<span class="token punctuation">,</span> STEPS<span class="token punctuation">,</span> eval_flag<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>reset_theta<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>retain_graph_flag<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

    
    sgd_losses<span class="token punctuation">,</span> sgd_sum_loss <span class="token operator">=</span> SGD_Learner<span class="token punctuation">(</span><span class="token punctuation">)</span>
    rms_losses<span class="token punctuation">,</span> rms_sum_loss <span class="token operator">=</span> RMS_Learner<span class="token punctuation">(</span><span class="token punctuation">)</span>
    adam_losses<span class="token punctuation">,</span> adam_sum_loss <span class="token operator">=</span> Adam_Learner<span class="token punctuation">(</span><span class="token punctuation">)</span>
    lstm_losses<span class="token punctuation">,</span> lstm_sum_loss <span class="token operator">=</span> LSTM_learner<span class="token punctuation">(</span><span class="token punctuation">)</span>

    p1<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> sgd_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;SGD&apos;</span><span class="token punctuation">)</span>
    p2<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> rms_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;RMS&apos;</span><span class="token punctuation">)</span>
    p3<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> adam_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;Adam&apos;</span><span class="token punctuation">)</span>
    p4<span class="token punctuation">,</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> lstm_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&apos;LSTM&apos;</span><span class="token punctuation">)</span>
    p1<span class="token punctuation">.</span>set_dashes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 2pt line, 2pt break, 10pt line, 2pt break</span>
    p2<span class="token punctuation">.</span>set_dashes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 2pt line, 2pt break, 10pt line, 2pt break</span>
    p3<span class="token punctuation">.</span>set_dashes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 2pt line, 2pt break, 10pt line, 2pt break</span>
    <span class="token comment">#p4.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break</span>
    plt<span class="token punctuation">.</span>yscale<span class="token punctuation">(</span><span class="token string">&apos;log&apos;</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>handles<span class="token operator">=</span><span class="token punctuation">[</span>p1<span class="token punctuation">,</span> p2<span class="token punctuation">,</span> p3<span class="token punctuation">,</span> p4<span class="token punctuation">]</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">&apos;Losses&apos;</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\n\nsum_loss:sgd={},rms={},adam={},lstm={}&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>sgd_sum_loss<span class="token punctuation">,</span>rms_sum_loss<span class="token punctuation">,</span>adam_sum_loss<span class="token punctuation">,</span>lstm_sum_loss <span class="token punctuation">)</span><span class="token punctuation">)</span>
</pre><pre class="language-text">==== &gt; load best LSTM model
reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True
</pre>
<p><img src="https://img-blog.csdnimg.cn/20181123204117756.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="&#x5728;&#x8FD9;&#x91CC;&#x63D2;&#x5165;&#x56FE;&#x7247;&#x63CF;&#x8FF0;"></p>
<pre class="language-text">sum_loss:sgd=967.908935546875,rms=257.03814697265625,adam=122.87742614746094,lstm=105.06891632080078
reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True
</pre>
<p><img src="https://img-blog.csdnimg.cn/20181123202329192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="&#x5728;&#x8FD9;&#x91CC;&#x63D2;&#x5165;&#x56FE;&#x7247;&#x63CF;&#x8FF0;"></p>
<pre class="language-text">sum_loss:sgd=966.5319213867188,rms=277.1605224609375,adam=143.6751251220703,lstm=109.35062408447266
</pre>
<p>&#xFF08;&#x4EE5;&#x4E0A;&#x4EE3;&#x7801;&#x5728;&#x4E00;&#x4E2A;&#x6587;&#x4EF6;&#x91CC;&#x9762;&#x6267;&#x884C;&#x3002;&#x590D;&#x5236;&#x7C98;&#x8D34;&#x683C;&#x5F0F;&#x4EE3;&#x7801;&#x597D;&#x50CF;&#x9700;&#x8981;Chrome&#x6216;&#x8005;IE&#x6D4F;&#x89C8;&#x5668;&#x6253;&#x5F00;&#x624D;&#x884C;&#xFF1F;&#xFF1F;&#xFF1F;&#xFF09;</p>
<h2 class="mume-header" id="%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90%E4%B8%8E%E7%BB%93%E8%AE%BA">&#x5B9E;&#x9A8C;&#x7ED3;&#x679C;&#x5206;&#x6790;&#x4E0E;&#x7ED3;&#x8BBA;</h2>

<p>&#x53EF;&#x4EE5;&#x770B;&#x5230;&#xFF1A;<mark><strong>SGD &#x4F18;&#x5316;&#x5668;</strong>&#x5BF9;&#x4E8E;&#x8FD9;&#x4E2A;&#x95EE;&#x9898;&#x5DF2;&#x7ECF;<strong>&#x4E0D;&#x5177;&#x5907;&#x4F18;&#x5316;&#x80FD;&#x529B;</strong>&#xFF0C;RMSprop&#x4F18;&#x5316;&#x5668;&#x8868;&#x73B0;&#x826F;&#x597D;&#xFF0C;<strong>Adam&#x4F18;&#x5316;&#x5668;&#x8868;&#x73B0;&#x4F9D;&#x65E7;&#x7A81;&#x51FA;&#xFF0C;LSTM&#x4F18;&#x5316;&#x5668;&#x80FD;&#x591F;&#x5AB2;&#x7F8E;&#x751A;&#x81F3;&#x8D85;&#x8D8A;Adam&#xFF08;Adam&#x5DF2;&#x7ECF;&#x662F;&#x4E1A;&#x754C;&#x8BA4;&#x53EF;&#x5E76;&#x5927;&#x89C4;&#x6A21;&#x4F7F;&#x7528;&#x7684;&#x4F18;&#x5316;&#x5668;&#x4E86;&#xFF09;</strong></mark></p>
<h3 class="mume-header" id="%E8%AF%B7%E6%B3%A8%E6%84%8Flstm%E4%BC%98%E5%8C%96%E5%99%A8%E6%9C%80%E7%BB%88%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5%E6%98%AF%E6%B2%A1%E6%9C%89%E4%BB%BB%E4%BD%95%E4%BA%BA%E5%B7%A5%E8%AE%BE%E8%AE%A1%E7%9A%84%E7%BB%8F%E9%AA%8C">&#x8BF7;&#x6CE8;&#x610F;&#xFF1A;LSTM&#x4F18;&#x5316;&#x5668;&#x6700;&#x7EC8;&#x4F18;&#x5316;&#x7B56;&#x7565;&#x662F;&#x6CA1;&#x6709;&#x4EFB;&#x4F55;&#x4EBA;&#x5DE5;&#x8BBE;&#x8BA1;&#x7684;&#x7ECF;&#x9A8C;</h3>

<p><strong>&#x662F;&#x81EA;&#x52A8;&#x5B66;&#x4E60;&#x51FA;&#x7684;&#x4E00;&#x79CD;&#x5B66;&#x4E60;&#x7B56;&#x7565;&#xFF01;&#x5E76;&#x4E14;&#x8FD9;&#x79CD;&#x65B9;&#x6CD5;&#x7406;&#x8BBA;&#x4E0A;&#x53EF;&#x4EE5;&#x5E94;&#x7528;&#x5230;&#x4EFB;&#x4F55;&#x4F18;&#x5316;&#x95EE;&#x9898;</strong></p>
<p>&#x6362;&#x4E00;&#x4E2A;&#x89D2;&#x5EA6;&#x8BB2;&#xFF0C;&#x9488;&#x5BF9;&#x7ED9;&#x5B9A;&#x7684;&#x4F18;&#x5316;&#x95EE;&#x9898;&#xFF0C;LSTM&#x53EF;&#x4EE5;&#x903C;&#x8FD1;&#x6216;&#x8D85;&#x8D8A;&#x73B0;&#x6709;&#x7684;&#x4EFB;&#x4F55;&#x4EBA;&#x5DE5;&#x4F18;&#x5316;&#x5668;&#xFF0C;&#x4E0D;&#x8FC7;&#x5BF9;&#x4E8E;&#x5927;&#x578B;&#x7684;&#x7F51;&#x7EDC;&#x548C;&#x590D;&#x6742;&#x7684;&#x4F18;&#x5316;&#x95EE;&#x9898;&#xFF0C;&#x8FD9;&#x4E2A;&#x65B9;&#x6CD5;&#x7684;&#x4F18;&#x5316;&#x6210;&#x672C;&#x592A;&#x5927;&#xFF0C;&#x4F18;&#x5316;&#x5668;&#x6027;&#x80FD;&#x7684;&#x7A33;&#x5B9A;&#x6027;&#x4E5F;&#x503C;&#x5F97;&#x8003;&#x8651;&#xFF0C;&#x6240;&#x4EE5;&#x8FD9;&#x4E2A;&#x5DE5;&#x4F5C;&#x7684;&#x521B;&#x610F;&#x662F;&#x72EC;&#x7279;&#x7684;&#xFF0C;&#x5B9E;&#x7528;&#x6027;&#x6709;&#x5F85;&#x8003;&#x8651;~~</p>
<p>&#x4EE5;&#x4E0A;&#x4EE3;&#x7801;&#x53C2;&#x8003;Deepmind&#x7684;<a href="https://github.com/deepmind/learning-to-learn">Tensorflow&#x7248;&#x672C;</a>&#xFF0C;&#x9075;&#x7167;&#x8BBA;&#x6587;&#x601D;&#x8DEF;&#xFF0C;&#x52A0;&#x4E0A;&#x4E2A;&#x4EBA;&#x7406;&#x89E3;&#xFF0C;&#x529B;&#x6C42;&#x6700;&#x7B80;&#xFF0C;&#x5F88;&#x591A;&#x5730;&#x65B9;&#x5199;&#x5F97;&#x4E0D;&#x591F;&#x5B8C;&#x5907;&#xFF0C;&#x5982;&#x679C;&#x6709;&#x95EE;&#x9898;&#xFF0C;&#x8FD8;&#x8BF7;&#x591A;&#x591A;&#x6307;&#x51FA;&#xFF01;</p>
<p><img src="https://img-blog.csdnimg.cn/20181125191428527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="&#x5728;&#x8FD9;&#x91CC;&#x63D2;&#x5165;&#x56FE;&#x7247;&#x63CF;&#x8FF0;"><br>
&#x4E0A;&#x56FE;&#x662F;&#x8BBA;&#x6587;&#x4E2D;&#x7ED9;&#x51FA;&#x7684;&#x7ED3;&#x679C;&#xFF0C;&#x4F5C;&#x8005;&#x53D6;&#x6700;&#x597D;&#x7684;&#x5B9E;&#x9A8C;&#x7ED3;&#x679C;&#x7684;&#x5E73;&#x5747;&#x8868;&#x73B0;&#xFF08;&#x8BD5;&#x51FA;&#x4E86;&#x6700;&#x4F73;&#x5B66;&#x4E60;&#x7387;&#xFF1F;&#xFF09;&#x5C55;&#x793A;&#xFF0C;&#x6211;&#x7528;&#x4E0B;&#x9762;&#x548C;&#x8BBA;&#x6587;&#x4E2D;&#x4E00;&#x6837;&#x7684;&#x5B9E;&#x9A8C;&#x6761;&#x4EF6;&#xFF08;&#x4E0D;&#x8FC7;&#x6CA1;&#x4F7F;&#x7528;NAG&#x4F18;&#x5316;&#x5668;&#xFF09;&#xFF0C;&#x57FA;&#x672C;&#x8FBE;&#x5230;&#x4E86;&#x8BBA;&#x6587;&#x4E2D;&#x6240;&#x793A;&#x7684;&#x540C;&#x6837;&#x6548;&#x679C;&#xFF1F;&#x6027;&#x80FD;&#x7A33;&#x5B9A;&#x6027;&#x8F83;&#x5DEE;&#x4E00;&#x4E9B;</p>
<p>&#xFF08;&#x6211;&#x6000;&#x7591;&#x8BBA;&#x6587;&#x7684;&#x5B9E;&#x9A8C;Batchsize&#x4E0D;&#x662F;&#x4EE3;&#x7801;&#x4E2D;&#x7684;128&#xFF1F;&#x53C8;&#x6216;&#x8005;&#x4F5C;&#x8005;&#x628A;batchsize&#x5F53;&#x4F5C;&#x51FD;&#x6570;&#x7684;&#x968F;&#x673A;&#x91C7;&#x6837;&#xFF1F;&#x6211;&#x8FD9;&#x91CC;&#x628A;batchsize&#x5F53;&#x4F5C;&#x786E;&#x5B9A;&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x968F;&#x673A;&#x91C7;&#x6837;&#x5355;&#x72EC;&#x7F16;&#x5199;&#xFF09;&#x3002;</p>
<h2 class="mume-header" id="%E5%AE%9E%E9%AA%8C%E6%9D%A1%E4%BB%B6">&#x5B9E;&#x9A8C;&#x6761;&#x4EF6;&#xFF1A;</h2>

<ul>
<li>
<p>PyTorch-0.4.1  cpu</p>
</li>
<li>
<p>&#x4F18;&#x5316;&#x95EE;&#x9898;&#x4E3A;Quadratic&#x51FD;&#x6570;:</p>
</li>
<li>
<p>W : [128,10,10]  Y: [128 , 10] x: [128, 10] &#x4ECE;IID&#x7684;&#x6807;&#x51C6;Gaussian&#x5206;&#x5E03;&#x4E2D;&#x91C7;&#x6837;&#xFF0C;&#x521D;&#x59CB; x = 0</p>
</li>
<li>
<p>&#x5168;&#x5C40;&#x4F18;&#x5316;&#x5668;optimizer&#x4F7F;&#x7528;Adam&#x4F18;&#x5316;&#x5668;&#xFF0C; &#x5B66;&#x4E60;&#x7387;&#x4E3A;0.1&#xFF08;&#x6216;&#x8BB8;&#x6709;&#x66F4;&#x597D;&#x7684;&#x9009;&#x62E9;&#xFF0C;&#x6CA1;&#x6709;&#x8FDB;&#x884C;&#x5BF9;&#x6BD4;&#x5B9E;&#x9A8C;&#xFF09;</p>
</li>
<li>
<p>CoordinateWise LSTM &#x4F7F;&#x7528;LSTM_Optimizee_Model&#xFF1A;</p>
<ul>
<li>(lstm): LSTM(10, 20, num_layers=2)</li>
<li>(Linear): Linear(in_features=20, out_features=10, bias=True)</li>
</ul>
</li>
<li>
<p><mark>&#x672A;&#x4F7F;&#x7528;&#x4EFB;&#x4F55;&#x6570;&#x636E;&#x9884;&#x5904;&#x7406;</mark>&#xFF08;LogAndSign&#xFF09;&#x548C;&#x540E;&#x5904;&#x7406;</p>
</li>
<li>
<p>UnRolled Steps = 20 Optimizee_Training_Steps = 100</p>
</li>
<li>
<p>*Global_Traing_steps = 1000 &#x539F;&#x4EE3;&#x7801;=10000&#xFF0C;&#x6216;&#x8BB8;&#x8FDB;&#x4E00;&#x6B65;&#x4F18;&#x5316;LSTM&#x4F18;&#x5316;&#x5668;&#xFF0C;&#x80FD;&#x591F;&#x5230;&#x8FBE;&#x66F4;&#x7A33;&#x5B9A;&#x7684;&#x6548;&#x679C;&#x3002;</p>
</li>
<li>
<p>&#x53E6;&#x5916;&#xFF0C;&#x539F;&#x8BBA;&#x6587;&#x8FDB;&#x884C;&#x4E86;mnist&#x548C;cifar10&#x7684;&#x5B9E;&#x9A8C;&#xFF0C;&#x672C;&#x7BC7;&#x535A;&#x5BA2;&#x6CA1;&#x6709;&#x8FDB;&#x884C;&#x5B9E;&#x9A8C;&#xFF0C;&#x4EE3;&#x7801;&#x90E8;&#x5206;&#x8FD8;&#x6709;&#x5F85;&#x5B8C;&#x5584;&#xFF0C;&#x8FD8;&#x662F;&#x5E0C;&#x671B;&#x8BFB;&#x8005;&#x591A;&#x8BFB;&#x539F;&#x8BBA;&#x6587;&#x548C;&#x539F;&#x4EE3;&#x7801;&#xFF0C;&#x591A;&#x52A8;&#x624B;&#x7F16;&#x7A0B;&#x5B9E;&#x9A8C;&#xFF01;</p>
</li>
</ul>
<pre data-role="codeBlock" data-info="python" class="language-python"><span class="token keyword">def</span> <span class="token function">learn</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">return</span> <span class="token string">&quot;&#x77E5;&#x8BC6;&quot;</span>

<span class="token keyword">def</span> <span class="token function">learning_to</span><span class="token punctuation">(</span>learn<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>learn<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token string">&quot;&#x5B66;&#x4E60;&#x7B56;&#x7565;&quot;</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>learning_to<span class="token punctuation">(</span>learn<span class="token punctuation">)</span><span class="token punctuation">)</span>
</pre><h2 class="mume-header" id="%E5%90%8E%E5%8F%99">&#x540E;&#x53D9;</h2>

<blockquote>
<p>&#x4EBA;&#x53EF;&#x4EE5;&#x4ECE;&#x81EA;&#x8EAB;&#x8BA4;&#x8BC6;&#x4E0E;&#x5BA2;&#x89C2;&#x5B58;&#x5728;&#x7684;&#x5DEE;&#x5F02;&#x4E2D;&#x5B66;&#x4E60;&#xFF0C;&#x6765;&#x4E0D;&#x65AD;&#x7684;&#x63D0;&#x5347;&#x8BA4;&#x77E5;&#x80FD;&#x529B;&#xFF0C;&#x8FD9;&#x662F;&#x6700;&#x57FA;&#x672C;&#x7684;&#x5B66;&#x4E60;&#x80FD;&#x529B;&#x3002;&#x800C;&#x53E6;&#x4E00;&#x79CD;&#x6F5C;&#x5728;&#x4E0D;&#x5BB9;&#x6613;&#x53D1;&#x6398;&#xFF0C;&#x4F46;&#x5374;&#x662F;&#x66F4;&#x5F3A;&#x5927;&#x7684;&#x80FD;&#x529B;--&#x5728;&#x5B66;&#x4E60;&#x4E2D;&#x4E0D;&#x65AD;&#x8C03;&#x6574;&#x9002;&#x5E94;&#x81EA;&#x8EAB;&#x4E0E;&#x5916;&#x754C;&#x7684;&#x5B66;&#x4E60;&#x6280;&#x5DE7;&#x6216;&#x8005;&#x89C4;&#x5219;--&#x5176;&#x5B9E;&#x6784;&#x5EFA;&#x4E86;&#x6211;&#x4EEC;&#x66F4;&#x9AD8;&#x9636;&#x7684;&#x667A;&#x80FD;&#x3002;&#x6BD4;&#x5982;&#xFF0C;&#x6211;&#x4EEC;&#x5728;&#x5B66;&#x4E60;&#x77E5;&#x8BC6;&#x65F6;&#xFF0C;&#x6211;&#x4EEC;&#x603B;&#x4F1A;&#x5148;&#x63A5;&#x89E6;&#x4E00;&#x4E9B;&#x7B80;&#x5355;&#x5BB9;&#x6613;&#x7406;&#x89E3;&#x7684;&#x57FA;&#x672C;&#x6982;&#x5FF5;&#xFF0C;&#x9047;&#x5230;&#x4E00;&#x4E9B;&#x7406;&#x89E3;&#x8D77;&#x6765;&#x56F0;&#x96BE;&#x6216;&#x8005;&#x5341;&#x5206;&#x62BD;&#x8C61;&#x7684;&#x6982;&#x5FF5;&#x65F6;&#xFF0C;&#x6211;&#x4EEC;&#x5F80;&#x5F80;&#x4E0D;&#x662F;&#x91C7;&#x53D6;&#x5F3A;&#x884C;&#x8BB0;&#x5FC6;&#xFF0C;&#x5373;&#x6211;&#x4EEC;&#x5E76;&#x4E0D;&#x4F1A;&#x7ACB;&#x523B;&#x5B66;&#x4E60;&#x8DDF;&#x6211;&#x4EEC;&#x5F53;&#x524D;&#x8BA4;&#x77E5;&#x7684;&#x504F;&#x5DEE;&#x975E;&#x5E38;&#x5927;&#x7684;&#x4E8B;&#x7269;&#xFF0C;&#x800C;&#x662F;&#x628A;&#x5B83;&#x5148;&#x653E;&#x5230;&#x4E00;&#x8FB9;&#xFF0C;&#x7EE7;&#x7EED;&#x5B66;&#x4E60;&#x66F4;&#x591A;&#x7B80;&#x5355;&#x7684;&#x6982;&#x5FF5;&#xFF0C;&#x76F4;&#x5230;&#x6709;&#x6240;&#x201C;&#x9886;&#x609F;&#x201D;&#x53D1;&#x73B0;&#x5148;&#x524D;&#x7684;&#x56F0;&#x96BE;&#x6982;&#x5FF5;&#x53D8;&#x5F97;&#x5BB9;&#x6613;&#x7406;&#x89E3;</p>
</blockquote>
<blockquote>
<p>&#x5FC3;&#x7406;&#x5B66;&#x4E0A;&#xFF0C;&#x5143;&#x8BA4;&#x77E5;&#x88AB;&#x79F0;&#x4F5C;&#x53CD;&#x7701;&#x8BA4;&#x77E5;&#xFF0C;&#x6307;&#x4EBA;&#x5BF9;&#x81EA;&#x6211;&#x8BA4;&#x77E5;&#x7684;&#x8BA4;&#x77E5;&#x3002;&#x5F17;&#x62C9;&#x5A01;&#x5C14;&#x79F0;&#xFF0C;&#x5143;&#x8BA4;&#x77E5;&#x662F;&#x5173;&#x4E8E;&#x4E2A;&#x4EBA;&#x81EA;&#x5DF1;&#x8BA4;&#x77E5;&#x8FC7;&#x7A0B;&#x7684;&#x77E5;&#x8BC6;&#x548C;&#x8C03;&#x8282;&#x8FD9;&#x4E9B;&#x8FC7;&#x7A0B;&#x7684;&#x80FD;&#x529B;&#xFF1A;&#x5BF9;&#x601D;&#x7EF4;&#x548C;&#x5B66;&#x4E60;&#x6D3B;&#x52A8;&#x7684;&#x77E5;&#x8BC6;&#x548C;&#x63A7;&#x5236;&#x3002;&#x90A3;&#x4E48;&#x5B66;&#x4F1A;&#x9002;&#x5E94;&#x6027;&#x5730;&#x8C03;&#x6574;&#x5B66;&#x4E60;&#x7B56;&#x7565;&#xFF0C;&#x4E5F;&#x6210;&#x4E3A;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x7684;&#x4E00;&#x4E2A;&#x7814;&#x7A76;&#x8BFE;&#x9898;&#xFF0C;a most ordinary problem for machine learning is that although we expect to find the invariant pattern in all data, for an individual instance in a specified dataset&#xFF0C;it has its own unique attribute, which requires the model taking different policy to understand them seperately .</p>
</blockquote>
<blockquote>
<p>&#x4EE5;&#x4E0A;&#x5747;&#x4E3A;&#x539F;&#x521B;&#xFF0C;&#x8F6C;&#x8F7D;&#x8BF7;&#x6CE8;&#x660E;&#x6765;&#x6E90;<br>
<a href="https://blog.csdn.net/senius/article/details/84483329">https://blog.csdn.net/senius/article/details/84483329</a><br>
or <a href="https://yangsenius.github.io/blog/LSTM_Meta/">https://yangsenius.github.io/blog/LSTM_Meta/</a><br>
&#x6E9C;&#x4E86;&#x6E9C;&#x4E86;</p>
</blockquote>
<h2 class="mume-header" id="%E4%B8%8B%E8%BD%BD%E5%9C%B0%E5%9D%80%E4%B8%8E%E5%8F%82%E8%80%83">&#x4E0B;&#x8F7D;&#x5730;&#x5740;&#x4E0E;&#x53C2;&#x8003;</h2>

<p><strong><a href="https://yangsenius.github.io/blog/LSTM_Meta/learning_to_learn_by_pytorch.py">&#x4E0B;&#x8F7D;&#x4EE3;&#x7801;: learning_to_learn_by_pytorch.py</a></strong></p>
<p><a href="https://github.com/yangsenius/learning-to-learn-by-pytorch">Github&#x5730;&#x5740;</a></p>
<p>&#x53C2;&#x8003;&#xFF1A;<br>
<a href="https://arxiv.org/abs/1606.04474">1.Learning to learn by gradient descent by gradient descent</a><br>
<a href="https://github.com/deepmind/learning-to-learn">2. Learning to learn in Tensorflow  by DeepMind</a><br>
<a href="https://hackernoon.com/learning-to-learn-by-gradient-descent-by-gradient-descent-4da2273d64f2">3.learning-to-learn-by-gradient-descent-by-gradient-descent-4da2273d64f2</a></p>
<p><em>&#x76EE;&#x5F55;</em></p>
<ul>
<li><a href="#%E5%BC%95%E8%A8%80">&#x5F15;&#x8A00;</a></li>
<li><a href="#learning-to-learn-by-gradient-descent-by-gradient-descent">Learning to learn  by gradient descent  by gradient descent</a></li>
<li><a href="#%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95">&#x6587;&#x7AE0;&#x76EE;&#x5F55;</a></li>
<li><a href="#%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98">&#x4F18;&#x5316;&#x95EE;&#x9898;</a>
<ul>
<li><a href="#%E5%AE%9A%E4%B9%89%E8%A6%81%E4%BC%98%E5%8C%96%E7%9A%84%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0">&#x5B9A;&#x4E49;&#x8981;&#x4F18;&#x5316;&#x7684;&#x76EE;&#x6807;&#x51FD;&#x6570;</a></li>
<li><a href="#%E5%AE%9A%E4%B9%89%E5%B8%B8%E7%94%A8%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%A6%82sgd-rmsprop-adam">&#x5B9A;&#x4E49;&#x5E38;&#x7528;&#x7684;&#x4F18;&#x5316;&#x5668;&#x5982;SGD, RMSProp, Adam&#x3002;</a></li>
<li><a href="#%E6%8E%A5%E4%B8%8B%E6%9D%A5-%E6%9E%84%E9%80%A0%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95">&#x63A5;&#x4E0B;&#x6765; &#x6784;&#x9020;&#x4F18;&#x5316;&#x7B97;&#x6CD5;</a></li>
<li><a href="#%E5%AF%B9%E6%AF%94%E4%B8%8D%E5%90%8C%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E4%BC%98%E5%8C%96%E6%95%88%E6%9E%9C">&#x5BF9;&#x6BD4;&#x4E0D;&#x540C;&#x4F18;&#x5316;&#x5668;&#x7684;&#x4F18;&#x5316;&#x6548;&#x679C;</a></li>
</ul>
</li>
<li><a href="#meta-optimizer-%E4%BB%8E%E6%89%8B%E5%B7%A5%E8%AE%BE%E8%AE%A1%E4%BC%98%E5%8C%96%E5%99%A8%E8%BF%88%E6%AD%A5%E5%88%B0%E8%87%AA%E5%8A%A8%E8%AE%BE%E8%AE%A1%E4%BC%98%E5%8C%96%E5%99%A8">Meta-optimizer &#xFF1A;&#x4ECE;&#x624B;&#x5DE5;&#x8BBE;&#x8BA1;&#x4F18;&#x5316;&#x5668;&#x8FC8;&#x6B65;&#x5230;&#x81EA;&#x52A8;&#x8BBE;&#x8BA1;&#x4F18;&#x5316;&#x5668;</a>
<ul>
<li><a href="#%E7%94%A8%E4%B8%80%E4%B8%AA%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99%E6%9B%BF%E4%BB%A3%E6%89%8B%E5%B7%A5%E8%AE%BE%E8%AE%A1%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99">&#x7528;&#x4E00;&#x4E2A;&#x53EF;&#x5B66;&#x4E60;&#x7684;&#x68AF;&#x5EA6;&#x66F4;&#x65B0;&#x89C4;&#x5219;&#xFF0C;&#x66FF;&#x4EE3;&#x624B;&#x5DE5;&#x8BBE;&#x8BA1;&#x7684;&#x68AF;&#x5EA6;&#x66F4;&#x65B0;&#x89C4;&#x5219;</a>
<ul>
<li><a href="#%E6%9E%84%E5%BB%BAlstm%E4%BC%98%E5%8C%96%E5%99%A8">&#x6784;&#x5EFA;LSTM&#x4F18;&#x5316;&#x5668;</a></li>
<li><a href="#%E4%BC%98%E5%8C%96%E5%99%A8%E6%9C%AC%E8%BA%AB%E7%9A%84%E5%8F%82%E6%95%B0%E5%8D%B3lstm%E7%9A%84%E5%8F%82%E6%95%B0%E4%BB%A3%E8%A1%A8%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E6%9B%B4%E6%96%B0%E7%AD%96%E7%95%A5">&#x4F18;&#x5316;&#x5668;&#x672C;&#x8EAB;&#x7684;&#x53C2;&#x6570;&#x5373;LSTM&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x4EE3;&#x8868;&#x4E86;&#x6211;&#x4EEC;&#x7684;&#x66F4;&#x65B0;&#x7B56;&#x7565;</a>
<ul>
<li><a href="#%E5%A5%BD%E4%BA%86%E7%9C%8B%E4%B8%80%E4%B8%8B%E6%88%91%E4%BB%AC%E4%BD%BF%E7%94%A8%E5%88%9A%E5%88%9A%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84lstm%E4%BC%98%E5%8C%96%E5%99%A8%E5%90%8E%E7%9A%84%E4%BC%98%E5%8C%96%E7%BB%93%E6%9E%9C">&#x597D;&#x4E86;&#xFF0C;&#x770B;&#x4E00;&#x4E0B;&#x6211;&#x4EEC;&#x4F7F;&#x7528;&#x521A;&#x521A;&#x521D;&#x59CB;&#x5316;&#x7684;LSTM&#x4F18;&#x5316;&#x5668;&#x540E;&#x7684;&#x4F18;&#x5316;&#x7ED3;&#x679C;</a></li>
<li><a href="#%E5%92%A6%E4%B8%BA%E4%BB%80%E4%B9%88lstm%E4%BC%98%E5%8C%96%E5%99%A8%E9%82%A3%E4%B9%88%E5%B7%AE%E6%A0%B9%E6%9C%AC%E6%B2%A1%E6%9C%89%E4%BC%98%E5%8C%96%E6%95%88%E6%9E%9C">&#x54A6;&#xFF0C;&#x4E3A;&#x4EC0;&#x4E48;LSTM&#x4F18;&#x5316;&#x5668;&#x90A3;&#x4E48;&#x5DEE;&#xFF0C;&#x6839;&#x672C;&#x6CA1;&#x6709;&#x4F18;&#x5316;&#x6548;&#x679C;&#xFF1F;</a></li>
</ul>
</li>
<li><a href="#%E4%B8%8B%E9%9D%A2%E6%88%91%E4%BB%AC%E5%B0%B1%E6%9D%A5%E4%BC%98%E5%8C%96lstm%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E5%8F%82%E6%95%B0">&#x4E0B;&#x9762;&#x6211;&#x4EEC;&#x5C31;&#x6765;&#x4F18;&#x5316;LSTM&#x4F18;&#x5316;&#x5668;&#x7684;&#x53C2;&#x6570;&#xFF01;</a>
<ul>
<li><a href="#%E4%B8%8B%E6%A3%8B%E6%89%8B">&#x201C;&#x4E0B;&#x68CB;&#x624B; &#x201D;</a></li>
<li><a href="#%E7%89%B9%E7%82%B9-2%E8%80%83%E8%99%91%E4%BC%98%E5%8C%96%E5%99%A8%E4%BC%98%E5%8C%96%E8%BF%87%E7%A8%8B%E7%9A%84%E5%8E%86%E5%8F%B2%E5%85%A8%E5%B1%80%E6%80%A7%E4%BF%A1%E6%81%AF-3%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83%E5%9C%B0%E9%87%87%E6%A0%B7%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%82%E6%95%B0">&#x7279;&#x70B9; &#xFF1A; 2.&#x8003;&#x8651;&#x4F18;&#x5316;&#x5668;&#x4F18;&#x5316;&#x8FC7;&#x7A0B;&#x7684;&#x5386;&#x53F2;&#x5168;&#x5C40;&#x6027;&#x4FE1;&#x606F;     3.&#x72EC;&#x7ACB;&#x540C;&#x5206;&#x5E03;&#x5730;&#x91C7;&#x6837;&#x4F18;&#x5316;&#x95EE;&#x9898;&#x76EE;&#x6807;&#x51FD;&#x6570;&#x7684;&#x53C2;&#x6570;</a></li>
</ul>
</li>
<li><a href="#%E9%80%9A%E8%BF%87%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%9D%A5%E4%BC%98%E5%8C%96-%E4%BC%98%E5%8C%96%E5%99%A8">&#x901A;&#x8FC7;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x6CD5;&#x6765;&#x4F18;&#x5316;  &#x4F18;&#x5316;&#x5668;</a>
<ul>
<li><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88loss%E5%80%BC%E6%B2%A1%E6%9C%89%E6%94%B9%E5%8F%98%E4%B8%BA%E4%BB%80%E4%B9%88lstm%E5%8F%82%E6%95%B0%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8D%E5%AD%98%E5%9C%A8%E7%9A%84">&#x4E3A;&#x4EC0;&#x4E48;loss&#x503C;&#x6CA1;&#x6709;&#x6539;&#x53D8;&#xFF1F;&#x4E3A;&#x4EC0;&#x4E48;LSTM&#x53C2;&#x6570;&#x7684;&#x68AF;&#x5EA6;&#x4E0D;&#x5B58;&#x5728;&#x7684;&#xFF1F;</a></li>
<li><a href="#%E9%97%AE%E9%A2%98%E5%87%BA%E5%9C%A8%E5%93%AA%E9%87%8C">&#x95EE;&#x9898;&#x51FA;&#x5728;&#x54EA;&#x91CC;&#xFF1F;</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8D%E5%86%8D%E4%B8%A2%E5%A4%B1%E4%BA%86lstm%E7%9A%84%E5%8F%82%E6%95%B0%E7%9A%84%E6%A2%AF%E5%BA%A6%E7%BB%8F%E8%BF%87%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%9A%84%E6%B5%81%E5%8A%A8%E5%B7%B2%E7%BB%8F%E4%BA%A7%E7%94%9F%E4%BA%86">&#x8BA1;&#x7B97;&#x56FE;&#x4E0D;&#x518D;&#x4E22;&#x5931;&#x4E86;&#xFF0C;LSTM&#x7684;&#x53C2;&#x6570;&#x7684;&#x68AF;&#x5EA6;&#x7ECF;&#x8FC7;&#x8BA1;&#x7B97;&#x56FE;&#x7684;&#x6D41;&#x52A8;&#x5DF2;&#x7ECF;&#x4EA7;&#x751F;&#x4E86;&#xFF01;</a></li>
</ul>
</li>
<li><a href="#%E5%8F%88%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E5%B9%BA%E8%9B%BE%E5%AD%90">&#x53C8;&#x51FA;&#x4E86;&#x4EC0;&#x4E48;&#x5E7A;&#x86FE;&#x5B50;&#xFF1F;</a>
<ul>
<li><a href="#%E4%B8%8D%E5%90%8C%E5%91%A8%E6%9C%9F%E4%B8%8B%E8%BE%93%E5%85%A5lstm%E7%9A%84%E6%A2%AF%E5%BA%A6%E5%B9%85%E5%80%BC%E6%95%B0%E9%87%8F%E7%BA%A7%E4%B8%8D%E5%9C%A8%E4%B8%80%E4%B8%AA%E7%AD%89%E7%BA%A7%E4%B8%8A%E9%9D%A2">&#x4E0D;&#x540C;&#x5468;&#x671F;&#x4E0B;&#x8F93;&#x5165;LSTM&#x7684;&#x68AF;&#x5EA6;&#x5E45;&#x503C;&#x6570;&#x91CF;&#x7EA7;&#x4E0D;&#x5728;&#x4E00;&#x4E2A;&#x7B49;&#x7EA7;&#x4E0A;&#x9762;</a></li>
<li><a href="#%E7%94%A8%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%BD%92%E4%B8%80%E5%8C%96%E5%B9%85%E5%80%BC%E6%96%B9%E5%90%91%E4%BA%8C%E5%85%83%E7%BB%84%E6%9B%BF%E4%BB%A3%E5%8E%9F%E6%A2%AF%E5%BA%A6%E4%BD%9C%E4%B8%BAlstm%E7%9A%84%E8%BE%93%E5%85%A5">&#x7528;&#x68AF;&#x5EA6;&#x7684;&#xFF08;&#x5F52;&#x4E00;&#x5316;&#x5E45;&#x503C;&#xFF0C;&#x65B9;&#x5411;&#xFF09;&#x4E8C;&#x5143;&#x7EC4;&#x66FF;&#x4EE3;&#x539F;&#x68AF;&#x5EA6;&#x4F5C;&#x4E3A;LSTM&#x7684;&#x8F93;&#x5165;</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E4%BB%A5%E4%B8%8A%E6%98%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E5%86%99%E9%81%87%E5%88%B0%E7%9A%84%E7%A7%8D%E7%A7%8D%E9%97%AE%E9%A2%98%E4%B8%8B%E9%9D%A2%E5%B0%B1%E6%98%AF%E6%9C%80%E5%AE%8C%E6%95%B4%E7%9A%84%E6%9C%89%E6%95%88%E4%BB%A3%E7%A0%81%E4%BA%86">&#x4EE5;&#x4E0A;&#x662F;&#x4EE3;&#x7801;&#x7F16;&#x5199;&#x9047;&#x5230;&#x7684;&#x79CD;&#x79CD;&#x95EE;&#x9898;&#xFF0C;&#x4E0B;&#x9762;&#x5C31;&#x662F;&#x6700;&#x5B8C;&#x6574;&#x7684;&#x6709;&#x6548;&#x4EE3;&#x7801;&#x4E86;&#xFF01;&#xFF01;&#xFF01;</a>
<ul>
<li><a href="#%E6%88%91%E4%BB%AC%E5%85%88%E6%9D%A5%E7%9C%8B%E7%9C%8B%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84lstm%E4%BC%98%E5%8C%96%E5%99%A8%E7%9A%84%E6%95%88%E6%9E%9C">&#x6211;&#x4EEC;&#x5148;&#x6765;&#x770B;&#x770B;&#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x7684;LSTM&#x4F18;&#x5316;&#x5668;&#x7684;&#x6548;&#x679C;</a><br>
* <a href="#%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84lstm%E4%BC%98%E5%8C%96%E5%99%A8%E6%B2%A1%E6%9C%89%E4%BB%BB%E4%BD%95%E6%95%88%E6%9E%9Closs%E5%8F%91%E6%95%A3%E4%BA%86%E5%9B%A0%E4%B8%BA%E8%BF%98%E6%B2%A1%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96%E5%99%A8">&#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x7684;LSTM&#x4F18;&#x5316;&#x5668;&#x6CA1;&#x6709;&#x4EFB;&#x4F55;&#x6548;&#x679C;&#xFF0C;loss&#x53D1;&#x6563;&#x4E86;&#xFF0C;&#x56E0;&#x4E3A;&#x8FD8;&#x6CA1;&#x8BAD;&#x7EC3;&#x4F18;&#x5316;&#x5668;</a><br>
* <a href="#%E6%8E%A5%E4%B8%8B%E6%9D%A5%E7%9C%8B%E4%B8%80%E4%B8%8B%E4%BC%98%E5%8C%96%E5%A5%BD%E7%9A%84lstm%E4%BC%98%E5%8C%96%E5%99%A8%E6%A8%A1%E5%9E%8B%E5%92%8Csgdrmspropadam%E7%9A%84%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94%E8%A1%A8%E7%8E%B0%E5%90%A7~">&#x63A5;&#x4E0B;&#x6765;&#x770B;&#x4E00;&#x4E0B;&#x4F18;&#x5316;&#x597D;&#x7684;LSTM&#x4F18;&#x5316;&#x5668;&#x6A21;&#x578B;&#x548C;SGD&#xFF0C;RMSProp&#xFF0C;Adam&#x7684;&#x4F18;&#x5316;&#x6027;&#x80FD;&#x5BF9;&#x6BD4;&#x8868;&#x73B0;&#x5427;~</a></li>
</ul>
</li>
<li><a href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90%E4%B8%8E%E7%BB%93%E8%AE%BA">&#x5B9E;&#x9A8C;&#x7ED3;&#x679C;&#x5206;&#x6790;&#x4E0E;&#x7ED3;&#x8BBA;</a>
<ul>
<li><a href="#%E8%AF%B7%E6%B3%A8%E6%84%8Flstm%E4%BC%98%E5%8C%96%E5%99%A8%E6%9C%80%E7%BB%88%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5%E6%98%AF%E6%B2%A1%E6%9C%89%E4%BB%BB%E4%BD%95%E4%BA%BA%E5%B7%A5%E8%AE%BE%E8%AE%A1%E7%9A%84%E7%BB%8F%E9%AA%8C">&#x8BF7;&#x6CE8;&#x610F;&#xFF1A;LSTM&#x4F18;&#x5316;&#x5668;&#x6700;&#x7EC8;&#x4F18;&#x5316;&#x7B56;&#x7565;&#x662F;&#x6CA1;&#x6709;&#x4EFB;&#x4F55;&#x4EBA;&#x5DE5;&#x8BBE;&#x8BA1;&#x7684;&#x7ECF;&#x9A8C;</a></li>
</ul>
</li>
<li><a href="#%E5%AE%9E%E9%AA%8C%E6%9D%A1%E4%BB%B6">&#x5B9E;&#x9A8C;&#x6761;&#x4EF6;&#xFF1A;</a></li>
<li><a href="#%E5%90%8E%E5%8F%99">&#x540E;&#x53D9;</a></li>
<li><a href="#%E4%B8%8B%E8%BD%BD%E5%9C%B0%E5%9D%80%E4%B8%8E%E5%8F%82%E8%80%83">&#x4E0B;&#x8F7D;&#x5730;&#x5740;&#x4E0E;&#x53C2;&#x8003;</a></li>
</ul>

      </div>
      
      
    
    
    
    
      <p>&nbsp;</p>
      <!--网页尾部--><!--以上为修改部分 --><!--以上为修改部分 --><!--以上为修改部分 --><!--以上为修改部分 -->
      <!--以上为修改部分 --><!--以上为修改部分 --><!--以上为修改部分 --><!--以上为修改部分 --><!--以上为修改部分 -->
        <!--网页尾部-->
      <script type="text/javascript" src="/html_style/foot.js"></script>
      </body>
      </html>
    